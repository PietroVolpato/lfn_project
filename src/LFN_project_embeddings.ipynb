{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d121cbd5",
   "metadata": {
    "id": "d121cbd5"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PietroVolpato/lfn_project/blob/main/src/LFN_project_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358080be-e62b-4a82-b8d3-e5becb180aac",
   "metadata": {
    "id": "358080be-e62b-4a82-b8d3-e5becb180aac"
   },
   "source": [
    "# Learning from networks project\n",
    "### Evaluation of different Node Embedding algorithms\n",
    "Members:<br>\n",
    "- D'Emilio Filippo, id : 2120931\n",
    "- Volpato Pietro, id : 2120825\n",
    "\n",
    "### Information about the notebook (have a look at the report for details)\n",
    "This notebook is responsable of computing the embeddings for every embedding technique and for every selected graph.<br>\n",
    "Each computed embedding is saved to file as a numpy array (extension .npy), in the directory /embeddings. In this way that once an embedding is computed, it won't be lost when the runtime of the notebook is terminated.<br>\n",
    "We can then efficiently load the embeddings in the \"test\" notebook, and evaluate the quality of the embeddings.<br>\n",
    "Selected embedding techniques:\n",
    "- Node2Vec\n",
    "- Line\n",
    "- ...\n",
    "\n",
    "For information about the graphs, se cells below.<br>\n",
    "*NOTE*: by implementation choice, the computation of each embedding is computed separately (e.g. there are no function to coincisely compute all embeddings).<br>\n",
    "This choice comes from the fact that computing embeddings is computationally intensive, and we might want to compute only a specific\n",
    "embedding strategy for a specific graph, in order to update only this entry in the folder containing the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d3b60-f774-4fc7-b0bc-7617057d6459",
   "metadata": {
    "id": "bb1d3b60-f774-4fc7-b0bc-7617057d6459"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93987c53-ecce-41d6-948b-4a4b7ac4978a",
   "metadata": {
    "id": "93987c53-ecce-41d6-948b-4a4b7ac4978a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gzip\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b64818-de78-4c30-945e-b95279c7f1c9",
   "metadata": {
    "id": "d6b64818-de78-4c30-945e-b95279c7f1c9"
   },
   "source": [
    "# configuration\n",
    "Here you can properly configure the names of the graphs and the names of the embedding strategies. Use meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5b26d0-84d0-46ad-83dd-ac826faac4fe",
   "metadata": {
    "id": "6e5b26d0-84d0-46ad-83dd-ac826faac4fe"
   },
   "outputs": [],
   "source": [
    "graph_keys = [\"facebook\",\"citation\",\"biological\",\"CL\",\"COX2\"]\n",
    "embedding_keys = [\"LINE\", \"node2vec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6ca3e-b5f5-4869-a3ef-47fb66cd7c56",
   "metadata": {
    "id": "79f6ca3e-b5f5-4869-a3ef-47fb66cd7c56"
   },
   "source": [
    "# Loading the graphs\n",
    "Selected graphs:\n",
    "- Facebook_combined    https://snap.stanford.edu/data/ego-Facebook.html          \n",
    "- cit-Helpth           https://networkrepository.com/cit-HepTh.php             \n",
    "- bio-CE-CX            https://networkrepository.com/bio-CE-CX.php             \n",
    "- CL-100K-1d8-L9       https://networkrepository.com/CL-100K-1d8-L9.php ---- the graph has node labels\n",
    "- COX2-MD              https://networkrepository.com/COX2-MD.php  ---- the graph has node labels\n",
    "\n",
    "To run this notebook, adjust the paths to match where the files are saved in your PC.<br>\n",
    "To keep paths as they are, create a \"data\" folder inside the directory of this notebook, and store the files there.<br><br>\n",
    "\n",
    "Graphs are stored as a dictionary: the key is the graph name, the value is the corresponding netowrkx graph.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TtpPrXlsIUgJ",
   "metadata": {
    "id": "TtpPrXlsIUgJ"
   },
   "outputs": [],
   "source": [
    "facebook_path = 'data/facebook_combined.txt.gz'\n",
    "citation_path = 'data/cit-HepTh.edges'\n",
    "biological_path = 'data/bio-CE-CX.edges'\n",
    "CL_path = \"data/CL-100K-1d8-L9/CL-100K-1d8-L9.edges\"\n",
    "COX2_path = \"data/COX2-MD/COX2-MD.edges\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e83f7ed1-5bbd-4e15-8d0c-1a93e9802783",
   "metadata": {
    "id": "e83f7ed1-5bbd-4e15-8d0c-1a93e9802783"
   },
   "outputs": [],
   "source": [
    "def load_graph(path):\n",
    "    \"\"\"\n",
    "    For files with extension .edges\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('%'):  # Skip comment lines\n",
    "                continue\n",
    "            # Split the line based on spaces or commas\n",
    "            data = re.split(r'[,\\s]+', line.strip())\n",
    "            if len(data) < 2:  # Skip lines that don't have at least two columns\n",
    "                continue\n",
    "            # Extract the first two columns (nodes)\n",
    "            node1, node2 = int(data[0]), int(data[1])\n",
    "            G.add_edge(node1, node2)\n",
    "    mapping = {node : i for i,node in enumerate(G.nodes)} # mappoing original : relabeled\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    return G\n",
    "\n",
    "def load_graph_with_gz(path):\n",
    "    \"\"\"\n",
    "    For files with extension .txt.gz\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            node1, node2 = map(int, line.strip().split())\n",
    "            G.add_edge(node1, node2)\n",
    "    mapping = {node : i for i,node in enumerate(G.nodes)} # mappoing original : relabeled\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    return G\n",
    "\n",
    "def print_graphs_info(graphs):\n",
    "    for k in graph_keys:\n",
    "        G = graphs[k]\n",
    "        print(f\"{k}: |V|={len(G.nodes)}, |E|={len(G.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2ab6207-3fe1-44b0-bf8e-636a7d9f9f2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2ab6207-3fe1-44b0-bf8e-636a7d9f9f2b",
    "outputId": "b810991c-26ec-47d1-b2c3-248de16db6fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook graph: |V|=4039, |E|=88234\n",
      "citation graph: |V|=22908, |E|=2444798\n",
      "biological graph: |V|=15229, |E|=245952\n",
      "CL graph: |V|=92482, |E|=436611\n",
      "COX2 graph: |V|=7962, |E|=101542\n"
     ]
    }
   ],
   "source": [
    "graphs = {}\n",
    "\n",
    "# facebook graph is the only one .tar.gz\n",
    "graphs[graph_keys[0]] = load_graph_with_gz(facebook_path)  # relabeling nodes to integer\n",
    "graphs[graph_keys[1]] = load_graph(citation_path)\n",
    "graphs[graph_keys[2]] = load_graph(biological_path)\n",
    "graphs[graph_keys[3]] = load_graph(CL_path)  # node labeled\n",
    "graphs[graph_keys[4]] = load_graph(COX2_path)  # node labeled\n",
    "\n",
    "print_graphs_info(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8tGjO8jcn7P5",
   "metadata": {
    "id": "8tGjO8jcn7P5"
   },
   "source": [
    "# Download the dataset from the GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pBB2zmeGoCXe",
   "metadata": {
    "id": "pBB2zmeGoCXe"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/PietroVolpato/lfn_project/main/data/\"\n",
    "filename = \"bio-CE-CX_edges.csv\"\n",
    "\n",
    "response = requests.get(url + filename)\n",
    "with open(filename, \"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ab17a-9328-47b5-bc28-15dbd59deabf",
   "metadata": {
    "id": "f14ab17a-9328-47b5-bc28-15dbd59deabf"
   },
   "source": [
    "# Functions and declarations for the embeddings\n",
    "Embedding data structure is defined as following:<br>\n",
    "- The first index refer to the graph (e.g. embeddings[\"facebook\"] contains the embeddings of the facebook graph for every embedding technique).<br>\n",
    "- The second index refer to the embedding technique (e.g. embeddings[\"facebook\"][\"LINE\"] cointans the embedding of facebook graph computed using LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e7e55ae-2aa3-4989-8903-0ccc3898309f",
   "metadata": {
    "id": "0e7e55ae-2aa3-4989-8903-0ccc3898309f"
   },
   "outputs": [],
   "source": [
    "def save(emb, graph_key, embedding_key):\n",
    "    path = f\"../result/embeddings_{graph_key}_{embedding_key}.npy\"\n",
    "    np.save(path, emb)\n",
    "    print(f\"Successfully saved the embeddings in {path}\")\n",
    "\n",
    "# dictionaries to store the embeddings, obtained by several techniques, for each graph\n",
    "embeddings = {}\n",
    "for k in graph_keys:\n",
    "    embeddings[k] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0aada-e8df-421a-bf3a-d40c09885e40",
   "metadata": {
    "id": "66b0aada-e8df-421a-bf3a-d40c09885e40"
   },
   "source": [
    "Spiegazione sui parametri di node2vec:<br>\n",
    "- G (required): The graph on which to run Node2Vec. Must be an undirected networkx.Graph object.\n",
    "- dimensions (default = 128): The dimensionality of the node embeddings. Higher dimensions allow for capturing more information but increase computational cost.\n",
    "- walk_length (default = 80): The number of steps for each random walk. A larger walk_length captures more of the network structure.\n",
    "- num_walks (default = 10): The number of random walks to start per node. Increasing this can improve the representation at the cost of additional computation.\n",
    "- workers (default = 1): The number of CPU cores to use for parallel processing. If you're running this on a multi-core machine, increasing this can speed up the computation.\n",
    "- p (return parameter): p<1: Increases the likelihood of revisiting a node (DFS-like behavior). p>1: Discourages revisiting nodes, encouraging exploration (BFS-like behavior).\n",
    "- q (in-out parameter): q<1: Encourages walks to nodes further away from the starting node (BFS-like).q>1: Biases walks to nodes closer to the starting node (DFS-like).\n",
    "\n",
    "Spiegazione di : model = node2vec.fit(window=5, min_count=1, batch_words=4)<br>\n",
    "This trains a Word2Vec model (from the gensim library) using the random walks. Let’s go over the parameters:<br>\n",
    "\n",
    "- window (default = 10): The maximum distance between the current and predicted nodes in the random walk sequence. Larger windows capture more context but require more computation.\n",
    "\n",
    "- min_count (default = 1): Minimum frequency for a node to be considered in the embedding. Since most graphs are sparse, this is often set to 1.\n",
    "\n",
    "- batch_words (default = 4): The number of words (or nodes) processed in each training batch. Adjust this for performance depending on your hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7649b02-023b-4ec3-b1e7-4d051776d0f2",
   "metadata": {
    "id": "d7649b02-023b-4ec3-b1e7-4d051776d0f2"
   },
   "source": [
    "# Node2Vec\n",
    "- pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c54d5a3a-a0ff-4044-8ba4-808588927dba",
   "metadata": {
    "id": "c54d5a3a-a0ff-4044-8ba4-808588927dba"
   },
   "outputs": [],
   "source": [
    "def get_node2vec_embeddings(G, dimensions=128, walk_length=10, num_walks=20, p=1, q=1, workers=1):\n",
    "    \"\"\"\n",
    "    Generate node embeddings for a graph using the Node2Vec algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph):\n",
    "            The input graph for which embeddings are to be generated.\n",
    "            The graph should have nodes labeled as integers, ideally sequentially starting from 0.\n",
    "\n",
    "        dimensions (int, optional):\n",
    "            The dimensionality of the embedding space. Default is 128.\n",
    "\n",
    "        walk_length (int, optional):\n",
    "            The length of each random walk. Default is 10.\n",
    "\n",
    "        num_walks (int, optional):\n",
    "            The number of random walks to start from each node. Default is 20.\n",
    "\n",
    "        p (float, optional):\n",
    "            The return parameter, controlling the likelihood of immediately revisiting a node in the walk.\n",
    "            A higher value makes it more likely to backtrack. Default is 1.\n",
    "\n",
    "        q (float, optional):\n",
    "            The in-out parameter, controlling the likelihood of exploring outward from the starting node.\n",
    "            A higher value makes it more likely to move outward. Default is 1.\n",
    "\n",
    "        workers (int, optional):\n",
    "            The number of parallel workers for random walk generation and model training. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            A NumPy array where each row represents the embedding of a node.\n",
    "            The row index corresponds to the node ID, and each row has `dimensions` elements.\n",
    "    \"\"\"\n",
    "    # Initialize Node2Vec model\n",
    "    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, p=p, q=q, workers=workers)\n",
    "\n",
    "    # Fit the Node2Vec model and generate embeddings\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "    # Convert embeddings to a NumPy array\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    embeddings = np.zeros((num_nodes, dimensions))  # Preallocate array\n",
    "    for node in G.nodes:\n",
    "        embeddings[node] = model.wv[node]\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d9eac-2b78-4283-bea5-354f1f324e49",
   "metadata": {},
   "source": [
    "## Produce the embeddings with node2vec\n",
    "here you can easily produce the embeddings for any of the loaded graphs using node2vec.<br>\n",
    "Adjust the variable curr_graph_key with the key of the graph you want to compute the embeddings for.<br>\n",
    "The embeddings are saved to file (look output to get path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48f11c92-dd61-476b-9c80-e152704a2e58",
   "metadata": {
    "id": "2535202c-086c-42ae-a7a0-2bce62d42e02"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb317ddb4aa4a168cf6114a26da2685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/15229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|███████████████████████████████████████████████████████| 20/20 [00:31<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the embeddings in ../result/embeddings_biological_node2vec.npy\n"
     ]
    }
   ],
   "source": [
    "# graph_keys[0] = facebook\n",
    "# graph_keys[1] = biological\n",
    "# graph_keys[2] = citation\n",
    "# graph_keys[3] = CL\n",
    "# graph_keys[4] = COX2\n",
    "curr_graph_key = graph_keys[2]   # chose the graph\n",
    "\n",
    "embeddings[curr_graph_key][\"node2vec\"] = get_node2vec_embeddings(graphs[curr_graph_key])\n",
    "save(embeddings[curr_graph_key][\"node2vec\"], curr_graph_key, \"node2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c4f4c-6f35-4c15-bff0-837960ef3411",
   "metadata": {
    "id": "900c4f4c-6f35-4c15-bff0-837960ef3411"
   },
   "source": [
    "# LINE : Large-scale information network embedding\n",
    "installation guide:\n",
    "- git clone https://github.com/VahidooX/LINE.git\n",
    "- !pip install keras\n",
    "- !pip install tensorflow\n",
    "- adjust the sys.path to where you downloaded LINE repository\n",
    "\n",
    "*NOTE*: it was necessary to modify utils.py to adapt it at current version of keras because some elements were deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e6cc917-5939-45b8-aca5-d192cb0fd1d9",
   "metadata": {
    "id": "3e6cc917-5939-45b8-aca5-d192cb0fd1d9"
   },
   "outputs": [],
   "source": [
    "sys.path.append(r'C:\\Users\\oppil\\OneDrive\\Desktop\\Universita\\magistrale\\2_1\\LFN\\REPO_PROJECT\\lfn_project\\src\\LINE')\n",
    "\n",
    "from model import create_model\n",
    "from utils import batchgen_train\n",
    "\n",
    "def get_LINE_embeddings(G, embedding_dim=128, batch_size=1024, negative_ratio=5, epochs=10, negative_sampling=\"UNIFORM\"):\n",
    "    \"\"\"\n",
    "    Generate LINE embeddings for a given graph.\n",
    "\n",
    "    Parameters:\n",
    "        G (nx.Graph): The graph for which embeddings are computed.\n",
    "        embedding_dim (int): Dimensionality of the embeddings.\n",
    "        batch_size (int): Batch size for training.\n",
    "        negative_ratio (int): Ratio of negative to positive samples.\n",
    "        epochs (int): Number of training epochs.\n",
    "        negative_sampling (str): Negative sampling strategy (\"UNIFORM\" or \"NON-UNIFORM\").\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Node embeddings (shape: [num_nodes, embedding_dim]).\n",
    "    \"\"\"\n",
    "    num_nodes = G.number_of_nodes()\n",
    "\n",
    "    # Convert networkx.Graph to adj_list (edge list as 2D numpy array)\n",
    "    adj_list = np.array(list(G.edges()), dtype=np.int32)\n",
    "\n",
    "    # Create LINE model\n",
    "    model, embed_generator = create_model(num_nodes, embedding_dim)\n",
    "\n",
    "    # Generate training batches\n",
    "    train_gen = batchgen_train(adj_list, num_nodes, batch_size, negative_ratio, negative_sampling)\n",
    "\n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    model.fit(train_gen, steps_per_epoch=500, epochs=epochs)\n",
    "\n",
    "    # Extract embeddings\n",
    "    node_ids = np.arange(num_nodes)  # Sequential node IDs\n",
    "    embeddings = embed_generator.predict_on_batch(node_ids)\n",
    "\n",
    "    print(\"Node Embeddings Shape:\", embeddings[0].shape)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94620d7e-7d58-494a-a968-acf7f5b20b4f",
   "metadata": {},
   "source": [
    "## Produce the embeddings with LINE\n",
    "here you can easily produce the embeddings for any of the loaded graphs using LINE.<br>\n",
    "Adjust the variable curr_graph_key with the key of the graph you want to compute the embeddings for.<br>\n",
    "The embeddings are saved to file (look output to get path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2535202c-086c-42ae-a7a0-2bce62d42e02",
   "metadata": {
    "id": "2535202c-086c-42ae-a7a0-2bce62d42e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 57s 113ms/step - loss: 0.9791\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 59s 117ms/step - loss: 0.4258\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 59s 118ms/step - loss: 0.3137\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 57s 114ms/step - loss: 0.2445\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 58s 117ms/step - loss: 0.2016\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 58s 116ms/step - loss: 0.1660\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 58s 116ms/step - loss: 0.1444\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 60s 119ms/step - loss: 0.1291\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 57s 114ms/step - loss: 0.1187\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 61s 123ms/step - loss: 0.1092\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 69s 137ms/step - loss: 0.1037\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 58s 116ms/step - loss: 0.0981\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 57s 115ms/step - loss: 0.0946\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 59s 117ms/step - loss: 0.0918\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 58s 116ms/step - loss: 0.0889\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 59s 119ms/step - loss: 0.0889\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 58s 117ms/step - loss: 0.0858\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 58s 117ms/step - loss: 0.0855\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 60s 120ms/step - loss: 0.0836\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 60s 120ms/step - loss: 0.0839\n",
      "Node Embeddings Shape: (128,)\n",
      "Successfully saved the embeddings in ../result/embeddings_facebook_LINE.npy\n"
     ]
    }
   ],
   "source": [
    "curr_graph_key = graph_keys[0]   # chose the graph\n",
    "\n",
    "embeddings[curr_graph_key][\"LINE\"] = get_LINE_embeddings(graphs[curr_graph_key], epochs = 20)\n",
    "save(embeddings[curr_graph_key][\"LINE\"], curr_graph_key, \"LINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc4a49",
   "metadata": {
    "id": "5abc4a49"
   },
   "source": [
    "# AttentionWalk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f540988",
   "metadata": {
    "id": "5f540988"
   },
   "source": [
    "## Installation guide\n",
    "<ol>\n",
    "<li>git clone https://github.com/benedekrozemberczki/AttentionWalk.git</li>\n",
    "<li>pip install texttable</li>\n",
    "</ol>\n",
    "\n",
    "It requires that the input file is a .csv, so first we have implemented a function that converts the .txt.gz and the .edges files to a .csv to be given as input to the AttentionWalk algorithm.<br>\n",
    "For starting the algorithm you have to enter to the AttentionWalk folder after having cloned it from the Github repository and then set the arguments as described in the README.md file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b872e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3b872e0",
    "outputId": "44fa23c4-3512-4e6c-e2e7-4459f560804d"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/benedekrozemberczki/AttentionWalk.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33c074",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d33c074",
    "outputId": "bdb2c444-b76b-4596-af4b-f60315b0747e"
   },
   "outputs": [],
   "source": [
    "!pip install texttable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b0bbb",
   "metadata": {
    "id": "d01b0bbb"
   },
   "source": [
    "## Test with the facebook network\n",
    "Save the embeddings in the result folder with also the attention path<br>\n",
    "Time: 1m 50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98fc24b7",
   "metadata": {
    "id": "98fc24b7",
    "outputId": "80702d2f-f99e-402c-a944-b6dcc8ab5146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------------------------------+\n",
      "| Attention path |      ../../result/facebook_attention.csv       |\n",
      "+================+================================================+\n",
      "| Beta           | 0.500                                          |\n",
      "+----------------+------------------------------------------------+\n",
      "| Dimensions     | 128                                            |\n",
      "+----------------+------------------------------------------------+\n",
      "| Edge path      | ../../data/facebook_combined.csv               |\n",
      "+----------------+------------------------------------------------+\n",
      "| Embedding path | ../../result/facebook_embeddings_attention.csv |\n",
      "+----------------+------------------------------------------------+\n",
      "| Epochs         | 200                                            |\n",
      "+----------------+------------------------------------------------+\n",
      "| Gamma          | 0.500                                          |\n",
      "+----------------+------------------------------------------------+\n",
      "| Learning rate  | 0.010                                          |\n",
      "+----------------+------------------------------------------------+\n",
      "| Num of walks   | 80                                             |\n",
      "+----------------+------------------------------------------------+\n",
      "| Window size    | 5                                              |\n",
      "+----------------+------------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n",
      "\n",
      "Starting training.\n",
      "\n",
      "Epoch 1/200 - Loss: 48.3133\n",
      "Epoch 2/200 - Loss: 48.3003\n",
      "Epoch 3/200 - Loss: 48.1250\n",
      "Epoch 4/200 - Loss: 47.8233\n",
      "Epoch 5/200 - Loss: 47.3144\n",
      "Epoch 6/200 - Loss: 46.5635\n",
      "Epoch 7/200 - Loss: 45.5540\n",
      "Epoch 8/200 - Loss: 44.2737\n",
      "Epoch 9/200 - Loss: 42.7165\n",
      "Epoch 10/200 - Loss: 40.8853\n",
      "Epoch 11/200 - Loss: 38.7935\n",
      "Epoch 12/200 - Loss: 36.4657\n",
      "Epoch 13/200 - Loss: 33.9372\n",
      "Epoch 14/200 - Loss: 31.2542\n",
      "Epoch 15/200 - Loss: 28.4725\n",
      "Epoch 16/200 - Loss: 25.6552\n",
      "Epoch 17/200 - Loss: 22.8696\n",
      "Epoch 18/200 - Loss: 20.1821\n",
      "Epoch 19/200 - Loss: 17.6527\n",
      "Epoch 20/200 - Loss: 15.3303\n",
      "Epoch 21/200 - Loss: 13.2480\n",
      "Epoch 22/200 - Loss: 11.4220\n",
      "Epoch 23/200 - Loss: 9.8521\n",
      "Epoch 24/200 - Loss: 8.5246\n",
      "Epoch 25/200 - Loss: 7.4169\n",
      "Epoch 26/200 - Loss: 6.5013\n",
      "Epoch 27/200 - Loss: 5.7493\n",
      "Epoch 28/200 - Loss: 5.1339\n",
      "Epoch 29/200 - Loss: 4.6308\n",
      "Epoch 30/200 - Loss: 4.2194\n",
      "Epoch 31/200 - Loss: 3.8825\n",
      "Epoch 32/200 - Loss: 3.6059\n",
      "Epoch 33/200 - Loss: 3.3783\n",
      "Epoch 34/200 - Loss: 3.1904\n",
      "Epoch 35/200 - Loss: 3.0348\n",
      "Epoch 36/200 - Loss: 2.9056\n",
      "Epoch 37/200 - Loss: 2.7979\n",
      "Epoch 38/200 - Loss: 2.7079\n",
      "Epoch 39/200 - Loss: 2.6322\n",
      "Epoch 40/200 - Loss: 2.5684\n",
      "Epoch 41/200 - Loss: 2.5143\n",
      "Epoch 42/200 - Loss: 2.4681\n",
      "Epoch 43/200 - Loss: 2.4284\n",
      "Epoch 44/200 - Loss: 2.3941\n",
      "Epoch 45/200 - Loss: 2.3642\n",
      "Epoch 46/200 - Loss: 2.3379\n",
      "Epoch 47/200 - Loss: 2.3146\n",
      "Epoch 48/200 - Loss: 2.2937\n",
      "Epoch 49/200 - Loss: 2.2749\n",
      "Epoch 50/200 - Loss: 2.2578\n",
      "Epoch 51/200 - Loss: 2.2421\n",
      "Epoch 52/200 - Loss: 2.2276\n",
      "Epoch 53/200 - Loss: 2.2141\n",
      "Epoch 54/200 - Loss: 2.2015\n",
      "Epoch 55/200 - Loss: 2.1896\n",
      "Epoch 56/200 - Loss: 2.1783\n",
      "Epoch 57/200 - Loss: 2.1676\n",
      "Epoch 58/200 - Loss: 2.1574\n",
      "Epoch 59/200 - Loss: 2.1477\n",
      "Epoch 60/200 - Loss: 2.1383\n",
      "Epoch 61/200 - Loss: 2.1293\n",
      "Epoch 62/200 - Loss: 2.1206\n",
      "Epoch 63/200 - Loss: 2.1123\n",
      "Epoch 64/200 - Loss: 2.1043\n",
      "Epoch 65/200 - Loss: 2.0965\n",
      "Epoch 66/200 - Loss: 2.0890\n",
      "Epoch 67/200 - Loss: 2.0817\n",
      "Epoch 68/200 - Loss: 2.0747\n",
      "Epoch 69/200 - Loss: 2.0679\n",
      "Epoch 70/200 - Loss: 2.0613\n",
      "Epoch 71/200 - Loss: 2.0549\n",
      "Epoch 72/200 - Loss: 2.0487\n",
      "Epoch 73/200 - Loss: 2.0427\n",
      "Epoch 74/200 - Loss: 2.0369\n",
      "Epoch 75/200 - Loss: 2.0312\n",
      "Epoch 76/200 - Loss: 2.0257\n",
      "Epoch 77/200 - Loss: 2.0204\n",
      "Epoch 78/200 - Loss: 2.0152\n",
      "Epoch 79/200 - Loss: 2.0101\n",
      "Epoch 80/200 - Loss: 2.0052\n",
      "Epoch 81/200 - Loss: 2.0004\n",
      "Epoch 82/200 - Loss: 1.9957\n",
      "Epoch 83/200 - Loss: 1.9912\n",
      "Epoch 84/200 - Loss: 1.9868\n",
      "Epoch 85/200 - Loss: 1.9824\n",
      "Epoch 86/200 - Loss: 1.9782\n",
      "Epoch 87/200 - Loss: 1.9741\n",
      "Epoch 88/200 - Loss: 1.9701\n",
      "Epoch 89/200 - Loss: 1.9661\n",
      "Epoch 90/200 - Loss: 1.9623\n",
      "Epoch 91/200 - Loss: 1.9585\n",
      "Epoch 92/200 - Loss: 1.9548\n",
      "Epoch 93/200 - Loss: 1.9512\n",
      "Epoch 94/200 - Loss: 1.9477\n",
      "Epoch 95/200 - Loss: 1.9442\n",
      "Epoch 96/200 - Loss: 1.9408\n",
      "Epoch 97/200 - Loss: 1.9375\n",
      "Epoch 98/200 - Loss: 1.9342\n",
      "Epoch 99/200 - Loss: 1.9310\n",
      "Epoch 100/200 - Loss: 1.9278\n",
      "Epoch 101/200 - Loss: 1.9247\n",
      "Epoch 102/200 - Loss: 1.9217\n",
      "Epoch 103/200 - Loss: 1.9187\n",
      "Epoch 104/200 - Loss: 1.9158\n",
      "Epoch 105/200 - Loss: 1.9129\n",
      "Epoch 106/200 - Loss: 1.9100\n",
      "Epoch 107/200 - Loss: 1.9072\n",
      "Epoch 108/200 - Loss: 1.9044\n",
      "Epoch 109/200 - Loss: 1.9017\n",
      "Epoch 110/200 - Loss: 1.8990\n",
      "Epoch 111/200 - Loss: 1.8964\n",
      "Epoch 112/200 - Loss: 1.8938\n",
      "Epoch 113/200 - Loss: 1.8913\n",
      "Epoch 114/200 - Loss: 1.8888\n",
      "Epoch 115/200 - Loss: 1.8863\n",
      "Epoch 116/200 - Loss: 1.8838\n",
      "Epoch 117/200 - Loss: 1.8814\n",
      "Epoch 118/200 - Loss: 1.8791\n",
      "Epoch 119/200 - Loss: 1.8768\n",
      "Epoch 120/200 - Loss: 1.8745\n",
      "Epoch 121/200 - Loss: 1.8722\n",
      "Epoch 122/200 - Loss: 1.8700\n",
      "Epoch 123/200 - Loss: 1.8678\n",
      "Epoch 124/200 - Loss: 1.8656\n",
      "Epoch 125/200 - Loss: 1.8635\n",
      "Epoch 126/200 - Loss: 1.8614\n",
      "Epoch 127/200 - Loss: 1.8594\n",
      "Epoch 128/200 - Loss: 1.8573\n",
      "Epoch 129/200 - Loss: 1.8553\n",
      "Epoch 130/200 - Loss: 1.8533\n",
      "Epoch 131/200 - Loss: 1.8514\n",
      "Epoch 132/200 - Loss: 1.8495\n",
      "Epoch 133/200 - Loss: 1.8476\n",
      "Epoch 134/200 - Loss: 1.8457\n",
      "Epoch 135/200 - Loss: 1.8439\n",
      "Epoch 136/200 - Loss: 1.8421\n",
      "Epoch 137/200 - Loss: 1.8403\n",
      "Epoch 138/200 - Loss: 1.8385\n",
      "Epoch 139/200 - Loss: 1.8368\n",
      "Epoch 140/200 - Loss: 1.8351\n",
      "Epoch 141/200 - Loss: 1.8334\n",
      "Epoch 142/200 - Loss: 1.8317\n",
      "Epoch 143/200 - Loss: 1.8301\n",
      "Epoch 144/200 - Loss: 1.8285\n",
      "Epoch 145/200 - Loss: 1.8269\n",
      "Epoch 146/200 - Loss: 1.8253\n",
      "Epoch 147/200 - Loss: 1.8237\n",
      "Epoch 148/200 - Loss: 1.8222\n",
      "Epoch 149/200 - Loss: 1.8207\n",
      "Epoch 150/200 - Loss: 1.8192\n",
      "Epoch 151/200 - Loss: 1.8177\n",
      "Epoch 152/200 - Loss: 1.8163\n",
      "Epoch 153/200 - Loss: 1.8149\n",
      "Epoch 154/200 - Loss: 1.8134\n",
      "Epoch 155/200 - Loss: 1.8120\n",
      "Epoch 156/200 - Loss: 1.8107\n",
      "Epoch 157/200 - Loss: 1.8093\n",
      "Epoch 158/200 - Loss: 1.8080\n",
      "Epoch 159/200 - Loss: 1.8066\n",
      "Epoch 160/200 - Loss: 1.8053\n",
      "Epoch 161/200 - Loss: 1.8041\n",
      "Epoch 162/200 - Loss: 1.8028\n",
      "Epoch 163/200 - Loss: 1.8015\n",
      "Epoch 164/200 - Loss: 1.8003\n",
      "Epoch 165/200 - Loss: 1.7991\n",
      "Epoch 166/200 - Loss: 1.7979\n",
      "Epoch 167/200 - Loss: 1.7967\n",
      "Epoch 168/200 - Loss: 1.7955\n",
      "Epoch 169/200 - Loss: 1.7943\n",
      "Epoch 170/200 - Loss: 1.7932\n",
      "Epoch 171/200 - Loss: 1.7921\n",
      "Epoch 172/200 - Loss: 1.7910\n",
      "Epoch 173/200 - Loss: 1.7899\n",
      "Epoch 174/200 - Loss: 1.7888\n",
      "Epoch 175/200 - Loss: 1.7877\n",
      "Epoch 176/200 - Loss: 1.7866\n",
      "Epoch 177/200 - Loss: 1.7856\n",
      "Epoch 178/200 - Loss: 1.7846\n",
      "Epoch 179/200 - Loss: 1.7835\n",
      "Epoch 180/200 - Loss: 1.7825\n",
      "Epoch 181/200 - Loss: 1.7815\n",
      "Epoch 182/200 - Loss: 1.7805\n",
      "Epoch 183/200 - Loss: 1.7796\n",
      "Epoch 184/200 - Loss: 1.7786\n",
      "Epoch 185/200 - Loss: 1.7777\n",
      "Epoch 186/200 - Loss: 1.7767\n",
      "Epoch 187/200 - Loss: 1.7758\n",
      "Epoch 188/200 - Loss: 1.7749\n",
      "Epoch 189/200 - Loss: 1.7740\n",
      "Epoch 190/200 - Loss: 1.7731\n",
      "Epoch 191/200 - Loss: 1.7722\n",
      "Epoch 192/200 - Loss: 1.7714\n",
      "Epoch 193/200 - Loss: 1.7705\n",
      "Epoch 194/200 - Loss: 1.7697\n",
      "Epoch 195/200 - Loss: 1.7688\n",
      "Epoch 196/200 - Loss: 1.7680\n",
      "Epoch 197/200 - Loss: 1.7672\n",
      "Epoch 198/200 - Loss: 1.7664\n",
      "Epoch 199/200 - Loss: 1.7656\n",
      "Epoch 200/200 - Loss: 1.7648\n",
      "\n",
      "Saving the model outputs.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjacency matrix powers:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:  60%|██████    | 3/5 [00:00<00:00,  7.41it/s]\n",
      "Adjacency matrix powers:  80%|████████  | 4/5 [00:01<00:00,  2.95it/s]\n",
      "Adjacency matrix powers: 100%|██████████| 5/5 [00:02<00:00,  1.64it/s]\n",
      "Adjacency matrix powers: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n",
      "\n",
      "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Training Progress:   0%|          | 1/200 [00:01<03:52,  1.17s/it]\n",
      "Training Progress:   1%|          | 2/200 [00:02<03:56,  1.19s/it]\n",
      "Training Progress:   2%|▏         | 3/200 [00:03<04:00,  1.22s/it]\n",
      "Training Progress:   2%|▏         | 4/200 [00:04<03:55,  1.20s/it]\n",
      "Training Progress:   2%|▎         | 5/200 [00:05<03:53,  1.20s/it]\n",
      "Training Progress:   3%|▎         | 6/200 [00:07<03:52,  1.20s/it]\n",
      "Training Progress:   4%|▎         | 7/200 [00:08<03:50,  1.19s/it]\n",
      "Training Progress:   4%|▍         | 8/200 [00:09<03:47,  1.19s/it]\n",
      "Training Progress:   4%|▍         | 9/200 [00:10<03:46,  1.18s/it]\n",
      "Training Progress:   5%|▌         | 10/200 [00:11<03:45,  1.19s/it]\n",
      "Training Progress:   6%|▌         | 11/200 [00:13<03:43,  1.18s/it]\n",
      "Training Progress:   6%|▌         | 12/200 [00:14<03:44,  1.19s/it]\n",
      "Training Progress:   6%|▋         | 13/200 [00:15<03:42,  1.19s/it]\n",
      "Training Progress:   7%|▋         | 14/200 [00:16<03:41,  1.19s/it]\n",
      "Training Progress:   8%|▊         | 15/200 [00:17<03:40,  1.19s/it]\n",
      "Training Progress:   8%|▊         | 16/200 [00:19<03:39,  1.19s/it]\n",
      "Training Progress:   8%|▊         | 17/200 [00:20<03:37,  1.19s/it]\n",
      "Training Progress:   9%|▉         | 18/200 [00:21<03:36,  1.19s/it]\n",
      "Training Progress:  10%|▉         | 19/200 [00:22<03:35,  1.19s/it]\n",
      "Training Progress:  10%|█         | 20/200 [00:23<03:34,  1.19s/it]\n",
      "Training Progress:  10%|█         | 21/200 [00:25<03:33,  1.19s/it]\n",
      "Training Progress:  11%|█         | 22/200 [00:26<03:31,  1.19s/it]\n",
      "Training Progress:  12%|█▏        | 23/200 [00:27<03:30,  1.19s/it]\n",
      "Training Progress:  12%|█▏        | 24/200 [00:28<03:29,  1.19s/it]\n",
      "Training Progress:  12%|█▎        | 25/200 [00:29<03:28,  1.19s/it]\n",
      "Training Progress:  13%|█▎        | 26/200 [00:30<03:26,  1.19s/it]\n",
      "Training Progress:  14%|█▎        | 27/200 [00:32<03:25,  1.19s/it]\n",
      "Training Progress:  14%|█▍        | 28/200 [00:33<03:24,  1.19s/it]\n",
      "Training Progress:  14%|█▍        | 29/200 [00:34<03:22,  1.18s/it]\n",
      "Training Progress:  15%|█▌        | 30/200 [00:35<03:21,  1.19s/it]\n",
      "Training Progress:  16%|█▌        | 31/200 [00:36<03:20,  1.18s/it]\n",
      "Training Progress:  16%|█▌        | 32/200 [00:38<03:19,  1.19s/it]\n",
      "Training Progress:  16%|█▋        | 33/200 [00:39<03:17,  1.18s/it]\n",
      "Training Progress:  17%|█▋        | 34/200 [00:40<03:15,  1.18s/it]\n",
      "Training Progress:  18%|█▊        | 35/200 [00:41<03:15,  1.19s/it]\n",
      "Training Progress:  18%|█▊        | 36/200 [00:42<03:14,  1.19s/it]\n",
      "Training Progress:  18%|█▊        | 37/200 [00:43<03:13,  1.19s/it]\n",
      "Training Progress:  19%|█▉        | 38/200 [00:45<03:11,  1.18s/it]\n",
      "Training Progress:  20%|█▉        | 39/200 [00:46<03:11,  1.19s/it]\n",
      "Training Progress:  20%|██        | 40/200 [00:47<03:09,  1.18s/it]\n",
      "Training Progress:  20%|██        | 41/200 [00:48<03:08,  1.18s/it]\n",
      "Training Progress:  21%|██        | 42/200 [00:49<03:07,  1.19s/it]\n",
      "Training Progress:  22%|██▏       | 43/200 [00:51<03:05,  1.18s/it]\n",
      "Training Progress:  22%|██▏       | 44/200 [00:52<03:05,  1.19s/it]\n",
      "Training Progress:  22%|██▎       | 45/200 [00:53<03:03,  1.19s/it]\n",
      "Training Progress:  23%|██▎       | 46/200 [00:54<03:02,  1.19s/it]\n",
      "Training Progress:  24%|██▎       | 47/200 [00:55<03:00,  1.18s/it]\n",
      "Training Progress:  24%|██▍       | 48/200 [00:57<02:59,  1.18s/it]\n",
      "Training Progress:  24%|██▍       | 49/200 [00:58<02:58,  1.18s/it]\n",
      "Training Progress:  25%|██▌       | 50/200 [00:59<02:57,  1.18s/it]\n",
      "Training Progress:  26%|██▌       | 51/200 [01:00<02:55,  1.18s/it]\n",
      "Training Progress:  26%|██▌       | 52/200 [01:01<02:55,  1.19s/it]\n",
      "Training Progress:  26%|██▋       | 53/200 [01:02<02:53,  1.18s/it]\n",
      "Training Progress:  27%|██▋       | 54/200 [01:04<02:52,  1.18s/it]\n",
      "Training Progress:  28%|██▊       | 55/200 [01:05<02:51,  1.18s/it]\n",
      "Training Progress:  28%|██▊       | 56/200 [01:06<02:51,  1.19s/it]\n",
      "Training Progress:  28%|██▊       | 57/200 [01:07<02:50,  1.19s/it]\n",
      "Training Progress:  29%|██▉       | 58/200 [01:08<02:50,  1.20s/it]\n",
      "Training Progress:  30%|██▉       | 59/200 [01:10<02:48,  1.20s/it]\n",
      "Training Progress:  30%|███       | 60/200 [01:11<02:47,  1.20s/it]\n",
      "Training Progress:  30%|███       | 61/200 [01:12<02:46,  1.20s/it]\n",
      "Training Progress:  31%|███       | 62/200 [01:13<02:44,  1.19s/it]\n",
      "Training Progress:  32%|███▏      | 63/200 [01:14<02:43,  1.19s/it]\n",
      "Training Progress:  32%|███▏      | 64/200 [01:16<02:42,  1.19s/it]\n",
      "Training Progress:  32%|███▎      | 65/200 [01:17<02:41,  1.19s/it]\n",
      "Training Progress:  33%|███▎      | 66/200 [01:18<02:39,  1.19s/it]\n",
      "Training Progress:  34%|███▎      | 67/200 [01:19<02:37,  1.19s/it]\n",
      "Training Progress:  34%|███▍      | 68/200 [01:20<02:36,  1.19s/it]\n",
      "Training Progress:  34%|███▍      | 69/200 [01:22<02:35,  1.19s/it]\n",
      "Training Progress:  35%|███▌      | 70/200 [01:23<02:34,  1.19s/it]\n",
      "Training Progress:  36%|███▌      | 71/200 [01:24<02:33,  1.19s/it]\n",
      "Training Progress:  36%|███▌      | 72/200 [01:25<02:32,  1.19s/it]\n",
      "Training Progress:  36%|███▋      | 73/200 [01:26<02:31,  1.19s/it]\n",
      "Training Progress:  37%|███▋      | 74/200 [01:27<02:29,  1.19s/it]\n",
      "Training Progress:  38%|███▊      | 75/200 [01:29<02:29,  1.19s/it]\n",
      "Training Progress:  38%|███▊      | 76/200 [01:30<02:27,  1.19s/it]\n",
      "Training Progress:  38%|███▊      | 77/200 [01:31<02:26,  1.19s/it]\n",
      "Training Progress:  39%|███▉      | 78/200 [01:32<02:24,  1.19s/it]\n",
      "Training Progress:  40%|███▉      | 79/200 [01:33<02:23,  1.19s/it]\n",
      "Training Progress:  40%|████      | 80/200 [01:35<02:23,  1.19s/it]\n",
      "Training Progress:  40%|████      | 81/200 [01:36<02:21,  1.19s/it]\n",
      "Training Progress:  41%|████      | 82/200 [01:37<02:19,  1.18s/it]\n",
      "Training Progress:  42%|████▏     | 83/200 [01:38<02:18,  1.19s/it]\n",
      "Training Progress:  42%|████▏     | 84/200 [01:39<02:17,  1.18s/it]\n",
      "Training Progress:  42%|████▎     | 85/200 [01:41<02:16,  1.18s/it]\n",
      "Training Progress:  43%|████▎     | 86/200 [01:42<02:15,  1.19s/it]\n",
      "Training Progress:  44%|████▎     | 87/200 [01:43<02:14,  1.19s/it]\n",
      "Training Progress:  44%|████▍     | 88/200 [01:44<02:13,  1.19s/it]\n",
      "Training Progress:  44%|████▍     | 89/200 [01:45<02:12,  1.19s/it]\n",
      "Training Progress:  45%|████▌     | 90/200 [01:46<02:11,  1.19s/it]\n",
      "Training Progress:  46%|████▌     | 91/200 [01:48<02:10,  1.19s/it]\n",
      "Training Progress:  46%|████▌     | 92/200 [01:49<02:08,  1.19s/it]\n",
      "Training Progress:  46%|████▋     | 93/200 [01:50<02:08,  1.20s/it]\n",
      "Training Progress:  47%|████▋     | 94/200 [01:51<02:07,  1.20s/it]\n",
      "Training Progress:  48%|████▊     | 95/200 [01:52<02:05,  1.20s/it]\n",
      "Training Progress:  48%|████▊     | 96/200 [01:54<02:04,  1.19s/it]\n",
      "Training Progress:  48%|████▊     | 97/200 [01:55<02:02,  1.19s/it]\n",
      "Training Progress:  49%|████▉     | 98/200 [01:56<02:01,  1.19s/it]\n",
      "Training Progress:  50%|████▉     | 99/200 [01:57<01:59,  1.18s/it]\n",
      "Training Progress:  50%|█████     | 100/200 [01:58<01:59,  1.19s/it]\n",
      "Training Progress:  50%|█████     | 101/200 [02:00<01:57,  1.19s/it]\n",
      "Training Progress:  51%|█████     | 102/200 [02:01<01:56,  1.19s/it]\n",
      "Training Progress:  52%|█████▏    | 103/200 [02:02<01:55,  1.19s/it]\n",
      "Training Progress:  52%|█████▏    | 104/200 [02:03<01:54,  1.19s/it]\n",
      "Training Progress:  52%|█████▎    | 105/200 [02:04<01:52,  1.19s/it]\n",
      "Training Progress:  53%|█████▎    | 106/200 [02:06<01:52,  1.19s/it]\n",
      "Training Progress:  54%|█████▎    | 107/200 [02:07<01:50,  1.19s/it]\n",
      "Training Progress:  54%|█████▍    | 108/200 [02:08<01:49,  1.19s/it]\n",
      "Training Progress:  55%|█████▍    | 109/200 [02:09<01:48,  1.19s/it]\n",
      "Training Progress:  55%|█████▌    | 110/200 [02:10<01:46,  1.18s/it]\n",
      "Training Progress:  56%|█████▌    | 111/200 [02:11<01:45,  1.18s/it]\n",
      "Training Progress:  56%|█████▌    | 112/200 [02:13<01:44,  1.19s/it]\n",
      "Training Progress:  56%|█████▋    | 113/200 [02:14<01:43,  1.19s/it]\n",
      "Training Progress:  57%|█████▋    | 114/200 [02:15<01:42,  1.19s/it]\n",
      "Training Progress:  57%|█████▊    | 115/200 [02:16<01:41,  1.19s/it]\n",
      "Training Progress:  58%|█████▊    | 116/200 [02:17<01:39,  1.19s/it]\n",
      "Training Progress:  58%|█████▊    | 117/200 [02:19<01:39,  1.19s/it]\n",
      "Training Progress:  59%|█████▉    | 118/200 [02:20<01:37,  1.19s/it]\n",
      "Training Progress:  60%|█████▉    | 119/200 [02:21<01:36,  1.19s/it]\n",
      "Training Progress:  60%|██████    | 120/200 [02:22<01:35,  1.19s/it]\n",
      "Training Progress:  60%|██████    | 121/200 [02:23<01:34,  1.19s/it]\n",
      "Training Progress:  61%|██████    | 122/200 [02:25<01:33,  1.19s/it]\n",
      "Training Progress:  62%|██████▏   | 123/200 [02:26<01:31,  1.19s/it]\n",
      "Training Progress:  62%|██████▏   | 124/200 [02:27<01:30,  1.19s/it]\n",
      "Training Progress:  62%|██████▎   | 125/200 [02:28<01:29,  1.19s/it]\n",
      "Training Progress:  63%|██████▎   | 126/200 [02:29<01:28,  1.19s/it]\n",
      "Training Progress:  64%|██████▎   | 127/200 [02:31<01:27,  1.20s/it]\n",
      "Training Progress:  64%|██████▍   | 128/200 [02:32<01:26,  1.20s/it]\n",
      "Training Progress:  64%|██████▍   | 129/200 [02:33<01:24,  1.19s/it]\n",
      "Training Progress:  65%|██████▌   | 130/200 [02:34<01:23,  1.19s/it]\n",
      "Training Progress:  66%|██████▌   | 131/200 [02:35<01:22,  1.19s/it]\n",
      "Training Progress:  66%|██████▌   | 132/200 [02:37<01:21,  1.19s/it]\n",
      "Training Progress:  66%|██████▋   | 133/200 [02:38<01:20,  1.20s/it]\n",
      "Training Progress:  67%|██████▋   | 134/200 [02:39<01:18,  1.20s/it]\n",
      "Training Progress:  68%|██████▊   | 135/200 [02:40<01:19,  1.22s/it]\n",
      "Training Progress:  68%|██████▊   | 136/200 [02:41<01:18,  1.22s/it]\n",
      "Training Progress:  68%|██████▊   | 137/200 [02:43<01:16,  1.22s/it]\n",
      "Training Progress:  69%|██████▉   | 138/200 [02:44<01:15,  1.22s/it]\n",
      "Training Progress:  70%|██████▉   | 139/200 [02:45<01:15,  1.24s/it]\n",
      "Training Progress:  70%|███████   | 140/200 [02:46<01:15,  1.26s/it]\n",
      "Training Progress:  70%|███████   | 141/200 [02:48<01:15,  1.27s/it]\n",
      "Training Progress:  71%|███████   | 142/200 [02:49<01:13,  1.27s/it]\n",
      "Training Progress:  72%|███████▏  | 143/200 [02:50<01:13,  1.29s/it]\n",
      "Training Progress:  72%|███████▏  | 144/200 [02:52<01:14,  1.33s/it]\n",
      "Training Progress:  72%|███████▎  | 145/200 [02:53<01:14,  1.35s/it]\n",
      "Training Progress:  73%|███████▎  | 146/200 [02:55<01:13,  1.37s/it]\n",
      "Training Progress:  74%|███████▎  | 147/200 [02:56<01:11,  1.36s/it]\n",
      "Training Progress:  74%|███████▍  | 148/200 [02:57<01:09,  1.33s/it]\n",
      "Training Progress:  74%|███████▍  | 149/200 [02:58<01:07,  1.31s/it]\n",
      "Training Progress:  75%|███████▌  | 150/200 [03:00<01:05,  1.31s/it]\n",
      "Training Progress:  76%|███████▌  | 151/200 [03:01<01:05,  1.33s/it]\n",
      "Training Progress:  76%|███████▌  | 152/200 [03:02<01:03,  1.33s/it]\n",
      "Training Progress:  76%|███████▋  | 153/200 [03:04<01:02,  1.34s/it]\n",
      "Training Progress:  77%|███████▋  | 154/200 [03:05<01:02,  1.36s/it]\n",
      "Training Progress:  78%|███████▊  | 155/200 [03:07<01:00,  1.35s/it]\n",
      "Training Progress:  78%|███████▊  | 156/200 [03:08<00:59,  1.35s/it]\n",
      "Training Progress:  78%|███████▊  | 157/200 [03:09<00:59,  1.37s/it]\n",
      "Training Progress:  79%|███████▉  | 158/200 [03:11<00:57,  1.37s/it]\n",
      "Training Progress:  80%|███████▉  | 159/200 [03:12<00:55,  1.34s/it]\n",
      "Training Progress:  80%|████████  | 160/200 [03:13<00:52,  1.31s/it]\n",
      "Training Progress:  80%|████████  | 161/200 [03:14<00:49,  1.28s/it]\n",
      "Training Progress:  81%|████████  | 162/200 [03:16<00:48,  1.28s/it]\n",
      "Training Progress:  82%|████████▏ | 163/200 [03:17<00:46,  1.27s/it]\n",
      "Training Progress:  82%|████████▏ | 164/200 [03:18<00:45,  1.27s/it]\n",
      "Training Progress:  82%|████████▎ | 165/200 [03:19<00:44,  1.27s/it]\n",
      "Training Progress:  83%|████████▎ | 166/200 [03:21<00:42,  1.25s/it]\n",
      "Training Progress:  84%|████████▎ | 167/200 [03:22<00:41,  1.25s/it]\n",
      "Training Progress:  84%|████████▍ | 168/200 [03:23<00:40,  1.26s/it]\n",
      "Training Progress:  84%|████████▍ | 169/200 [03:24<00:38,  1.26s/it]\n",
      "Training Progress:  85%|████████▌ | 170/200 [03:26<00:37,  1.24s/it]\n",
      "Training Progress:  86%|████████▌ | 171/200 [03:27<00:36,  1.24s/it]\n",
      "Training Progress:  86%|████████▌ | 172/200 [03:28<00:34,  1.25s/it]\n",
      "Training Progress:  86%|████████▋ | 173/200 [03:29<00:33,  1.24s/it]\n",
      "Training Progress:  87%|████████▋ | 174/200 [03:31<00:32,  1.23s/it]\n",
      "Training Progress:  88%|████████▊ | 175/200 [03:32<00:30,  1.22s/it]\n",
      "Training Progress:  88%|████████▊ | 176/200 [03:33<00:29,  1.23s/it]\n",
      "Training Progress:  88%|████████▊ | 177/200 [03:35<00:29,  1.30s/it]\n",
      "Training Progress:  89%|████████▉ | 178/200 [03:36<00:29,  1.32s/it]\n",
      "Training Progress:  90%|████████▉ | 179/200 [03:37<00:27,  1.29s/it]\n",
      "Training Progress:  90%|█████████ | 180/200 [03:38<00:25,  1.28s/it]\n",
      "Training Progress:  90%|█████████ | 181/200 [03:40<00:24,  1.26s/it]\n",
      "Training Progress:  91%|█████████ | 182/200 [03:41<00:22,  1.28s/it]\n",
      "Training Progress:  92%|█████████▏| 183/200 [03:42<00:21,  1.28s/it]\n",
      "Training Progress:  92%|█████████▏| 184/200 [03:44<00:20,  1.29s/it]\n",
      "Training Progress:  92%|█████████▎| 185/200 [03:45<00:19,  1.27s/it]\n",
      "Training Progress:  93%|█████████▎| 186/200 [03:46<00:17,  1.25s/it]\n",
      "Training Progress:  94%|█████████▎| 187/200 [03:47<00:16,  1.23s/it]\n",
      "Training Progress:  94%|█████████▍| 188/200 [03:48<00:14,  1.22s/it]\n",
      "Training Progress:  94%|█████████▍| 189/200 [03:50<00:13,  1.22s/it]\n",
      "Training Progress:  95%|█████████▌| 190/200 [03:51<00:12,  1.21s/it]\n",
      "Training Progress:  96%|█████████▌| 191/200 [03:52<00:10,  1.22s/it]\n",
      "Training Progress:  96%|█████████▌| 192/200 [03:53<00:09,  1.22s/it]\n",
      "Training Progress:  96%|█████████▋| 193/200 [03:54<00:08,  1.23s/it]\n",
      "Training Progress:  97%|█████████▋| 194/200 [03:56<00:07,  1.21s/it]\n",
      "Training Progress:  98%|█████████▊| 195/200 [03:57<00:06,  1.21s/it]\n",
      "Training Progress:  98%|█████████▊| 196/200 [03:58<00:04,  1.21s/it]\n",
      "Training Progress:  98%|█████████▊| 197/200 [03:59<00:03,  1.21s/it]\n",
      "Training Progress:  99%|█████████▉| 198/200 [04:00<00:02,  1.20s/it]\n",
      "Training Progress: 100%|█████████▉| 199/200 [04:02<00:01,  1.20s/it]\n",
      "Training Progress: 100%|██████████| 200/200 [04:03<00:00,  1.20s/it]\n",
      "Training Progress: 100%|██████████| 200/200 [04:03<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/facebook_combined.csv --embedding-path ../../result/facebook_embeddings_attention.csv --attention-path ../../result/facebook_attention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c81e3",
   "metadata": {
    "id": "de1c81e3"
   },
   "source": [
    "## Test with the citation network\n",
    "Infeasible!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1dc864",
   "metadata": {
    "id": "ab1dc864",
    "outputId": "a9337ff5-b81b-407b-80a2-072176a06703"
   },
   "outputs": [],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/cit-HepTh_edges.csv --embedding-path ../../result/cit-HepTh_embeddings_attention.csv --attention-path ../../result/cit-HepTh_attention.csv --epochs 176"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P2KVPWY9ovVm",
   "metadata": {
    "id": "P2KVPWY9ovVm"
   },
   "source": [
    "## Test with the biological network\n",
    "Infeasible!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-URjbGoLozuS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-URjbGoLozuS",
    "outputId": "17f2f3d5-e0ad-49dd-ffb1-760414c60dec"
   },
   "outputs": [],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/bio-CE-CX_edges.csv --embedding-path ../../result/bio-CE-CX_embeddings_attention.csv --attention-path ....//result/bio-CE-CX_attention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac8179",
   "metadata": {},
   "source": [
    "## Test with email-Enron\n",
    "Infeasibile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f520ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/email-Enron.csv --embedding-path ../../result/email-Enron_embeddings_attention.csv --attention-path ../../result/email-Enron_attention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f6d07",
   "metadata": {},
   "source": [
    "## Test with CL-100K-1d8-L9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674d6f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------------------------------------+\n",
      "| Attention path |      ../../result/CL-100K-1d8-L9_attention.csv       |\n",
      "+================+======================================================+\n",
      "| Beta           | 0.500                                                |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Dimensions     | 128                                                  |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Edge path      | ../../data/CL-100K-1d8-L9.csv                        |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Embedding path | ../../result/CL-100K-1d8-L9_embeddings_attention.csv |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Epochs         | 200                                                  |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Gamma          | 0.500                                                |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Learning rate  | 0.010                                                |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Num of walks   | 80                                                   |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Window size    | 5                                                    |\n",
      "+----------------+------------------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjacency matrix powers:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:  40%|████      | 2/5 [00:00<00:00,  4.25it/s]\n",
      "Adjacency matrix powers:  60%|██████    | 3/5 [00:24<00:20, 10.05s/it]\n",
      "Adjacency matrix powers:  60%|██████    | 3/5 [01:46<01:11, 35.56s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\main.py\", line 19, in <module>\n",
      "    main()\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\main.py\", line 14, in main\n",
      "    model = AttentionWalkTrainer(args)\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\attentionwalk.py\", line 71, in __init__\n",
      "    self._initialize_model_and_data()\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\attentionwalk.py\", line 75, in _initialize_model_and_data\n",
      "    sparse_target_tensor = feature_calculator(self.args, self.graph)\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\utils.py\", line 56, in feature_calculator\n",
      "    powered_A = powered_A @ normalized_adjacency_matrix\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 695, in __matmul__\n",
      "    return self._matmul_dispatch(other)\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 606, in _matmul_dispatch\n",
      "    return self._matmul_sparse(other)\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py\", line 532, in _matmul_sparse\n",
      "    data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype))\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 20.9 GiB for an array with shape (5621787628,) and data type float32\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/CL-100K-1d8-L9.csv --embedding-path ../../result/CL-100K-1d8-L9_embeddings_attention.csv --attention-path ../../result/CL-100K-1d8-L9_attention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66578b",
   "metadata": {},
   "source": [
    "## Test with COX2-MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a7f122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 7964\n",
      "Number of edges: 101543\n",
      "Is the graph connected? False\n",
      "Number of connected components: 304\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "def load_graph_from_edgelist(path):\n",
    "    \"\"\"\n",
    "    Load a graph from an edge list file.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the edge list file.\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: The loaded graph.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    G = nx.from_pandas_edgelist(df, source=0, target=1)\n",
    "    return G\n",
    "\n",
    "G = load_graph_from_edgelist(\"../data/COX2-MD.csv\")\n",
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())\n",
    "\n",
    "# Check if the graph is connected\n",
    "print(\"Is the graph connected?\", nx.is_connected(G))\n",
    "if not nx.is_connected(G):\n",
    "    print(\"Number of connected components:\", nx.number_connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aed70496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------------------------------+\n",
      "| Attention path |      ../../result/COX2-MD_attention.csv       |\n",
      "+================+===============================================+\n",
      "| Beta           | 0.500                                         |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Dimensions     | 128                                           |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Edge path      | ../../data/COX2-MD.csv                        |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Embedding path | ../../result/COX2-MD_embeddings_attention.csv |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Epochs         | 200                                           |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Gamma          | 0.500                                         |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Learning rate  | 0.010                                         |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Num of walks   | 80                                            |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Window size    | 5                                             |\n",
      "+----------------+-----------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n",
      "\n",
      "Starting training.\n",
      "\n",
      "Epoch 1/200 - Loss: 56.1477\n",
      "Epoch 2/200 - Loss: 56.1165\n",
      "Epoch 3/200 - Loss: 55.7642\n",
      "Epoch 4/200 - Loss: 55.2798\n",
      "Epoch 5/200 - Loss: 54.5563\n",
      "Epoch 6/200 - Loss: 53.5235\n",
      "Epoch 7/200 - Loss: 52.1899\n",
      "Epoch 8/200 - Loss: 50.5581\n",
      "Epoch 9/200 - Loss: 48.6134\n",
      "Epoch 10/200 - Loss: 46.3438\n",
      "Epoch 11/200 - Loss: 43.7559\n",
      "Epoch 12/200 - Loss: 40.8748\n",
      "Epoch 13/200 - Loss: 37.7409\n",
      "Epoch 14/200 - Loss: 34.4084\n",
      "Epoch 15/200 - Loss: 30.9453\n",
      "Epoch 16/200 - Loss: 27.4311\n",
      "Epoch 17/200 - Loss: 23.9530\n",
      "Epoch 18/200 - Loss: 20.5994\n",
      "Epoch 19/200 - Loss: 17.4528\n",
      "Epoch 20/200 - Loss: 14.5819\n",
      "Epoch 21/200 - Loss: 12.0356\n",
      "Epoch 22/200 - Loss: 9.8388\n",
      "Epoch 23/200 - Loss: 7.9932\n",
      "Epoch 24/200 - Loss: 6.4797\n",
      "Epoch 25/200 - Loss: 5.2651\n",
      "Epoch 26/200 - Loss: 4.3079\n",
      "Epoch 27/200 - Loss: 3.5644\n",
      "Epoch 28/200 - Loss: 2.9932\n",
      "Epoch 29/200 - Loss: 2.5579\n",
      "Epoch 30/200 - Loss: 2.2277\n",
      "Epoch 31/200 - Loss: 1.9777\n",
      "Epoch 32/200 - Loss: 1.7886\n",
      "Epoch 33/200 - Loss: 1.6454\n",
      "Epoch 34/200 - Loss: 1.5366\n",
      "Epoch 35/200 - Loss: 1.4538\n",
      "Epoch 36/200 - Loss: 1.3904\n",
      "Epoch 37/200 - Loss: 1.3418\n",
      "Epoch 38/200 - Loss: 1.3043\n",
      "Epoch 39/200 - Loss: 1.2754\n",
      "Epoch 40/200 - Loss: 1.2529\n",
      "Epoch 41/200 - Loss: 1.2353\n",
      "Epoch 42/200 - Loss: 1.2217\n",
      "Epoch 43/200 - Loss: 1.2109\n",
      "Epoch 44/200 - Loss: 1.2025\n",
      "Epoch 45/200 - Loss: 1.1959\n",
      "Epoch 46/200 - Loss: 1.1907\n",
      "Epoch 47/200 - Loss: 1.1866\n",
      "Epoch 48/200 - Loss: 1.1834\n",
      "Epoch 49/200 - Loss: 1.1808\n",
      "Epoch 50/200 - Loss: 1.1788\n",
      "Epoch 51/200 - Loss: 1.1773\n",
      "Epoch 52/200 - Loss: 1.1760\n",
      "Epoch 53/200 - Loss: 1.1750\n",
      "Epoch 54/200 - Loss: 1.1743\n",
      "Epoch 55/200 - Loss: 1.1737\n",
      "Epoch 56/200 - Loss: 1.1732\n",
      "Epoch 57/200 - Loss: 1.1728\n",
      "Epoch 58/200 - Loss: 1.1726\n",
      "Epoch 59/200 - Loss: 1.1723\n",
      "Epoch 60/200 - Loss: 1.1721\n",
      "Epoch 61/200 - Loss: 1.1720\n",
      "Epoch 62/200 - Loss: 1.1719\n",
      "Epoch 63/200 - Loss: 1.1718\n",
      "Epoch 64/200 - Loss: 1.1717\n",
      "Epoch 65/200 - Loss: 1.1716\n",
      "Epoch 66/200 - Loss: 1.1716\n",
      "Epoch 67/200 - Loss: 1.1715\n",
      "Epoch 68/200 - Loss: 1.1714\n",
      "Epoch 69/200 - Loss: 1.1714\n",
      "Epoch 70/200 - Loss: 1.1713\n",
      "Epoch 71/200 - Loss: 1.1713\n",
      "Epoch 72/200 - Loss: 1.1712\n",
      "Epoch 73/200 - Loss: 1.1711\n",
      "Epoch 74/200 - Loss: 1.1711\n",
      "Epoch 75/200 - Loss: 1.1710\n",
      "Epoch 76/200 - Loss: 1.1709\n",
      "Epoch 77/200 - Loss: 1.1708\n",
      "Epoch 78/200 - Loss: 1.1708\n",
      "Epoch 79/200 - Loss: 1.1707\n",
      "Epoch 80/200 - Loss: 1.1706\n",
      "Epoch 81/200 - Loss: 1.1705\n",
      "Epoch 82/200 - Loss: 1.1704\n",
      "Epoch 83/200 - Loss: 1.1703\n",
      "Epoch 84/200 - Loss: 1.1702\n",
      "Epoch 85/200 - Loss: 1.1701\n",
      "Epoch 86/200 - Loss: 1.1700\n",
      "Epoch 87/200 - Loss: 1.1699\n",
      "Epoch 88/200 - Loss: 1.1698\n",
      "Epoch 89/200 - Loss: 1.1697\n",
      "Epoch 90/200 - Loss: 1.1696\n",
      "Epoch 91/200 - Loss: 1.1695\n",
      "Epoch 92/200 - Loss: 1.1694\n",
      "Epoch 93/200 - Loss: 1.1693\n",
      "Epoch 94/200 - Loss: 1.1692\n",
      "Epoch 95/200 - Loss: 1.1691\n",
      "Epoch 96/200 - Loss: 1.1689\n",
      "Epoch 97/200 - Loss: 1.1688\n",
      "Epoch 98/200 - Loss: 1.1687\n",
      "Epoch 99/200 - Loss: 1.1686\n",
      "Epoch 100/200 - Loss: 1.1685\n",
      "Epoch 101/200 - Loss: 1.1684\n",
      "Epoch 102/200 - Loss: 1.1683\n",
      "Epoch 103/200 - Loss: 1.1682\n",
      "Epoch 104/200 - Loss: 1.1681\n",
      "Epoch 105/200 - Loss: 1.1680\n",
      "Epoch 106/200 - Loss: 1.1679\n",
      "Epoch 107/200 - Loss: 1.1677\n",
      "Epoch 108/200 - Loss: 1.1676\n",
      "Epoch 109/200 - Loss: 1.1675\n",
      "Epoch 110/200 - Loss: 1.1674\n",
      "Epoch 111/200 - Loss: 1.1673\n",
      "Epoch 112/200 - Loss: 1.1672\n",
      "Epoch 113/200 - Loss: 1.1671\n",
      "Epoch 114/200 - Loss: 1.1670\n",
      "Epoch 115/200 - Loss: 1.1669\n",
      "Epoch 116/200 - Loss: 1.1668\n",
      "Epoch 117/200 - Loss: 1.1667\n",
      "Epoch 118/200 - Loss: 1.1666\n",
      "Epoch 119/200 - Loss: 1.1665\n",
      "Epoch 120/200 - Loss: 1.1664\n",
      "Epoch 121/200 - Loss: 1.1663\n",
      "Epoch 122/200 - Loss: 1.1662\n",
      "Epoch 123/200 - Loss: 1.1661\n",
      "Epoch 124/200 - Loss: 1.1660\n",
      "Epoch 125/200 - Loss: 1.1659\n",
      "Epoch 126/200 - Loss: 1.1658\n",
      "Epoch 127/200 - Loss: 1.1657\n",
      "Epoch 128/200 - Loss: 1.1656\n",
      "Epoch 129/200 - Loss: 1.1655\n",
      "Epoch 130/200 - Loss: 1.1654\n",
      "Epoch 131/200 - Loss: 1.1653\n",
      "Epoch 132/200 - Loss: 1.1652\n",
      "Epoch 133/200 - Loss: 1.1651\n",
      "Epoch 134/200 - Loss: 1.1650\n",
      "Epoch 135/200 - Loss: 1.1649\n",
      "Epoch 136/200 - Loss: 1.1649\n",
      "Epoch 137/200 - Loss: 1.1648\n",
      "Epoch 138/200 - Loss: 1.1647\n",
      "Epoch 139/200 - Loss: 1.1646\n",
      "Epoch 140/200 - Loss: 1.1645\n",
      "Epoch 141/200 - Loss: 1.1644\n",
      "Epoch 142/200 - Loss: 1.1643\n",
      "Epoch 143/200 - Loss: 1.1642\n",
      "Epoch 144/200 - Loss: 1.1642\n",
      "Epoch 145/200 - Loss: 1.1641\n",
      "Epoch 146/200 - Loss: 1.1640\n",
      "Epoch 147/200 - Loss: 1.1639\n",
      "Epoch 148/200 - Loss: 1.1638\n",
      "Epoch 149/200 - Loss: 1.1637\n",
      "Epoch 150/200 - Loss: 1.1636\n",
      "Epoch 151/200 - Loss: 1.1636\n",
      "Epoch 152/200 - Loss: 1.1635\n",
      "Epoch 153/200 - Loss: 1.1634\n",
      "Epoch 154/200 - Loss: 1.1633\n",
      "Epoch 155/200 - Loss: 1.1632\n",
      "Epoch 156/200 - Loss: 1.1632\n",
      "Epoch 157/200 - Loss: 1.1631\n",
      "Epoch 158/200 - Loss: 1.1630\n",
      "Epoch 159/200 - Loss: 1.1629\n",
      "Epoch 160/200 - Loss: 1.1629\n",
      "Epoch 161/200 - Loss: 1.1628\n",
      "Epoch 162/200 - Loss: 1.1627\n",
      "Epoch 163/200 - Loss: 1.1626\n",
      "Epoch 164/200 - Loss: 1.1626\n",
      "Epoch 165/200 - Loss: 1.1625\n",
      "Epoch 166/200 - Loss: 1.1624\n",
      "Epoch 167/200 - Loss: 1.1623\n",
      "Epoch 168/200 - Loss: 1.1623\n",
      "Epoch 169/200 - Loss: 1.1622\n",
      "Epoch 170/200 - Loss: 1.1621\n",
      "Epoch 171/200 - Loss: 1.1621\n",
      "Epoch 172/200 - Loss: 1.1620\n",
      "Epoch 173/200 - Loss: 1.1619\n",
      "Epoch 174/200 - Loss: 1.1618\n",
      "Epoch 175/200 - Loss: 1.1618\n",
      "Epoch 176/200 - Loss: 1.1617\n",
      "Epoch 177/200 - Loss: 1.1616\n",
      "Epoch 178/200 - Loss: 1.1616\n",
      "Epoch 179/200 - Loss: 1.1615\n",
      "Epoch 180/200 - Loss: 1.1614\n",
      "Epoch 181/200 - Loss: 1.1614\n",
      "Epoch 182/200 - Loss: 1.1613\n",
      "Epoch 183/200 - Loss: 1.1612\n",
      "Epoch 184/200 - Loss: 1.1612\n",
      "Epoch 185/200 - Loss: 1.1611\n",
      "Epoch 186/200 - Loss: 1.1610\n",
      "Epoch 187/200 - Loss: 1.1610\n",
      "Epoch 188/200 - Loss: 1.1609\n",
      "Epoch 189/200 - Loss: 1.1608\n",
      "Epoch 190/200 - Loss: 1.1608\n",
      "Epoch 191/200 - Loss: 1.1607\n",
      "Epoch 192/200 - Loss: 1.1607\n",
      "Epoch 193/200 - Loss: 1.1606\n",
      "Epoch 194/200 - Loss: 1.1605\n",
      "Epoch 195/200 - Loss: 1.1605\n",
      "Epoch 196/200 - Loss: 1.1604\n",
      "Epoch 197/200 - Loss: 1.1603\n",
      "Epoch 198/200 - Loss: 1.1603\n",
      "Epoch 199/200 - Loss: 1.1602\n",
      "Epoch 200/200 - Loss: 1.1602\n",
      "\n",
      "Saving the model outputs.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjacency matrix powers:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers: 100%|██████████| 5/5 [00:00<00:00, 138.89it/s]\n",
      "\n",
      "Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Training Progress:   0%|          | 1/200 [00:04<13:57,  4.21s/it]\n",
      "Training Progress:   1%|          | 2/200 [00:07<11:45,  3.56s/it]\n",
      "Training Progress:   2%|▏         | 3/200 [00:10<11:03,  3.37s/it]\n",
      "Training Progress:   2%|▏         | 4/200 [00:13<10:29,  3.21s/it]\n",
      "Training Progress:   2%|▎         | 5/200 [00:16<10:12,  3.14s/it]\n",
      "Training Progress:   3%|▎         | 6/200 [00:19<09:58,  3.08s/it]\n",
      "Training Progress:   4%|▎         | 7/200 [00:22<09:48,  3.05s/it]\n",
      "Training Progress:   4%|▍         | 8/200 [00:25<09:57,  3.11s/it]\n",
      "Training Progress:   4%|▍         | 9/200 [00:28<09:55,  3.12s/it]\n",
      "Training Progress:   5%|▌         | 10/200 [00:31<09:55,  3.13s/it]\n",
      "Training Progress:   6%|▌         | 11/200 [00:35<09:50,  3.12s/it]\n",
      "Training Progress:   6%|▌         | 12/200 [00:38<09:42,  3.10s/it]\n",
      "Training Progress:   6%|▋         | 13/200 [00:41<09:30,  3.05s/it]\n",
      "Training Progress:   7%|▋         | 14/200 [00:43<09:20,  3.01s/it]\n",
      "Training Progress:   8%|▊         | 15/200 [00:46<09:11,  2.98s/it]\n",
      "Training Progress:   8%|▊         | 16/200 [00:49<08:58,  2.93s/it]\n",
      "Training Progress:   8%|▊         | 17/200 [00:52<08:48,  2.89s/it]\n",
      "Training Progress:   9%|▉         | 18/200 [00:55<08:39,  2.85s/it]\n",
      "Training Progress:  10%|▉         | 19/200 [00:58<08:37,  2.86s/it]\n",
      "Training Progress:  10%|█         | 20/200 [01:01<08:38,  2.88s/it]\n",
      "Training Progress:  10%|█         | 21/200 [01:03<08:31,  2.86s/it]\n",
      "Training Progress:  11%|█         | 22/200 [01:06<08:25,  2.84s/it]\n",
      "Training Progress:  12%|█▏        | 23/200 [01:09<08:18,  2.81s/it]\n",
      "Training Progress:  12%|█▏        | 24/200 [01:12<08:13,  2.80s/it]\n",
      "Training Progress:  12%|█▎        | 25/200 [01:14<08:07,  2.79s/it]\n",
      "Training Progress:  13%|█▎        | 26/200 [01:17<08:02,  2.77s/it]\n",
      "Training Progress:  14%|█▎        | 27/200 [01:20<07:59,  2.77s/it]\n",
      "Training Progress:  14%|█▍        | 28/200 [01:23<07:56,  2.77s/it]\n",
      "Training Progress:  14%|█▍        | 29/200 [01:25<07:52,  2.76s/it]\n",
      "Training Progress:  15%|█▌        | 30/200 [01:28<07:49,  2.76s/it]\n",
      "Training Progress:  16%|█▌        | 31/200 [01:31<07:46,  2.76s/it]\n",
      "Training Progress:  16%|█▌        | 32/200 [01:34<07:43,  2.76s/it]\n",
      "Training Progress:  16%|█▋        | 33/200 [01:36<07:41,  2.76s/it]\n",
      "Training Progress:  17%|█▋        | 34/200 [01:39<07:39,  2.77s/it]\n",
      "Training Progress:  18%|█▊        | 35/200 [01:42<07:38,  2.78s/it]\n",
      "Training Progress:  18%|█▊        | 36/200 [01:45<07:35,  2.78s/it]\n",
      "Training Progress:  18%|█▊        | 37/200 [01:48<07:32,  2.78s/it]\n",
      "Training Progress:  19%|█▉        | 38/200 [01:50<07:31,  2.79s/it]\n",
      "Training Progress:  20%|█▉        | 39/200 [01:53<07:27,  2.78s/it]\n",
      "Training Progress:  20%|██        | 40/200 [01:56<07:24,  2.78s/it]\n",
      "Training Progress:  20%|██        | 41/200 [01:59<07:21,  2.78s/it]\n",
      "Training Progress:  21%|██        | 42/200 [02:02<07:18,  2.78s/it]\n",
      "Training Progress:  22%|██▏       | 43/200 [02:04<07:15,  2.77s/it]\n",
      "Training Progress:  22%|██▏       | 44/200 [02:07<07:12,  2.77s/it]\n",
      "Training Progress:  22%|██▎       | 45/200 [02:10<07:10,  2.78s/it]\n",
      "Training Progress:  23%|██▎       | 46/200 [02:13<07:07,  2.78s/it]\n",
      "Training Progress:  24%|██▎       | 47/200 [02:15<07:04,  2.77s/it]\n",
      "Training Progress:  24%|██▍       | 48/200 [02:18<07:01,  2.77s/it]\n",
      "Training Progress:  24%|██▍       | 49/200 [02:21<06:58,  2.77s/it]\n",
      "Training Progress:  25%|██▌       | 50/200 [02:24<06:55,  2.77s/it]\n",
      "Training Progress:  26%|██▌       | 51/200 [02:26<06:51,  2.76s/it]\n",
      "Training Progress:  26%|██▌       | 52/200 [02:29<06:50,  2.77s/it]\n",
      "Training Progress:  26%|██▋       | 53/200 [02:32<06:54,  2.82s/it]\n",
      "Training Progress:  27%|██▋       | 54/200 [02:35<07:02,  2.89s/it]\n",
      "Training Progress:  28%|██▊       | 55/200 [02:38<07:00,  2.90s/it]\n",
      "Training Progress:  28%|██▊       | 56/200 [02:41<06:56,  2.90s/it]\n",
      "Training Progress:  28%|██▊       | 57/200 [02:44<06:49,  2.86s/it]\n",
      "Training Progress:  29%|██▉       | 58/200 [02:47<06:41,  2.83s/it]\n",
      "Training Progress:  30%|██▉       | 59/200 [02:49<06:38,  2.83s/it]\n",
      "Training Progress:  30%|███       | 60/200 [02:52<06:35,  2.82s/it]\n",
      "Training Progress:  30%|███       | 61/200 [02:55<06:31,  2.82s/it]\n",
      "Training Progress:  31%|███       | 62/200 [02:58<06:26,  2.80s/it]\n",
      "Training Progress:  32%|███▏      | 63/200 [03:01<06:23,  2.80s/it]\n",
      "Training Progress:  32%|███▏      | 64/200 [03:03<06:18,  2.79s/it]\n",
      "Training Progress:  32%|███▎      | 65/200 [03:06<06:15,  2.78s/it]\n",
      "Training Progress:  33%|███▎      | 66/200 [03:09<06:11,  2.78s/it]\n",
      "Training Progress:  34%|███▎      | 67/200 [03:12<06:08,  2.77s/it]\n",
      "Training Progress:  34%|███▍      | 68/200 [03:14<06:06,  2.78s/it]\n",
      "Training Progress:  34%|███▍      | 69/200 [03:17<06:02,  2.77s/it]\n",
      "Training Progress:  35%|███▌      | 70/200 [03:20<06:01,  2.78s/it]\n",
      "Training Progress:  36%|███▌      | 71/200 [03:23<05:58,  2.78s/it]\n",
      "Training Progress:  36%|███▌      | 72/200 [03:25<05:55,  2.77s/it]\n",
      "Training Progress:  36%|███▋      | 73/200 [03:28<05:52,  2.77s/it]\n",
      "Training Progress:  37%|███▋      | 74/200 [03:31<05:51,  2.79s/it]\n",
      "Training Progress:  38%|███▊      | 75/200 [03:34<05:48,  2.79s/it]\n",
      "Training Progress:  38%|███▊      | 76/200 [03:37<05:45,  2.78s/it]\n",
      "Training Progress:  38%|███▊      | 77/200 [03:39<05:41,  2.78s/it]\n",
      "Training Progress:  39%|███▉      | 78/200 [03:42<05:38,  2.78s/it]\n",
      "Training Progress:  40%|███▉      | 79/200 [03:45<05:39,  2.80s/it]\n",
      "Training Progress:  40%|████      | 80/200 [03:48<05:38,  2.82s/it]\n",
      "Training Progress:  40%|████      | 81/200 [03:51<05:34,  2.81s/it]\n",
      "Training Progress:  41%|████      | 82/200 [03:53<05:29,  2.79s/it]\n",
      "Training Progress:  42%|████▏     | 83/200 [03:56<05:25,  2.78s/it]\n",
      "Training Progress:  42%|████▏     | 84/200 [03:59<05:22,  2.78s/it]\n",
      "Training Progress:  42%|████▎     | 85/200 [04:02<05:21,  2.79s/it]\n",
      "Training Progress:  43%|████▎     | 86/200 [04:05<05:18,  2.79s/it]\n",
      "Training Progress:  44%|████▎     | 87/200 [04:07<05:14,  2.78s/it]\n",
      "Training Progress:  44%|████▍     | 88/200 [04:10<05:10,  2.78s/it]\n",
      "Training Progress:  44%|████▍     | 89/200 [04:13<05:08,  2.77s/it]\n",
      "Training Progress:  45%|████▌     | 90/200 [04:16<05:04,  2.77s/it]\n",
      "Training Progress:  46%|████▌     | 91/200 [04:18<05:02,  2.77s/it]\n",
      "Training Progress:  46%|████▌     | 92/200 [04:21<04:58,  2.76s/it]\n",
      "Training Progress:  46%|████▋     | 93/200 [04:24<04:55,  2.77s/it]\n",
      "Training Progress:  47%|████▋     | 94/200 [04:27<04:54,  2.78s/it]\n",
      "Training Progress:  48%|████▊     | 95/200 [04:29<04:51,  2.77s/it]\n",
      "Training Progress:  48%|████▊     | 96/200 [04:32<04:50,  2.80s/it]\n",
      "Training Progress:  48%|████▊     | 97/200 [04:35<04:48,  2.80s/it]\n",
      "Training Progress:  49%|████▉     | 98/200 [04:38<04:44,  2.79s/it]\n",
      "Training Progress:  50%|████▉     | 99/200 [04:41<04:41,  2.78s/it]\n",
      "Training Progress:  50%|█████     | 100/200 [04:43<04:38,  2.79s/it]\n",
      "Training Progress:  50%|█████     | 101/200 [04:46<04:35,  2.79s/it]\n",
      "Training Progress:  51%|█████     | 102/200 [04:49<04:33,  2.79s/it]\n",
      "Training Progress:  52%|█████▏    | 103/200 [04:52<04:30,  2.78s/it]\n",
      "Training Progress:  52%|█████▏    | 104/200 [04:55<04:28,  2.79s/it]\n",
      "Training Progress:  52%|█████▎    | 105/200 [04:57<04:23,  2.78s/it]\n",
      "Training Progress:  53%|█████▎    | 106/200 [05:00<04:21,  2.78s/it]\n",
      "Training Progress:  54%|█████▎    | 107/200 [05:03<04:19,  2.79s/it]\n",
      "Training Progress:  54%|█████▍    | 108/200 [05:06<04:16,  2.79s/it]\n",
      "Training Progress:  55%|█████▍    | 109/200 [05:09<04:21,  2.88s/it]\n",
      "Training Progress:  55%|█████▌    | 110/200 [05:12<04:22,  2.92s/it]\n",
      "Training Progress:  56%|█████▌    | 111/200 [05:15<04:23,  2.96s/it]\n",
      "Training Progress:  56%|█████▌    | 112/200 [05:18<04:15,  2.91s/it]\n",
      "Training Progress:  56%|█████▋    | 113/200 [05:20<04:09,  2.86s/it]\n",
      "Training Progress:  57%|█████▋    | 114/200 [05:23<04:02,  2.82s/it]\n",
      "Training Progress:  57%|█████▊    | 115/200 [05:26<03:59,  2.81s/it]\n",
      "Training Progress:  58%|█████▊    | 116/200 [05:29<03:54,  2.79s/it]\n",
      "Training Progress:  58%|█████▊    | 117/200 [05:31<03:50,  2.78s/it]\n",
      "Training Progress:  59%|█████▉    | 118/200 [05:34<03:47,  2.78s/it]\n",
      "Training Progress:  60%|█████▉    | 119/200 [05:37<03:44,  2.77s/it]\n",
      "Training Progress:  60%|██████    | 120/200 [05:40<03:41,  2.77s/it]\n",
      "Training Progress:  60%|██████    | 121/200 [05:43<03:38,  2.76s/it]\n",
      "Training Progress:  61%|██████    | 122/200 [05:45<03:36,  2.77s/it]\n",
      "Training Progress:  62%|██████▏   | 123/200 [05:48<03:33,  2.77s/it]\n",
      "Training Progress:  62%|██████▏   | 124/200 [05:51<03:30,  2.77s/it]\n",
      "Training Progress:  62%|██████▎   | 125/200 [05:54<03:27,  2.76s/it]\n",
      "Training Progress:  63%|██████▎   | 126/200 [05:56<03:23,  2.76s/it]\n",
      "Training Progress:  64%|██████▎   | 127/200 [05:59<03:21,  2.76s/it]\n",
      "Training Progress:  64%|██████▍   | 128/200 [06:02<03:18,  2.76s/it]\n",
      "Training Progress:  64%|██████▍   | 129/200 [06:05<03:15,  2.75s/it]\n",
      "Training Progress:  65%|██████▌   | 130/200 [06:07<03:12,  2.75s/it]\n",
      "Training Progress:  66%|██████▌   | 131/200 [06:10<03:09,  2.75s/it]\n",
      "Training Progress:  66%|██████▌   | 132/200 [06:13<03:07,  2.76s/it]\n",
      "Training Progress:  66%|██████▋   | 133/200 [06:16<03:05,  2.77s/it]\n",
      "Training Progress:  67%|██████▋   | 134/200 [06:18<03:02,  2.77s/it]\n",
      "Training Progress:  68%|██████▊   | 135/200 [06:21<02:59,  2.76s/it]\n",
      "Training Progress:  68%|██████▊   | 136/200 [06:24<02:56,  2.76s/it]\n",
      "Training Progress:  68%|██████▊   | 137/200 [06:27<02:53,  2.76s/it]\n",
      "Training Progress:  69%|██████▉   | 138/200 [06:29<02:50,  2.76s/it]\n",
      "Training Progress:  70%|██████▉   | 139/200 [06:32<02:47,  2.75s/it]\n",
      "Training Progress:  70%|███████   | 140/200 [06:35<02:45,  2.76s/it]\n",
      "Training Progress:  70%|███████   | 141/200 [06:38<02:43,  2.76s/it]\n",
      "Training Progress:  71%|███████   | 142/200 [06:40<02:39,  2.75s/it]\n",
      "Training Progress:  72%|███████▏  | 143/200 [06:43<02:37,  2.76s/it]\n",
      "Training Progress:  72%|███████▏  | 144/200 [06:46<02:34,  2.76s/it]\n",
      "Training Progress:  72%|███████▎  | 145/200 [06:49<02:32,  2.77s/it]\n",
      "Training Progress:  73%|███████▎  | 146/200 [06:52<02:28,  2.75s/it]\n",
      "Training Progress:  74%|███████▎  | 147/200 [06:54<02:25,  2.75s/it]\n",
      "Training Progress:  74%|███████▍  | 148/200 [06:57<02:23,  2.76s/it]\n",
      "Training Progress:  74%|███████▍  | 149/200 [07:00<02:19,  2.74s/it]\n",
      "Training Progress:  75%|███████▌  | 150/200 [07:02<02:17,  2.75s/it]\n",
      "Training Progress:  76%|███████▌  | 151/200 [07:05<02:13,  2.72s/it]\n",
      "Training Progress:  76%|███████▌  | 152/200 [07:08<02:11,  2.74s/it]\n",
      "Training Progress:  76%|███████▋  | 153/200 [07:11<02:09,  2.75s/it]\n",
      "Training Progress:  77%|███████▋  | 154/200 [07:13<02:06,  2.76s/it]\n",
      "Training Progress:  78%|███████▊  | 155/200 [07:16<02:04,  2.76s/it]\n",
      "Training Progress:  78%|███████▊  | 156/200 [07:19<02:01,  2.76s/it]\n",
      "Training Progress:  78%|███████▊  | 157/200 [07:22<01:58,  2.76s/it]\n",
      "Training Progress:  79%|███████▉  | 158/200 [07:25<01:55,  2.75s/it]\n",
      "Training Progress:  80%|███████▉  | 159/200 [07:27<01:52,  2.75s/it]\n",
      "Training Progress:  80%|████████  | 160/200 [07:30<01:50,  2.75s/it]\n",
      "Training Progress:  80%|████████  | 161/200 [07:33<01:48,  2.79s/it]\n",
      "Training Progress:  81%|████████  | 162/200 [07:36<01:46,  2.79s/it]\n",
      "Training Progress:  82%|████████▏ | 163/200 [07:38<01:43,  2.78s/it]\n",
      "Training Progress:  82%|████████▏ | 164/200 [07:41<01:39,  2.77s/it]\n",
      "Training Progress:  82%|████████▎ | 165/200 [07:44<01:37,  2.78s/it]\n",
      "Training Progress:  83%|████████▎ | 166/200 [07:47<01:34,  2.77s/it]\n",
      "Training Progress:  84%|████████▎ | 167/200 [07:50<01:31,  2.77s/it]\n",
      "Training Progress:  84%|████████▍ | 168/200 [07:52<01:28,  2.77s/it]\n",
      "Training Progress:  84%|████████▍ | 169/200 [07:55<01:25,  2.76s/it]\n",
      "Training Progress:  85%|████████▌ | 170/200 [07:58<01:22,  2.76s/it]\n",
      "Training Progress:  86%|████████▌ | 171/200 [08:01<01:20,  2.76s/it]\n",
      "Training Progress:  86%|████████▌ | 172/200 [08:03<01:17,  2.76s/it]\n",
      "Training Progress:  86%|████████▋ | 173/200 [08:06<01:14,  2.76s/it]\n",
      "Training Progress:  87%|████████▋ | 174/200 [08:09<01:11,  2.75s/it]\n",
      "Training Progress:  88%|████████▊ | 175/200 [08:12<01:08,  2.75s/it]\n",
      "Training Progress:  88%|████████▊ | 176/200 [08:14<01:06,  2.76s/it]\n",
      "Training Progress:  88%|████████▊ | 177/200 [08:17<01:03,  2.76s/it]\n",
      "Training Progress:  89%|████████▉ | 178/200 [08:20<01:00,  2.76s/it]\n",
      "Training Progress:  90%|████████▉ | 179/200 [08:23<00:58,  2.77s/it]\n",
      "Training Progress:  90%|█████████ | 180/200 [08:25<00:55,  2.77s/it]\n",
      "Training Progress:  90%|█████████ | 181/200 [08:28<00:52,  2.76s/it]\n",
      "Training Progress:  91%|█████████ | 182/200 [08:31<00:49,  2.76s/it]\n",
      "Training Progress:  92%|█████████▏| 183/200 [08:34<00:47,  2.77s/it]\n",
      "Training Progress:  92%|█████████▏| 184/200 [08:37<00:44,  2.78s/it]\n",
      "Training Progress:  92%|█████████▎| 185/200 [08:39<00:41,  2.77s/it]\n",
      "Training Progress:  93%|█████████▎| 186/200 [08:42<00:38,  2.77s/it]\n",
      "Training Progress:  94%|█████████▎| 187/200 [08:45<00:35,  2.77s/it]\n",
      "Training Progress:  94%|█████████▍| 188/200 [08:48<00:33,  2.77s/it]\n",
      "Training Progress:  94%|█████████▍| 189/200 [08:50<00:30,  2.78s/it]\n",
      "Training Progress:  95%|█████████▌| 190/200 [08:53<00:27,  2.78s/it]\n",
      "Training Progress:  96%|█████████▌| 191/200 [08:56<00:24,  2.77s/it]\n",
      "Training Progress:  96%|█████████▌| 192/200 [08:59<00:22,  2.77s/it]\n",
      "Training Progress:  96%|█████████▋| 193/200 [09:01<00:19,  2.77s/it]\n",
      "Training Progress:  97%|█████████▋| 194/200 [09:04<00:16,  2.78s/it]\n",
      "Training Progress:  98%|█████████▊| 195/200 [09:07<00:13,  2.78s/it]\n",
      "Training Progress:  98%|█████████▊| 196/200 [09:10<00:11,  2.78s/it]\n",
      "Training Progress:  98%|█████████▊| 197/200 [09:13<00:08,  2.78s/it]\n",
      "Training Progress:  99%|█████████▉| 198/200 [09:15<00:05,  2.78s/it]\n",
      "Training Progress: 100%|█████████▉| 199/200 [09:18<00:02,  2.77s/it]\n",
      "Training Progress: 100%|██████████| 200/200 [09:21<00:00,  2.77s/it]\n",
      "Training Progress: 100%|██████████| 200/200 [09:21<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/COX2-MD.csv --embedding-path ../../result/COX2-MD_embeddings_attention.csv --attention-path ../../result/COX2-MD_attention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a558d7",
   "metadata": {},
   "source": [
    "# GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd gae && python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95228f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd gae/gae && python train.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
