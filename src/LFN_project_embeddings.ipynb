{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d121cbd5",
   "metadata": {
    "id": "d121cbd5"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PietroVolpato/lfn_project/blob/main/src/LFN_project_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358080be-e62b-4a82-b8d3-e5becb180aac",
   "metadata": {
    "id": "358080be-e62b-4a82-b8d3-e5becb180aac"
   },
   "source": [
    "# Learning from networks project\n",
    "### Evaluation of different Node Embedding algorithms\n",
    "Members:<br>\n",
    "- D'Emilio Filippo, id : 2120931\n",
    "- Volpato Pietro, id : 2120825\n",
    "\n",
    "### Information about the notebook (have a look at the report for details)\n",
    "This notebook are computed the embeddings for every selected using 3 different embedding algorithms.<br>\n",
    "Each embedding is saved to file as a numpy array (extension .npy), in the directory /results. In this way once an embedding is computed, it won't be lost when the runtime of the notebook is terminated.<br>\n",
    "We can then efficiently load the embeddings in the \"test\" notebook, and evaluate the quality of the embeddings.<br>\n",
    "Selected embedding techniques:\n",
    "- Node2Vec\n",
    "- Line\n",
    "- Attention Walk\n",
    "\n",
    "For information about the graphs, se cells below.<br>\n",
    "*NOTE*: by implementation choice, the computation of each embedding is computed separately (there are no function to coincisely compute all embeddings).<br>\n",
    "This choice comes from the fact that computing embeddings is computationally intensive, and we might want to compute only a specific embedding strategy for a specific graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d3b60-f774-4fc7-b0bc-7617057d6459",
   "metadata": {
    "id": "bb1d3b60-f774-4fc7-b0bc-7617057d6459"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93987c53-ecce-41d6-948b-4a4b7ac4978a",
   "metadata": {
    "id": "93987c53-ecce-41d6-948b-4a4b7ac4978a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gzip\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b64818-de78-4c30-945e-b95279c7f1c9",
   "metadata": {
    "id": "d6b64818-de78-4c30-945e-b95279c7f1c9"
   },
   "source": [
    "# configuration\n",
    "Here you can properly configure the names of the graphs and the names of the embedding strategies. Use meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5b26d0-84d0-46ad-83dd-ac826faac4fe",
   "metadata": {
    "id": "6e5b26d0-84d0-46ad-83dd-ac826faac4fe"
   },
   "outputs": [],
   "source": [
    "graph_keys = [\"facebook\",\"citation\",\"biological\",\"proteins\", \"spam\"]\n",
    "embedding_keys = [\"LINE\", \"node2vec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6ca3e-b5f5-4869-a3ef-47fb66cd7c56",
   "metadata": {
    "id": "79f6ca3e-b5f5-4869-a3ef-47fb66cd7c56"
   },
   "source": [
    "# Loading the graphs\n",
    "Selected graphs:\n",
    "- Facebook_combined    https://snap.stanford.edu/data/ego-Facebook.html          \n",
    "- cit-Helpth           https://networkrepository.com/cit-HepTh.php             \n",
    "- bio-CE-CX            https://networkrepository.com/bio-CE-CX.php             \n",
    "- proteins-full        https://networkrepository.com/PROTEINS-full.php ---- the graph has node labels\n",
    "- COX2-MD              https://networkrepository.com/COX2-MD.php  ---- the graph has node labels\n",
    "\n",
    "To run this notebook, adjust the paths to match where the files are saved in your PC.<br>\n",
    "To keep paths as they are, create a \"data\" folder inside the directory of this notebook, and store the files there.<br>\n",
    "Graphs are stored as a dictionary: the key is the graph name, the value is the corresponding netowrkx graph.<br>\n",
    "\n",
    "When it is created a networkX graph from a text file the node are renamed as integers form 0 to |V|-1, so that we can store the embeddings\n",
    "on a matrix, and each row index corresponds to the embedding vector of the corrisponding node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "TtpPrXlsIUgJ",
   "metadata": {
    "id": "TtpPrXlsIUgJ"
   },
   "outputs": [],
   "source": [
    "facebook_path = '../data/facebook/facebook_combined.txt.gz'\n",
    "citation_path = '../data/citation/cit-HepTh.edges'\n",
    "biological_path = '../data/biological/bio-CE-CX.edges'\n",
    "proteins_path = \"../data/proteins/PROTEINS-full.edges\"\n",
    "spam_path = \"../data/spam/web-spam-detection.edges\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83f7ed1-5bbd-4e15-8d0c-1a93e9802783",
   "metadata": {
    "id": "e83f7ed1-5bbd-4e15-8d0c-1a93e9802783"
   },
   "outputs": [],
   "source": [
    "def load_graph(path):\n",
    "    \"\"\"\n",
    "    For files with extension .edges\n",
    "    nodes are renamed as integers, starting from 0\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('%'):  # Skip comment lines\n",
    "                continue\n",
    "            # Split the line based on spaces or commas\n",
    "            data = re.split(r'[,\\s]+', line.strip())\n",
    "            if len(data) < 2:  # Skip lines that don't have at least two columns\n",
    "                continue\n",
    "            # Extract the first two columns (nodes)\n",
    "            node1, node2 = int(data[0]), int(data[1])\n",
    "            G.add_edge(node1, node2)\n",
    "    mapping = {node : i for i,node in enumerate(G.nodes)} # mappoing original : relabeled\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    return G\n",
    "\n",
    "def load_graph_with_gz(path):\n",
    "    \"\"\"\n",
    "    For files with extension .txt.gz\n",
    "    nodes are renamed as integers, starting from 0\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            node1, node2 = map(int, line.strip().split())\n",
    "            G.add_edge(node1, node2)\n",
    "    mapping = {node : i for i,node in enumerate(G.nodes)} # mappoing original : relabeled\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    return G\n",
    "\n",
    "def print_graphs_info(graphs):\n",
    "    for k in graph_keys:\n",
    "        G = graphs[k]\n",
    "        print(f\"{k}: |V|={len(G.nodes)}, |E|={len(G.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ab6207-3fe1-44b0-bf8e-636a7d9f9f2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2ab6207-3fe1-44b0-bf8e-636a7d9f9f2b",
    "outputId": "b810991c-26ec-47d1-b2c3-248de16db6fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook: |V|=4039, |E|=88234\n",
      "citation: |V|=22908, |E|=2444798\n",
      "biological: |V|=15229, |E|=245952\n",
      "proteins: |V|=43471, |E|=81049\n",
      "spam: |V|=9072, |E|=473854\n"
     ]
    }
   ],
   "source": [
    "graphs = {}\n",
    "\n",
    "# facebook graph is the only one .tar.gz\n",
    "graphs[graph_keys[0]] = load_graph_with_gz(facebook_path)  # relabeling nodes to integer\n",
    "graphs[graph_keys[1]] = load_graph(citation_path)\n",
    "graphs[graph_keys[2]] = load_graph(biological_path)\n",
    "graphs[graph_keys[3]] = load_graph(proteins_path)  # node labeled\n",
    "graphs[graph_keys[4]] = load_graph(spam_path)  # node labeled\n",
    "\n",
    "print_graphs_info(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8tGjO8jcn7P5",
   "metadata": {
    "id": "8tGjO8jcn7P5"
   },
   "source": [
    "# Download the dataset from the GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pBB2zmeGoCXe",
   "metadata": {
    "id": "pBB2zmeGoCXe"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/PietroVolpato/lfn_project/main/data/\"\n",
    "filename = \"bio-CE-CX_edges.csv\"\n",
    "\n",
    "response = requests.get(url + filename)\n",
    "with open(filename, \"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ab17a-9328-47b5-bc28-15dbd59deabf",
   "metadata": {
    "id": "f14ab17a-9328-47b5-bc28-15dbd59deabf"
   },
   "source": [
    "# Functions and declarations for the embeddings\n",
    "Embedding data structure is defined as following:<br>\n",
    "- The first index refer to the graph (e.g. embeddings[\"facebook\"] contains the embeddings of the facebook graph for every embedding technique).<br>\n",
    "- The second index refer to the embedding technique (e.g. embeddings[\"facebook\"][\"LINE\"] cointans the embedding of facebook graph computed using LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7e55ae-2aa3-4989-8903-0ccc3898309f",
   "metadata": {
    "id": "0e7e55ae-2aa3-4989-8903-0ccc3898309f"
   },
   "outputs": [],
   "source": [
    "def save(emb, graph_key, embedding_key, emb_dim):\n",
    "    path = f\"../result/embeddings_{graph_key}_{embedding_key}_{emb_dim}.npy\"\n",
    "    np.save(path, emb)\n",
    "    print(f\"Successfully saved the embeddings in {path}\")\n",
    "\n",
    "# dictionaries to store the embeddings, obtained by several techniques, for each graph\n",
    "embeddings = {}\n",
    "for k in graph_keys:\n",
    "    embeddings[k] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7649b02-023b-4ec3-b1e7-4d051776d0f2",
   "metadata": {
    "id": "d7649b02-023b-4ec3-b1e7-4d051776d0f2"
   },
   "source": [
    "# Node2Vec\n",
    "- pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c54d5a3a-a0ff-4044-8ba4-808588927dba",
   "metadata": {
    "id": "c54d5a3a-a0ff-4044-8ba4-808588927dba"
   },
   "outputs": [],
   "source": [
    "def get_node2vec_embeddings(G, dimensions=128, walk_length=50, num_walks=40, neighborhood_size = 10, p=0.5, q=2):\n",
    "    \"\"\"\n",
    "    Generate node embeddings for a graph using the Node2Vec algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph):The input graph for which embeddings are to be generated.\n",
    "            The graph should have nodes labeled as integers, ideally sequentially starting from 0.\n",
    "        dimensions (int, optional): The dimensionality of the embedding space. Default is 128.\n",
    "        walk_length (int, optional): The length of each random walk. Default is 10.\n",
    "        num_walks (int, optional): The number of random walks to start from each node. Default is 20.\n",
    "        p (float, optional):\n",
    "            The return parameter, controlling the likelihood of immediately revisiting a node in the walk.\n",
    "            A higher value makes it more likely to backtrack. Default is 1.\n",
    "        q (float, optional):\n",
    "            The in-out parameter, controlling the likelihood of exploring outward from the starting node.\n",
    "            A higher value makes it more likely to move outward. Default is 1.\n",
    "        workers (int, optional): The number of parallel workers for random walk generation and model training. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array where each row represents the embedding of a node.\n",
    "            The row index corresponds to the node ID, and each row has `dimensions` elements.\n",
    "    \"\"\"\n",
    "    # Initialize Node2Vec model\n",
    "    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, p=p, q=q, workers=1)\n",
    "\n",
    "    # Fit the Node2Vec model and generate embeddings\n",
    "    model = node2vec.fit(window=neighborhood_size, min_count=1, batch_words=4)\n",
    "\n",
    "    # Convert embeddings to a NumPy array\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    embeddings = np.zeros((num_nodes, dimensions))  # Preallocate array\n",
    "    for node in G.nodes:\n",
    "        embeddings[node] = model.wv[str(node)]  # in the vocabulary node names are converted always to strings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d9eac-2b78-4283-bea5-354f1f324e49",
   "metadata": {},
   "source": [
    "## Produce the embeddings with node2vec\n",
    "here you can easily produce the embeddings for any of the loaded graphs using node2vec.<br>\n",
    "Adjust the variable curr_graph_key with the key of the graph you want to compute the embeddings for.<br>\n",
    "The embeddings are saved to file (look output to get path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f11c92-dd61-476b-9c80-e152704a2e58",
   "metadata": {
    "id": "2535202c-086c-42ae-a7a0-2bce62d42e02"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7627024875dc4a369426ff18e05336ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/15229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1):  22%|████████████▍                                           | 4/18 [00:43<02:44, 11.72s/it]"
     ]
    }
   ],
   "source": [
    "# graph_keys[0] = facebook\n",
    "# graph_keys[1] = citation \n",
    "# graph_keys[2] = biological\n",
    "# graph_keys[3] = proteins\n",
    "# graph_keys[4] = spam\n",
    "curr_graph_key = \"biological\"   # chose the graph\n",
    "emb_dim = 128\n",
    "\n",
    "# DENSE GRAPH (G, dimensions=emb_dim, walk_length=20, num_walks=20, p=1, q=0.5, epochs = 10)\n",
    "# PAPER SETTINGS: (G, dimensions=128, walk_length=80, num_walks=10, p=1, q=0.5)\n",
    "# PAPER SETTINGS BLOG GRAPH 10k, 333k:  (G, dimensions=128, walk_length=80, num_walks=18, p=0.25, q=0.25)\n",
    "\n",
    "# facebook : (walk_length=80, num_walks=18, neighborhood_size = 16, p=0.25, q=0.25)\n",
    "# spam : (walk_length=80, num_walks=18, neighborhood_size = 16, p=0.25, q=0.25)\n",
    "# biological: (walk_length=80, num_walks=18, neighborhood_size = 16, p=0.25, q=0.25)\n",
    "# proteins: G, (walk_length=20, num_walks=20, neighborhood_size = 8, p=0.25, q=1)\n",
    "\n",
    "# facebook community detection: walk_length=60, num_walks=15, neighborhood_size=10, p=2, q=0.5\n",
    "\n",
    "G = graphs[curr_graph_key]\n",
    "embeddings[curr_graph_key][\"node2vec\"] = get_node2vec_embeddings(\n",
    "    G, dimensions=emb_dim, walk_length=80, num_walks=18, neighborhood_size = 16, p=0.25, q=0.25\n",
    ")\n",
    "save(embeddings[curr_graph_key][\"node2vec\"], curr_graph_key, \"node2vec\", emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c4f4c-6f35-4c15-bff0-837960ef3411",
   "metadata": {
    "id": "900c4f4c-6f35-4c15-bff0-837960ef3411"
   },
   "source": [
    "# LINE : Large-scale information network embedding\n",
    "installation guide:\n",
    "- git clone https://github.com/VahidooX/LINE.git\n",
    "- !pip install keras\n",
    "- !pip install tensorflow\n",
    "- adjust the sys.path to where you downloaded LINE repository\n",
    "\n",
    "*NOTE*: it was necessary to modify utils.py to adapt it at current version of keras because some elements were deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e6cc917-5939-45b8-aca5-d192cb0fd1d9",
   "metadata": {
    "id": "3e6cc917-5939-45b8-aca5-d192cb0fd1d9"
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"LINE\")\n",
    "\n",
    "from model import create_model\n",
    "from utils import batchgen_train\n",
    "\n",
    "def get_LINE_embeddings(G, embedding_dim=128, batch_size=1024, negative_ratio=5, epochs=10, negative_sampling=\"UNIFORM\"):\n",
    "    \"\"\"\n",
    "    Generate LINE embeddings for a given graph.\n",
    "\n",
    "    Parameters:\n",
    "        G (nx.Graph): The graph for which embeddings are computed.\n",
    "        embedding_dim (int): Dimensionality of the embeddings.\n",
    "        batch_size (int): Batch size for training.\n",
    "        negative_ratio (int): Ratio of negative to positive samples.\n",
    "        epochs (int): Number of training epochs.\n",
    "        negative_sampling (str): Negative sampling strategy (\"UNIFORM\" or \"NON-UNIFORM\").\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Node embeddings (shape: [num_nodes, embedding_dim]).\n",
    "    \"\"\"\n",
    "    num_nodes = G.number_of_nodes()\n",
    "\n",
    "    # Convert networkx.Graph to adj_list (edge list as 2D numpy array)\n",
    "    adj_list = np.array(list(G.edges()), dtype=np.int32)\n",
    "\n",
    "    # Create LINE model\n",
    "    model, embed_generator = create_model(num_nodes, embedding_dim)\n",
    "\n",
    "    # Generate training batches\n",
    "    train_gen = batchgen_train(adj_list, num_nodes, batch_size, negative_ratio, negative_sampling)\n",
    "\n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    model.fit(train_gen, steps_per_epoch=500, epochs=epochs)\n",
    "\n",
    "    # Extract embeddings\n",
    "    node_ids = np.arange(num_nodes)  # Sequential node IDs\n",
    "    embeddings = embed_generator.predict_on_batch(node_ids)\n",
    "\n",
    "    print(\"Node Embeddings Shape:\", embeddings[0].shape)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94620d7e-7d58-494a-a968-acf7f5b20b4f",
   "metadata": {},
   "source": [
    "## Produce the embeddings with LINE\n",
    "here you can easily produce the embeddings for any of the loaded graphs using LINE.<br>\n",
    "Adjust the variable curr_graph_key with the key of the graph you want to compute the embeddings for.<br>\n",
    "The embeddings are saved to file (look output to get path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2535202c-086c-42ae-a7a0-2bce62d42e02",
   "metadata": {
    "collapsed": true,
    "id": "2535202c-086c-42ae-a7a0-2bce62d42e02",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 20/500 [>.............................] - ETA: 1:47 - loss: 1.7294"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m curr_graph_key \u001b[38;5;241m=\u001b[39m graph_keys[\u001b[38;5;241m3\u001b[39m]   \u001b[38;5;66;03m# chose the graph\u001b[39;00m\n\u001b[0;32m      2\u001b[0m emb_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m----> 4\u001b[0m embeddings[curr_graph_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLINE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mget_LINE_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraphs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurr_graph_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m save(embeddings[curr_graph_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLINE\u001b[39m\u001b[38;5;124m\"\u001b[39m], curr_graph_key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLINE\u001b[39m\u001b[38;5;124m\"\u001b[39m, emb_dim)\n",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m, in \u001b[0;36mget_LINE_embeddings\u001b[1;34m(G, embedding_dim, batch_size, negative_ratio, epochs, negative_sampling)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Compile and train the model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Extract embeddings\u001b[39;00m\n\u001b[0;32m     37\u001b[0m node_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(num_nodes)  \u001b[38;5;66;03m# Sequential node IDs\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\engine\\training.py:1748\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1746\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1747\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1748\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    327\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 393\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m     logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m-> 1170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\utils\\generic_utils.py:296\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    293\u001b[0m         info \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m info\n\u001b[1;32m--> 296\u001b[0m     \u001b[43mio_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\src\\utils\\io_utils.py:79\u001b[0m, in \u001b[0;36mprint_msg\u001b[1;34m(message, line_break)\u001b[0m\n\u001b[0;32m     77\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m         \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ipykernel\\iostream.py:694\u001b[0m, in \u001b[0;36mOutStream.write\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(string)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ipykernel\\iostream.py:590\u001b[0m, in \u001b[0;36mOutStream._schedule_flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_schedule_in_thread\u001b[39m():\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_io_loop\u001b[38;5;241m.\u001b[39mcall_later(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_interval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[1;32m--> 590\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_schedule_in_thread\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\ipykernel\\iostream.py:267\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     f()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\zmq\\sugar\\socket.py:696\u001b[0m, in \u001b[0;36mSocket.send\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    689\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[0;32m    690\u001b[0m             data,\n\u001b[0;32m    691\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[0;32m    692\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    693\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:742\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:789\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "curr_graph_key = graph_keys[3]   # chose the graph\n",
    "emb_dim = 128\n",
    "\n",
    "embeddings[curr_graph_key][\"LINE\"] = get_LINE_embeddings(graphs[curr_graph_key], epochs = 20)\n",
    "save(embeddings[curr_graph_key][\"LINE\"], curr_graph_key, \"LINE\", emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc4a49",
   "metadata": {
    "id": "5abc4a49"
   },
   "source": [
    "# AttentionWalk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f540988",
   "metadata": {
    "id": "5f540988"
   },
   "source": [
    "## Installation guide\n",
    "<ol>\n",
    "<li>git clone https://github.com/benedekrozemberczki/AttentionWalk.git</li>\n",
    "<li>pip install texttable</li>\n",
    "</ol>\n",
    "\n",
    "It requires that the input file is a .csv, so first we have implemented a function that converts the .txt.gz and the .edges files to a .csv to be given as input to the AttentionWalk algorithm.<br>\n",
    "For starting the algorithm you have to enter to the AttentionWalk folder after having cloned it from the Github repository and then set the arguments as described in the README.md file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b872e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3b872e0",
    "outputId": "44fa23c4-3512-4e6c-e2e7-4459f560804d"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/benedekrozemberczki/AttentionWalk.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33c074",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d33c074",
    "outputId": "bdb2c444-b76b-4596-af4b-f60315b0747e"
   },
   "outputs": [],
   "source": [
    "!pip install texttable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b0bbb",
   "metadata": {
    "id": "d01b0bbb"
   },
   "source": [
    "## Test with the facebook network\n",
    "Save the embeddings in the result folder<br>\n",
    "Time: 1m 50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98fc24b7",
   "metadata": {
    "collapsed": true,
    "id": "98fc24b7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "80702d2f-f99e-402c-a944-b6dcc8ab5146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------------------------------+\n",
      "| Attention path |     ./output/chameleon_AW_attention.csv     |\n",
      "+================+=============================================+\n",
      "| Beta           | 0.500                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Dimensions     | 256                                         |\n",
      "+----------------+---------------------------------------------+\n",
      "| Edge path      | ../../data/facebook/facebook_combined.csv   |\n",
      "+----------------+---------------------------------------------+\n",
      "| Embedding path | ../../result/embeddings_facebook_AW_256.csv |\n",
      "+----------------+---------------------------------------------+\n",
      "| Epochs         | 200                                         |\n",
      "+----------------+---------------------------------------------+\n",
      "| Gamma          | 0.500                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Learning rate  | 0.010                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Num of walks   | 80                                          |\n",
      "+----------------+---------------------------------------------+\n",
      "| Window size    | 5                                           |\n",
      "+----------------+---------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n",
      "\n",
      "Training the model.\n",
      "\n",
      "Loss contains NaN or Inf. Stopping training.\n",
      "\n",
      "Saving the model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjacency matrix powers:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:  50%|█████     | 2/4 [00:00<00:00,  4.80it/s]\n",
      "Adjacency matrix powers:  75%|███████▌  | 3/4 [00:01<00:00,  2.16it/s]\n",
      "Adjacency matrix powers: 100%|██████████| 4/4 [00:02<00:00,  1.22it/s]\n",
      "Adjacency matrix powers: 100%|██████████| 4/4 [00:02<00:00,  1.51it/s]\n",
      "\n",
      "Loss:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Attention Walk (Loss=48.3137):   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Attention Walk (Loss=48.3137):   0%|          | 1/200 [00:00<01:21,  2.45it/s]\n",
      "Attention Walk (Loss=48.2877):   0%|          | 1/200 [00:00<01:21,  2.45it/s]\n",
      "Attention Walk (Loss=48.2877):   1%|          | 2/200 [00:00<01:22,  2.41it/s]\n",
      "Attention Walk (Loss=47.8906):   1%|          | 2/200 [00:01<01:22,  2.41it/s]\n",
      "Attention Walk (Loss=47.8906):   2%|▏         | 3/200 [00:01<01:22,  2.39it/s]\n",
      "Attention Walk (Loss=47.2749):   2%|▏         | 3/200 [00:01<01:22,  2.39it/s]\n",
      "Attention Walk (Loss=47.2749):   2%|▏         | 4/200 [00:01<01:20,  2.44it/s]\n",
      "Attention Walk (Loss=46.2631):   2%|▏         | 4/200 [00:02<01:20,  2.44it/s]\n",
      "Attention Walk (Loss=46.2631):   2%|▎         | 5/200 [00:02<01:20,  2.43it/s]\n",
      "Attention Walk (Loss=44.7931):   2%|▎         | 5/200 [00:02<01:20,  2.43it/s]\n",
      "Attention Walk (Loss=44.7931):   3%|▎         | 6/200 [00:02<01:20,  2.41it/s]\n",
      "Attention Walk (Loss=42.8592):   3%|▎         | 6/200 [00:02<01:20,  2.41it/s]\n",
      "Attention Walk (Loss=42.8592):   4%|▎         | 7/200 [00:02<01:18,  2.46it/s]\n",
      "Attention Walk (Loss=40.4672):   4%|▎         | 7/200 [00:03<01:18,  2.46it/s]\n",
      "Attention Walk (Loss=40.4672):   4%|▍         | 8/200 [00:03<01:17,  2.48it/s]\n",
      "Attention Walk (Loss=37.6414):   4%|▍         | 8/200 [00:03<01:17,  2.48it/s]\n",
      "Attention Walk (Loss=37.6414):   4%|▍         | 9/200 [00:03<01:16,  2.50it/s]\n",
      "Attention Walk (Loss=34.4376):   4%|▍         | 9/200 [00:04<01:16,  2.50it/s]\n",
      "Attention Walk (Loss=34.4376):   5%|▌         | 10/200 [00:04<01:15,  2.53it/s]\n",
      "Attention Walk (Loss=30.945):   5%|▌         | 10/200 [00:04<01:15,  2.53it/s] \n",
      "Attention Walk (Loss=30.945):   6%|▌         | 11/200 [00:04<01:13,  2.56it/s]\n",
      "Attention Walk (Loss=27.2832):   6%|▌         | 11/200 [00:04<01:13,  2.56it/s]\n",
      "Attention Walk (Loss=27.2832):   6%|▌         | 12/200 [00:04<01:13,  2.56it/s]\n",
      "Attention Walk (Loss=23.5931):   6%|▌         | 12/200 [00:05<01:13,  2.56it/s]\n",
      "Attention Walk (Loss=23.5931):   6%|▋         | 13/200 [00:05<01:12,  2.57it/s]\n",
      "Attention Walk (Loss=20.0233):   6%|▋         | 13/200 [00:05<01:12,  2.57it/s]\n",
      "Attention Walk (Loss=20.0233):   7%|▋         | 14/200 [00:05<01:12,  2.57it/s]\n",
      "Attention Walk (Loss=16.7116):   7%|▋         | 14/200 [00:05<01:12,  2.57it/s]\n",
      "Attention Walk (Loss=16.7116):   8%|▊         | 15/200 [00:05<01:12,  2.56it/s]\n",
      "Attention Walk (Loss=13.7654):   8%|▊         | 15/200 [00:06<01:12,  2.56it/s]\n",
      "Attention Walk (Loss=13.7654):   8%|▊         | 16/200 [00:06<01:12,  2.54it/s]\n",
      "Attention Walk (Loss=11.2478):   8%|▊         | 16/200 [00:06<01:12,  2.54it/s]\n",
      "Attention Walk (Loss=11.2478):   8%|▊         | 17/200 [00:06<01:13,  2.49it/s]\n",
      "Attention Walk (Loss=9.1726):   8%|▊         | 17/200 [00:07<01:13,  2.49it/s] \n",
      "Attention Walk (Loss=9.1726):   9%|▉         | 18/200 [00:07<01:13,  2.48it/s]\n",
      "Attention Walk (Loss=7.5127):   9%|▉         | 18/200 [00:07<01:13,  2.48it/s]\n",
      "Attention Walk (Loss=7.5127):  10%|▉         | 19/200 [00:07<01:12,  2.50it/s]\n",
      "Attention Walk (Loss=6.2146):  10%|▉         | 19/200 [00:07<01:12,  2.50it/s]\n",
      "Attention Walk (Loss=6.2146):  10%|█         | 20/200 [00:07<01:11,  2.52it/s]\n",
      "Attention Walk (Loss=5.2148):  10%|█         | 20/200 [00:08<01:11,  2.52it/s]\n",
      "Attention Walk (Loss=5.2148):  10%|█         | 21/200 [00:08<01:11,  2.50it/s]\n",
      "Attention Walk (Loss=4.4512):  10%|█         | 21/200 [00:08<01:11,  2.50it/s]\n",
      "Attention Walk (Loss=4.4512):  11%|█         | 22/200 [00:08<01:12,  2.46it/s]\n",
      "Attention Walk (Loss=3.8701):  11%|█         | 22/200 [00:09<01:12,  2.46it/s]\n",
      "Attention Walk (Loss=3.8701):  12%|█▏        | 23/200 [00:09<01:12,  2.44it/s]\n",
      "Attention Walk (Loss=3.4282):  12%|█▏        | 23/200 [00:09<01:12,  2.44it/s]\n",
      "Attention Walk (Loss=3.4282):  12%|█▏        | 24/200 [00:09<01:12,  2.43it/s]\n",
      "Attention Walk (Loss=3.0917):  12%|█▏        | 24/200 [00:10<01:12,  2.43it/s]\n",
      "Attention Walk (Loss=3.0917):  12%|█▎        | 25/200 [00:10<01:12,  2.41it/s]\n",
      "Attention Walk (Loss=2.8352):  12%|█▎        | 25/200 [00:10<01:12,  2.41it/s]\n",
      "Attention Walk (Loss=2.8352):  13%|█▎        | 26/200 [00:10<01:11,  2.42it/s]\n",
      "Attention Walk (Loss=2.6394):  13%|█▎        | 26/200 [00:10<01:11,  2.42it/s]\n",
      "Attention Walk (Loss=2.6394):  14%|█▎        | 27/200 [00:10<01:12,  2.37it/s]\n",
      "Attention Walk (Loss=2.49):  14%|█▎        | 27/200 [00:11<01:12,  2.37it/s]  \n",
      "Attention Walk (Loss=2.49):  14%|█▍        | 28/200 [00:11<01:11,  2.42it/s]\n",
      "Attention Walk (Loss=2.3761):  14%|█▍        | 28/200 [00:11<01:11,  2.42it/s]\n",
      "Attention Walk (Loss=2.3761):  14%|█▍        | 29/200 [00:11<01:09,  2.47it/s]\n",
      "Attention Walk (Loss=2.2893):  14%|█▍        | 29/200 [00:12<01:09,  2.47it/s]\n",
      "Attention Walk (Loss=2.2893):  15%|█▌        | 30/200 [00:12<01:08,  2.49it/s]\n",
      "Attention Walk (Loss=2.2233):  15%|█▌        | 30/200 [00:12<01:08,  2.49it/s]\n",
      "Attention Walk (Loss=2.2233):  16%|█▌        | 31/200 [00:12<01:06,  2.52it/s]\n",
      "Attention Walk (Loss=2.1732):  16%|█▌        | 31/200 [00:12<01:06,  2.52it/s]\n",
      "Attention Walk (Loss=2.1732):  16%|█▌        | 32/200 [00:12<01:06,  2.53it/s]\n",
      "Attention Walk (Loss=2.1353):  16%|█▌        | 32/200 [00:13<01:06,  2.53it/s]\n",
      "Attention Walk (Loss=2.1353):  16%|█▋        | 33/200 [00:13<01:07,  2.47it/s]\n",
      "Attention Walk (Loss=2.1067):  16%|█▋        | 33/200 [00:13<01:07,  2.47it/s]\n",
      "Attention Walk (Loss=2.1067):  17%|█▋        | 34/200 [00:13<01:05,  2.55it/s]\n",
      "Attention Walk (Loss=2.085):  17%|█▋        | 34/200 [00:14<01:05,  2.55it/s] \n",
      "Attention Walk (Loss=2.085):  18%|█▊        | 35/200 [00:14<01:05,  2.51it/s]\n",
      "Attention Walk (Loss=2.0684):  18%|█▊        | 35/200 [00:14<01:05,  2.51it/s]\n",
      "Attention Walk (Loss=2.0684):  18%|█▊        | 36/200 [00:14<01:05,  2.51it/s]\n",
      "Attention Walk (Loss=2.0558):  18%|█▊        | 36/200 [00:14<01:05,  2.51it/s]\n",
      "Attention Walk (Loss=2.0558):  18%|█▊        | 37/200 [00:14<01:04,  2.54it/s]\n",
      "Attention Walk (Loss=2.0459):  18%|█▊        | 37/200 [00:15<01:04,  2.54it/s]\n",
      "Attention Walk (Loss=2.0459):  19%|█▉        | 38/200 [00:15<01:03,  2.56it/s]\n",
      "Attention Walk (Loss=2.0379):  19%|█▉        | 38/200 [00:15<01:03,  2.56it/s]\n",
      "Attention Walk (Loss=2.0379):  20%|█▉        | 39/200 [00:15<01:03,  2.52it/s]\n",
      "Attention Walk (Loss=2.0313):  20%|█▉        | 39/200 [00:16<01:03,  2.52it/s]\n",
      "Attention Walk (Loss=2.0313):  20%|██        | 40/200 [00:16<01:03,  2.54it/s]\n",
      "Attention Walk (Loss=2.0256):  20%|██        | 40/200 [00:16<01:03,  2.54it/s]\n",
      "Attention Walk (Loss=2.0256):  20%|██        | 41/200 [00:16<01:04,  2.45it/s]\n",
      "Attention Walk (Loss=2.0204):  20%|██        | 41/200 [00:16<01:04,  2.45it/s]\n",
      "Attention Walk (Loss=2.0204):  21%|██        | 42/200 [00:16<01:05,  2.43it/s]\n",
      "Attention Walk (Loss=2.0155):  21%|██        | 42/200 [00:17<01:05,  2.43it/s]\n",
      "Attention Walk (Loss=2.0155):  22%|██▏       | 43/200 [00:17<01:06,  2.37it/s]\n",
      "Attention Walk (Loss=2.0106):  22%|██▏       | 43/200 [00:17<01:06,  2.37it/s]\n",
      "Attention Walk (Loss=2.0106):  22%|██▏       | 44/200 [00:17<01:06,  2.35it/s]\n",
      "Attention Walk (Loss=2.0057):  22%|██▏       | 44/200 [00:18<01:06,  2.35it/s]\n",
      "Attention Walk (Loss=2.0057):  22%|██▎       | 45/200 [00:18<01:06,  2.33it/s]\n",
      "Attention Walk (Loss=2.0006):  22%|██▎       | 45/200 [00:18<01:06,  2.33it/s]\n",
      "Attention Walk (Loss=2.0006):  23%|██▎       | 46/200 [00:18<01:06,  2.30it/s]\n",
      "Attention Walk (Loss=1.9954):  23%|██▎       | 46/200 [00:19<01:06,  2.30it/s]\n",
      "Attention Walk (Loss=1.9954):  24%|██▎       | 47/200 [00:19<01:06,  2.29it/s]\n",
      "Attention Walk (Loss=1.9899):  24%|██▎       | 47/200 [00:19<01:06,  2.29it/s]\n",
      "Attention Walk (Loss=1.9899):  24%|██▍       | 48/200 [00:19<01:07,  2.25it/s]\n",
      "Attention Walk (Loss=1.9842):  24%|██▍       | 48/200 [00:19<01:07,  2.25it/s]\n",
      "Attention Walk (Loss=1.9842):  24%|██▍       | 49/200 [00:19<01:06,  2.28it/s]\n",
      "Attention Walk (Loss=1.9784):  24%|██▍       | 49/200 [00:20<01:06,  2.28it/s]\n",
      "Attention Walk (Loss=1.9784):  25%|██▌       | 50/200 [00:20<01:04,  2.31it/s]\n",
      "Attention Walk (Loss=1.9723):  25%|██▌       | 50/200 [00:20<01:04,  2.31it/s]\n",
      "Attention Walk (Loss=1.9723):  26%|██▌       | 51/200 [00:20<01:03,  2.34it/s]\n",
      "Attention Walk (Loss=1.966):  26%|██▌       | 51/200 [00:21<01:03,  2.34it/s] \n",
      "Attention Walk (Loss=1.966):  26%|██▌       | 52/200 [00:21<01:02,  2.38it/s]\n",
      "Attention Walk (Loss=1.9596):  26%|██▌       | 52/200 [00:21<01:02,  2.38it/s]\n",
      "Attention Walk (Loss=1.9596):  26%|██▋       | 53/200 [00:21<01:00,  2.42it/s]\n",
      "Attention Walk (Loss=1.9531):  26%|██▋       | 53/200 [00:22<01:00,  2.42it/s]\n",
      "Attention Walk (Loss=1.9531):  27%|██▋       | 54/200 [00:22<01:00,  2.41it/s]\n",
      "Attention Walk (Loss=1.9464):  27%|██▋       | 54/200 [00:22<01:00,  2.41it/s]\n",
      "Attention Walk (Loss=1.9464):  28%|██▊       | 55/200 [00:22<01:00,  2.42it/s]\n",
      "Attention Walk (Loss=1.9397):  28%|██▊       | 55/200 [00:22<01:00,  2.42it/s]\n",
      "Attention Walk (Loss=1.9397):  28%|██▊       | 56/200 [00:22<00:59,  2.41it/s]\n",
      "Attention Walk (Loss=1.933):  28%|██▊       | 56/200 [00:23<00:59,  2.41it/s] \n",
      "Attention Walk (Loss=1.933):  28%|██▊       | 57/200 [00:23<00:57,  2.48it/s]\n",
      "Attention Walk (Loss=1.9263):  28%|██▊       | 57/200 [00:23<00:57,  2.48it/s]\n",
      "Attention Walk (Loss=1.9263):  29%|██▉       | 58/200 [00:23<00:57,  2.46it/s]\n",
      "Attention Walk (Loss=1.9196):  29%|██▉       | 58/200 [00:24<00:57,  2.46it/s]\n",
      "Attention Walk (Loss=1.9196):  30%|██▉       | 59/200 [00:24<00:56,  2.48it/s]\n",
      "Attention Walk (Loss=1.9129):  30%|██▉       | 59/200 [00:24<00:56,  2.48it/s]\n",
      "Attention Walk (Loss=1.9129):  30%|███       | 60/200 [00:24<00:55,  2.51it/s]\n",
      "Attention Walk (Loss=1.9063):  30%|███       | 60/200 [00:24<00:55,  2.51it/s]\n",
      "Attention Walk (Loss=1.9063):  30%|███       | 61/200 [00:24<00:55,  2.51it/s]\n",
      "Attention Walk (Loss=1.8998):  30%|███       | 61/200 [00:25<00:55,  2.51it/s]\n",
      "Attention Walk (Loss=1.8998):  31%|███       | 62/200 [00:25<00:54,  2.53it/s]\n",
      "Attention Walk (Loss=1.8934):  31%|███       | 62/200 [00:25<00:54,  2.53it/s]\n",
      "Attention Walk (Loss=1.8934):  32%|███▏      | 63/200 [00:25<00:54,  2.52it/s]\n",
      "Attention Walk (Loss=1.887):  32%|███▏      | 63/200 [00:26<00:54,  2.52it/s] \n",
      "Attention Walk (Loss=1.887):  32%|███▏      | 64/200 [00:26<00:54,  2.50it/s]\n",
      "Attention Walk (Loss=1.8808):  32%|███▏      | 64/200 [00:26<00:54,  2.50it/s]\n",
      "Attention Walk (Loss=1.8808):  32%|███▎      | 65/200 [00:26<00:53,  2.51it/s]\n",
      "Attention Walk (Loss=1.8747):  32%|███▎      | 65/200 [00:26<00:53,  2.51it/s]\n",
      "Attention Walk (Loss=1.8747):  33%|███▎      | 66/200 [00:26<00:53,  2.50it/s]\n",
      "Attention Walk (Loss=1.8687):  33%|███▎      | 66/200 [00:27<00:53,  2.50it/s]\n",
      "Attention Walk (Loss=1.8687):  34%|███▎      | 67/200 [00:27<00:53,  2.50it/s]\n",
      "Attention Walk (Loss=1.8629):  34%|███▎      | 67/200 [00:27<00:53,  2.50it/s]\n",
      "Attention Walk (Loss=1.8629):  34%|███▍      | 68/200 [00:27<00:52,  2.53it/s]\n",
      "Attention Walk (Loss=1.8572):  34%|███▍      | 68/200 [00:28<00:52,  2.53it/s]\n",
      "Attention Walk (Loss=1.8572):  34%|███▍      | 69/200 [00:28<00:51,  2.53it/s]\n",
      "Attention Walk (Loss=1.8516):  34%|███▍      | 69/200 [00:28<00:51,  2.53it/s]\n",
      "Attention Walk (Loss=1.8516):  35%|███▌      | 70/200 [00:28<00:51,  2.53it/s]\n",
      "Attention Walk (Loss=1.8461):  35%|███▌      | 70/200 [00:28<00:51,  2.53it/s]\n",
      "Attention Walk (Loss=1.8461):  36%|███▌      | 71/200 [00:28<00:50,  2.57it/s]\n",
      "Attention Walk (Loss=1.8408):  36%|███▌      | 71/200 [00:29<00:50,  2.57it/s]\n",
      "Attention Walk (Loss=1.8408):  36%|███▌      | 72/200 [00:29<00:50,  2.55it/s]\n",
      "Attention Walk (Loss=1.8356):  36%|███▌      | 72/200 [00:29<00:50,  2.55it/s]\n",
      "Attention Walk (Loss=1.8356):  36%|███▋      | 73/200 [00:29<00:50,  2.54it/s]\n",
      "Attention Walk (Loss=1.8306):  36%|███▋      | 73/200 [00:30<00:50,  2.54it/s]\n",
      "Attention Walk (Loss=1.8306):  37%|███▋      | 74/200 [00:30<00:50,  2.51it/s]\n",
      "Attention Walk (Loss=1.8257):  37%|███▋      | 74/200 [00:30<00:50,  2.51it/s]\n",
      "Attention Walk (Loss=1.8257):  38%|███▊      | 75/200 [00:30<00:50,  2.45it/s]\n",
      "Attention Walk (Loss=1.8209):  38%|███▊      | 75/200 [00:30<00:50,  2.45it/s]\n",
      "Attention Walk (Loss=1.8209):  38%|███▊      | 76/200 [00:30<00:50,  2.45it/s]\n",
      "Attention Walk (Loss=1.8163):  38%|███▊      | 76/200 [00:31<00:50,  2.45it/s]\n",
      "Attention Walk (Loss=1.8163):  38%|███▊      | 77/200 [00:31<00:49,  2.48it/s]\n",
      "Attention Walk (Loss=1.8117):  38%|███▊      | 77/200 [00:31<00:49,  2.48it/s]\n",
      "Attention Walk (Loss=1.8117):  39%|███▉      | 78/200 [00:31<00:49,  2.48it/s]\n",
      "Attention Walk (Loss=1.8073):  39%|███▉      | 78/200 [00:32<00:49,  2.48it/s]\n",
      "Attention Walk (Loss=1.8073):  40%|███▉      | 79/200 [00:32<00:48,  2.47it/s]\n",
      "Attention Walk (Loss=1.803):  40%|███▉      | 79/200 [00:32<00:48,  2.47it/s] \n",
      "Attention Walk (Loss=1.803):  40%|████      | 80/200 [00:32<00:48,  2.48it/s]\n",
      "Attention Walk (Loss=1.7989):  40%|████      | 80/200 [00:32<00:48,  2.48it/s]\n",
      "Attention Walk (Loss=1.7989):  40%|████      | 81/200 [00:32<00:47,  2.49it/s]\n",
      "Attention Walk (Loss=1.7948):  40%|████      | 81/200 [00:33<00:47,  2.49it/s]\n",
      "Attention Walk (Loss=1.7948):  41%|████      | 82/200 [00:33<00:47,  2.49it/s]\n",
      "Attention Walk (Loss=1.7909):  41%|████      | 82/200 [00:33<00:47,  2.49it/s]\n",
      "Attention Walk (Loss=1.7909):  42%|████▏     | 83/200 [00:33<00:46,  2.49it/s]\n",
      "Attention Walk (Loss=1.787):  42%|████▏     | 83/200 [00:34<00:46,  2.49it/s] \n",
      "Attention Walk (Loss=1.787):  42%|████▏     | 84/200 [00:34<00:46,  2.48it/s]\n",
      "Attention Walk (Loss=1.7833):  42%|████▏     | 84/200 [00:34<00:46,  2.48it/s]\n",
      "Attention Walk (Loss=1.7833):  42%|████▎     | 85/200 [00:34<00:46,  2.45it/s]\n",
      "Attention Walk (Loss=1.7796):  42%|████▎     | 85/200 [00:34<00:46,  2.45it/s]\n",
      "Attention Walk (Loss=1.7796):  43%|████▎     | 86/200 [00:34<00:47,  2.41it/s]\n",
      "Attention Walk (Loss=1.7761):  43%|████▎     | 86/200 [00:35<00:47,  2.41it/s]\n",
      "Attention Walk (Loss=1.7761):  44%|████▎     | 87/200 [00:35<00:46,  2.42it/s]\n",
      "Attention Walk (Loss=1.7726):  44%|████▎     | 87/200 [00:35<00:46,  2.42it/s]\n",
      "Attention Walk (Loss=1.7726):  44%|████▍     | 88/200 [00:35<00:45,  2.48it/s]\n",
      "Attention Walk (Loss=1.7693):  44%|████▍     | 88/200 [00:36<00:45,  2.48it/s]\n",
      "Attention Walk (Loss=1.7693):  44%|████▍     | 89/200 [00:36<00:45,  2.46it/s]\n",
      "Attention Walk (Loss=1.766):  44%|████▍     | 89/200 [00:36<00:45,  2.46it/s] \n",
      "Attention Walk (Loss=1.766):  45%|████▌     | 90/200 [00:36<00:44,  2.49it/s]\n",
      "Attention Walk (Loss=1.7628):  45%|████▌     | 90/200 [00:36<00:44,  2.49it/s]\n",
      "Attention Walk (Loss=1.7628):  46%|████▌     | 91/200 [00:36<00:43,  2.51it/s]\n",
      "Attention Walk (Loss=1.7597):  46%|████▌     | 91/200 [00:37<00:43,  2.51it/s]\n",
      "Attention Walk (Loss=1.7597):  46%|████▌     | 92/200 [00:37<00:42,  2.52it/s]\n",
      "Attention Walk (Loss=1.7567):  46%|████▌     | 92/200 [00:37<00:42,  2.52it/s]\n",
      "Attention Walk (Loss=1.7567):  46%|████▋     | 93/200 [00:37<00:42,  2.54it/s]\n",
      "Attention Walk (Loss=1.7537):  46%|████▋     | 93/200 [00:38<00:42,  2.54it/s]\n",
      "Attention Walk (Loss=1.7537):  47%|████▋     | 94/200 [00:38<00:41,  2.55it/s]\n",
      "Attention Walk (Loss=1.7508):  47%|████▋     | 94/200 [00:38<00:41,  2.55it/s]\n",
      "Attention Walk (Loss=1.7508):  48%|████▊     | 95/200 [00:38<00:41,  2.54it/s]\n",
      "Attention Walk (Loss=1.748):  48%|████▊     | 95/200 [00:38<00:41,  2.54it/s] \n",
      "Attention Walk (Loss=1.748):  48%|████▊     | 96/200 [00:38<00:41,  2.53it/s]\n",
      "Attention Walk (Loss=1.7453):  48%|████▊     | 96/200 [00:39<00:41,  2.53it/s]\n",
      "Attention Walk (Loss=1.7453):  48%|████▊     | 97/200 [00:39<00:40,  2.52it/s]\n",
      "Attention Walk (Loss=1.7426):  48%|████▊     | 97/200 [00:39<00:40,  2.52it/s]\n",
      "Attention Walk (Loss=1.7426):  49%|████▉     | 98/200 [00:39<00:41,  2.44it/s]\n",
      "Attention Walk (Loss=1.74):  49%|████▉     | 98/200 [00:40<00:41,  2.44it/s]  \n",
      "Attention Walk (Loss=1.74):  50%|████▉     | 99/200 [00:40<00:41,  2.46it/s]\n",
      "Attention Walk (Loss=1.7374):  50%|████▉     | 99/200 [00:40<00:41,  2.46it/s]\n",
      "Attention Walk (Loss=1.7374):  50%|█████     | 100/200 [00:40<00:40,  2.47it/s]\n",
      "Attention Walk (Loss=1.7374):  50%|█████     | 100/200 [00:40<00:40,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/facebook/facebook_combined.csv --embedding-path ../../result/embeddings_facebook_AW_256.csv --dimensions 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c81e3",
   "metadata": {
    "id": "de1c81e3"
   },
   "source": [
    "## Test with the citation network\n",
    "Infeasible!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1dc864",
   "metadata": {
    "id": "ab1dc864",
    "outputId": "a9337ff5-b81b-407b-80a2-072176a06703"
   },
   "outputs": [],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/cit-HepTh_edges.csv --embedding-path ../../result/cit-HepTh_embeddings_attention.csv --attention-path ../../result/cit-HepTh_attention.csv --epochs 176"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P2KVPWY9ovVm",
   "metadata": {
    "id": "P2KVPWY9ovVm"
   },
   "source": [
    "## Test with the biological network\n",
    "Infeasible!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-URjbGoLozuS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-URjbGoLozuS",
    "outputId": "17f2f3d5-e0ad-49dd-ffb1-760414c60dec"
   },
   "outputs": [],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/bio-CE-CX_edges.csv --embedding-path ../../result/bio-CE-CX_embeddings_attention.csv --attention-path ....//result/bio-CE-CX_attention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc8b89",
   "metadata": {},
   "source": [
    "## Test proteins network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee1c589",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------------------------------+\n",
      "| Attention path |     ./output/chameleon_AW_attention.csv     |\n",
      "+================+=============================================+\n",
      "| Beta           | 0.500                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Dimensions     | 128                                         |\n",
      "+----------------+---------------------------------------------+\n",
      "| Edge path      | ../../data/proteins/PROTEINS-full.csv       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Embedding path | ../../result/embeddings_PROTEINS_AW_128.csv |\n",
      "+----------------+---------------------------------------------+\n",
      "| Epochs         | 200                                         |\n",
      "+----------------+---------------------------------------------+\n",
      "| Gamma          | 0.500                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Learning rate  | 0.010                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Num of walks   | 80                                          |\n",
      "+----------------+---------------------------------------------+\n",
      "| Window size    | 5                                           |\n",
      "+----------------+---------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_data.py:133: RuntimeWarning: divide by zero encountered in power\n",
      "  return self._with_data(data ** n)\n",
      "\n",
      "Adjacency matrix powers:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:  25%|██▌       | 1/4 [00:00<00:01,  1.75it/s]\n",
      "Adjacency matrix powers:  50%|█████     | 2/4 [00:01<00:01,  1.07it/s]\n",
      "Adjacency matrix powers:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\main.py\", line 19, in <module>\n",
      "    main()\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\main.py\", line 14, in main\n",
      "    model = AttentionWalkTrainer(args)\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\attentionwalk.py\", line 78, in __init__\n",
      "    self.initialize_model_and_features()\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\attentionwalk.py\", line 84, in initialize_model_and_features\n",
      "    self.target_tensor = feature_calculator(self.args, self.graph)\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\utils.py\", line 56, in feature_calculator\n",
      "    to_add = powered_A.todense()\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 959, in todense\n",
      "    return self._ascontainer(self.toarray(order=order, out=out))\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py\", line 1106, in toarray\n",
      "    out = self._process_toarray_args(order, out)\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 1327, in _process_toarray_args\n",
      "    return np.zeros(self.shape, dtype=self.dtype, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 14.1 GiB for an array with shape (43472, 43472) and data type float64\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/proteins/PROTEINS-full.csv --embedding-path ../../result/embeddings_PROTEINS_AW_128.csv --dimensions 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac8179",
   "metadata": {},
   "source": [
    "## Test with email-Enron\n",
    "Infeasibile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f520ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/email-Enron.csv --embedding-path ../../result/email-Enron_embeddings_attention.csv --attention-path ../../result/email-Enron_attention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f6d07",
   "metadata": {},
   "source": [
    "## Test with CL-100K-1d8-L9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674d6f6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------------------------------------+\n",
      "| Attention path |      ../../result/CL-100K-1d8-L9_attention.csv       |\n",
      "+================+======================================================+\n",
      "| Beta           | 0.500                                                |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Dimensions     | 128                                                  |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Edge path      | ../../data/CL-100K-1d8-L9.csv                        |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Embedding path | ../../result/CL-100K-1d8-L9_embeddings_attention.csv |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Epochs         | 200                                                  |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Gamma          | 0.500                                                |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Learning rate  | 0.010                                                |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Num of walks   | 80                                                   |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Window size    | 5                                                    |\n",
      "+----------------+------------------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjacency matrix powers:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:  40%|████      | 2/5 [00:00<00:00,  4.25it/s]\n",
      "Adjacency matrix powers:  60%|██████    | 3/5 [00:24<00:20, 10.05s/it]\n",
      "Adjacency matrix powers:  60%|██████    | 3/5 [01:46<01:11, 35.56s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\main.py\", line 19, in <module>\n",
      "    main()\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\main.py\", line 14, in main\n",
      "    model = AttentionWalkTrainer(args)\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\attentionwalk.py\", line 71, in __init__\n",
      "    self._initialize_model_and_data()\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\attentionwalk.py\", line 75, in _initialize_model_and_data\n",
      "    sparse_target_tensor = feature_calculator(self.args, self.graph)\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\utils.py\", line 56, in feature_calculator\n",
      "    powered_A = powered_A @ normalized_adjacency_matrix\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 695, in __matmul__\n",
      "    return self._matmul_dispatch(other)\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 606, in _matmul_dispatch\n",
      "    return self._matmul_sparse(other)\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py\", line 532, in _matmul_sparse\n",
      "    data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype))\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 20.9 GiB for an array with shape (5621787628,) and data type float32\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/CL-100K-1d8-L9.csv --embedding-path ../../result/CL-100K-1d8-L9_embeddings_attention.csv --attention-path ../../result/CL-100K-1d8-L9_attention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125957f4",
   "metadata": {},
   "source": [
    "## Test spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b5a6a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------------------------+\n",
      "| Attention path |   ./output/chameleon_AW_attention.csv   |\n",
      "+================+=========================================+\n",
      "| Beta           | 0.500                                   |\n",
      "+----------------+-----------------------------------------+\n",
      "| Dimensions     | 128                                     |\n",
      "+----------------+-----------------------------------------+\n",
      "| Edge path      | ../../data/spam/spam.csv                |\n",
      "+----------------+-----------------------------------------+\n",
      "| Embedding path | ../../result/embeddings_spam_AW_128.csv |\n",
      "+----------------+-----------------------------------------+\n",
      "| Epochs         | 200                                     |\n",
      "+----------------+-----------------------------------------+\n",
      "| Gamma          | 0.500                                   |\n",
      "+----------------+-----------------------------------------+\n",
      "| Learning rate  | 0.010                                   |\n",
      "+----------------+-----------------------------------------+\n",
      "| Num of walks   | 80                                      |\n",
      "+----------------+-----------------------------------------+\n",
      "| Window size    | 5                                       |\n",
      "+----------------+-----------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n",
      "\n",
      "Training the model.\n",
      "\n",
      "\n",
      "Saving the model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_data.py:133: RuntimeWarning: divide by zero encountered in power\n",
      "  return self._with_data(data ** n)\n",
      "\n",
      "Adjacency matrix powers:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:  25%|██▌       | 1/4 [00:01<00:05,  1.99s/it]\n",
      "Adjacency matrix powers:  50%|█████     | 2/4 [00:19<00:21, 10.95s/it]\n",
      "Adjacency matrix powers:  75%|███████▌  | 3/4 [00:41<00:15, 15.91s/it]\n",
      "Adjacency matrix powers: 100%|██████████| 4/4 [01:07<00:00, 20.24s/it]\n",
      "Adjacency matrix powers: 100%|██████████| 4/4 [01:07<00:00, 16.97s/it]\n",
      "\n",
      "Loss:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Attention Walk (Loss=35.2986):   0%|          | 0/200 [00:02<?, ?it/s]\n",
      "Attention Walk (Loss=35.2986):   0%|          | 1/200 [00:02<09:04,  2.73s/it]\n",
      "Attention Walk (Loss=35.283):   0%|          | 1/200 [00:05<09:04,  2.73s/it] \n",
      "Attention Walk (Loss=35.283):   1%|          | 2/200 [00:05<08:33,  2.59s/it]\n",
      "Attention Walk (Loss=35.224):   1%|          | 2/200 [00:07<08:33,  2.59s/it]\n",
      "Attention Walk (Loss=35.224):   2%|▏         | 3/200 [00:07<08:16,  2.52s/it]\n",
      "Attention Walk (Loss=35.0499):   2%|▏         | 3/200 [00:10<08:16,  2.52s/it]\n",
      "Attention Walk (Loss=35.0499):   2%|▏         | 4/200 [00:10<08:07,  2.49s/it]\n",
      "Attention Walk (Loss=34.7554):   2%|▏         | 4/200 [00:12<08:07,  2.49s/it]\n",
      "Attention Walk (Loss=34.7554):   2%|▎         | 5/200 [00:12<07:56,  2.44s/it]\n",
      "Attention Walk (Loss=34.2896):   2%|▎         | 5/200 [00:14<07:56,  2.44s/it]\n",
      "Attention Walk (Loss=34.2896):   3%|▎         | 6/200 [00:14<07:51,  2.43s/it]\n",
      "Attention Walk (Loss=33.626):   3%|▎         | 6/200 [00:17<07:51,  2.43s/it] \n",
      "Attention Walk (Loss=33.626):   4%|▎         | 7/200 [00:17<07:45,  2.41s/it]\n",
      "Attention Walk (Loss=32.7489):   4%|▎         | 7/200 [00:19<07:45,  2.41s/it]\n",
      "Attention Walk (Loss=32.7489):   4%|▍         | 8/200 [00:19<07:36,  2.38s/it]\n",
      "Attention Walk (Loss=31.6489):   4%|▍         | 8/200 [00:21<07:36,  2.38s/it]\n",
      "Attention Walk (Loss=31.6489):   4%|▍         | 9/200 [00:21<07:34,  2.38s/it]\n",
      "Attention Walk (Loss=30.3254):   4%|▍         | 9/200 [00:24<07:34,  2.38s/it]\n",
      "Attention Walk (Loss=30.3254):   5%|▌         | 10/200 [00:24<07:34,  2.39s/it]\n",
      "Attention Walk (Loss=28.7865):   5%|▌         | 10/200 [00:26<07:34,  2.39s/it]\n",
      "Attention Walk (Loss=28.7865):   6%|▌         | 11/200 [00:26<07:35,  2.41s/it]\n",
      "Attention Walk (Loss=27.0497):   6%|▌         | 11/200 [00:29<07:35,  2.41s/it]\n",
      "Attention Walk (Loss=27.0497):   6%|▌         | 12/200 [00:29<07:30,  2.40s/it]\n",
      "Attention Walk (Loss=25.1424):   6%|▌         | 12/200 [00:31<07:30,  2.40s/it]\n",
      "Attention Walk (Loss=25.1424):   6%|▋         | 13/200 [00:31<07:28,  2.40s/it]\n",
      "Attention Walk (Loss=23.1017):   6%|▋         | 13/200 [00:34<07:28,  2.40s/it]\n",
      "Attention Walk (Loss=23.1017):   7%|▋         | 14/200 [00:34<07:28,  2.41s/it]\n",
      "Attention Walk (Loss=20.9744):   7%|▋         | 14/200 [00:36<07:28,  2.41s/it]\n",
      "Attention Walk (Loss=20.9744):   8%|▊         | 15/200 [00:36<07:26,  2.41s/it]\n",
      "Attention Walk (Loss=18.8149):   8%|▊         | 15/200 [00:38<07:26,  2.41s/it]\n",
      "Attention Walk (Loss=18.8149):   8%|▊         | 16/200 [00:38<07:21,  2.40s/it]\n",
      "Attention Walk (Loss=16.6831):   8%|▊         | 16/200 [00:41<07:21,  2.40s/it]\n",
      "Attention Walk (Loss=16.6831):   8%|▊         | 17/200 [00:41<07:16,  2.39s/it]\n",
      "Attention Walk (Loss=14.6396):   8%|▊         | 17/200 [00:43<07:16,  2.39s/it]\n",
      "Attention Walk (Loss=14.6396):   9%|▉         | 18/200 [00:43<07:14,  2.39s/it]\n",
      "Attention Walk (Loss=12.741):   9%|▉         | 18/200 [00:46<07:14,  2.39s/it] \n",
      "Attention Walk (Loss=12.741):  10%|▉         | 19/200 [00:46<07:18,  2.42s/it]\n",
      "Attention Walk (Loss=11.0345):  10%|▉         | 19/200 [00:48<07:18,  2.42s/it]\n",
      "Attention Walk (Loss=11.0345):  10%|█         | 20/200 [00:48<07:20,  2.45s/it]\n",
      "Attention Walk (Loss=9.5528):  10%|█         | 20/200 [00:50<07:20,  2.45s/it] \n",
      "Attention Walk (Loss=9.5528):  10%|█         | 21/200 [00:50<07:14,  2.43s/it]\n",
      "Attention Walk (Loss=8.3116):  10%|█         | 21/200 [00:53<07:14,  2.43s/it]\n",
      "Attention Walk (Loss=8.3116):  11%|█         | 22/200 [00:53<07:08,  2.41s/it]\n",
      "Attention Walk (Loss=7.3094):  11%|█         | 22/200 [00:55<07:08,  2.41s/it]\n",
      "Attention Walk (Loss=7.3094):  12%|█▏        | 23/200 [00:55<07:04,  2.40s/it]\n",
      "Attention Walk (Loss=6.53):  12%|█▏        | 23/200 [00:58<07:04,  2.40s/it]  \n",
      "Attention Walk (Loss=6.53):  12%|█▏        | 24/200 [00:58<07:01,  2.39s/it]\n",
      "Attention Walk (Loss=5.9468):  12%|█▏        | 24/200 [01:00<07:01,  2.39s/it]\n",
      "Attention Walk (Loss=5.9468):  12%|█▎        | 25/200 [01:00<06:56,  2.38s/it]\n",
      "Attention Walk (Loss=5.5279):  12%|█▎        | 25/200 [01:02<06:56,  2.38s/it]\n",
      "Attention Walk (Loss=5.5279):  13%|█▎        | 26/200 [01:02<06:53,  2.37s/it]\n",
      "Attention Walk (Loss=5.2405):  13%|█▎        | 26/200 [01:05<06:53,  2.37s/it]\n",
      "Attention Walk (Loss=5.2405):  14%|█▎        | 27/200 [01:05<06:52,  2.38s/it]\n",
      "Attention Walk (Loss=5.0541):  14%|█▎        | 27/200 [01:07<06:52,  2.38s/it]\n",
      "Attention Walk (Loss=5.0541):  14%|█▍        | 28/200 [01:07<06:53,  2.40s/it]\n",
      "Attention Walk (Loss=4.9422):  14%|█▍        | 28/200 [01:10<06:53,  2.40s/it]\n",
      "Attention Walk (Loss=4.9422):  14%|█▍        | 29/200 [01:10<06:52,  2.41s/it]\n",
      "Attention Walk (Loss=4.8829):  14%|█▍        | 29/200 [01:12<06:52,  2.41s/it]\n",
      "Attention Walk (Loss=4.8829):  15%|█▌        | 30/200 [01:12<06:53,  2.43s/it]\n",
      "Attention Walk (Loss=4.8592):  15%|█▌        | 30/200 [01:14<06:53,  2.43s/it]\n",
      "Attention Walk (Loss=4.8592):  16%|█▌        | 31/200 [01:14<06:52,  2.44s/it]\n",
      "Attention Walk (Loss=4.8579):  16%|█▌        | 31/200 [01:17<06:52,  2.44s/it]\n",
      "Attention Walk (Loss=4.8579):  16%|█▌        | 32/200 [01:17<06:49,  2.44s/it]\n",
      "Attention Walk (Loss=4.8694):  16%|█▌        | 32/200 [01:19<06:49,  2.44s/it]\n",
      "Attention Walk (Loss=4.8694):  16%|█▋        | 33/200 [01:19<06:43,  2.41s/it]\n",
      "Attention Walk (Loss=4.8867):  16%|█▋        | 33/200 [01:22<06:43,  2.41s/it]\n",
      "Attention Walk (Loss=4.8867):  17%|█▋        | 34/200 [01:22<06:40,  2.41s/it]\n",
      "Attention Walk (Loss=4.905):  17%|█▋        | 34/200 [01:24<06:40,  2.41s/it] \n",
      "Attention Walk (Loss=4.905):  18%|█▊        | 35/200 [01:24<06:39,  2.42s/it]\n",
      "Attention Walk (Loss=4.9212):  18%|█▊        | 35/200 [01:27<06:39,  2.42s/it]\n",
      "Attention Walk (Loss=4.9212):  18%|█▊        | 36/200 [01:27<06:36,  2.41s/it]\n",
      "Attention Walk (Loss=4.9333):  18%|█▊        | 36/200 [01:29<06:36,  2.41s/it]\n",
      "Attention Walk (Loss=4.9333):  18%|█▊        | 37/200 [01:29<06:35,  2.43s/it]\n",
      "Attention Walk (Loss=4.9403):  18%|█▊        | 37/200 [01:31<06:35,  2.43s/it]\n",
      "Attention Walk (Loss=4.9403):  19%|█▉        | 38/200 [01:31<06:28,  2.40s/it]\n",
      "Attention Walk (Loss=4.9419):  19%|█▉        | 38/200 [01:34<06:28,  2.40s/it]\n",
      "Attention Walk (Loss=4.9419):  20%|█▉        | 39/200 [01:34<06:24,  2.39s/it]\n",
      "Attention Walk (Loss=4.9383):  20%|█▉        | 39/200 [01:36<06:24,  2.39s/it]\n",
      "Attention Walk (Loss=4.9383):  20%|██        | 40/200 [01:36<06:20,  2.38s/it]\n",
      "Attention Walk (Loss=4.9298):  20%|██        | 40/200 [01:38<06:20,  2.38s/it]\n",
      "Attention Walk (Loss=4.9298):  20%|██        | 41/200 [01:38<06:18,  2.38s/it]\n",
      "Attention Walk (Loss=4.9171):  20%|██        | 41/200 [01:41<06:18,  2.38s/it]\n",
      "Attention Walk (Loss=4.9171):  21%|██        | 42/200 [01:41<06:15,  2.38s/it]\n",
      "Attention Walk (Loss=4.9007):  21%|██        | 42/200 [01:43<06:15,  2.38s/it]\n",
      "Attention Walk (Loss=4.9007):  22%|██▏       | 43/200 [01:43<06:09,  2.35s/it]\n",
      "Attention Walk (Loss=4.8814):  22%|██▏       | 43/200 [01:45<06:09,  2.35s/it]\n",
      "Attention Walk (Loss=4.8814):  22%|██▏       | 44/200 [01:45<06:06,  2.35s/it]\n",
      "Attention Walk (Loss=4.8597):  22%|██▏       | 44/200 [01:48<06:06,  2.35s/it]\n",
      "Attention Walk (Loss=4.8597):  22%|██▎       | 45/200 [01:48<06:06,  2.37s/it]\n",
      "Attention Walk (Loss=4.8362):  22%|██▎       | 45/200 [01:50<06:06,  2.37s/it]\n",
      "Attention Walk (Loss=4.8362):  23%|██▎       | 46/200 [01:50<06:00,  2.34s/it]\n",
      "Attention Walk (Loss=4.8113):  23%|██▎       | 46/200 [01:52<06:00,  2.34s/it]\n",
      "Attention Walk (Loss=4.8113):  24%|██▎       | 47/200 [01:52<06:00,  2.36s/it]\n",
      "Attention Walk (Loss=4.7855):  24%|██▎       | 47/200 [01:55<06:00,  2.36s/it]\n",
      "Attention Walk (Loss=4.7855):  24%|██▍       | 48/200 [01:55<06:00,  2.37s/it]\n",
      "Attention Walk (Loss=4.7593):  24%|██▍       | 48/200 [01:57<06:00,  2.37s/it]\n",
      "Attention Walk (Loss=4.7593):  24%|██▍       | 49/200 [01:57<05:53,  2.34s/it]\n",
      "Attention Walk (Loss=4.7331):  24%|██▍       | 49/200 [01:59<05:53,  2.34s/it]\n",
      "Attention Walk (Loss=4.7331):  25%|██▌       | 50/200 [01:59<05:49,  2.33s/it]\n",
      "Attention Walk (Loss=4.7073):  25%|██▌       | 50/200 [02:02<05:49,  2.33s/it]\n",
      "Attention Walk (Loss=4.7073):  26%|██▌       | 51/200 [02:02<05:46,  2.33s/it]\n",
      "Attention Walk (Loss=4.6822):  26%|██▌       | 51/200 [02:04<05:46,  2.33s/it]\n",
      "Attention Walk (Loss=4.6822):  26%|██▌       | 52/200 [02:04<05:49,  2.36s/it]\n",
      "Attention Walk (Loss=4.658):  26%|██▌       | 52/200 [02:07<05:49,  2.36s/it] \n",
      "Attention Walk (Loss=4.658):  26%|██▋       | 53/200 [02:07<05:47,  2.36s/it]\n",
      "Attention Walk (Loss=4.6349):  26%|██▋       | 53/200 [02:09<05:47,  2.36s/it]\n",
      "Attention Walk (Loss=4.6349):  27%|██▋       | 54/200 [02:09<05:45,  2.37s/it]\n",
      "Attention Walk (Loss=4.6131):  27%|██▋       | 54/200 [02:11<05:45,  2.37s/it]\n",
      "Attention Walk (Loss=4.6131):  28%|██▊       | 55/200 [02:11<05:43,  2.37s/it]\n",
      "Attention Walk (Loss=4.5927):  28%|██▊       | 55/200 [02:14<05:43,  2.37s/it]\n",
      "Attention Walk (Loss=4.5927):  28%|██▊       | 56/200 [02:14<05:43,  2.39s/it]\n",
      "Attention Walk (Loss=4.5736):  28%|██▊       | 56/200 [02:16<05:43,  2.39s/it]\n",
      "Attention Walk (Loss=4.5736):  28%|██▊       | 57/200 [02:16<05:42,  2.40s/it]\n",
      "Attention Walk (Loss=4.556):  28%|██▊       | 57/200 [02:19<05:42,  2.40s/it] \n",
      "Attention Walk (Loss=4.556):  29%|██▉       | 58/200 [02:19<05:38,  2.38s/it]\n",
      "Attention Walk (Loss=4.5397):  29%|██▉       | 58/200 [02:21<05:38,  2.38s/it]\n",
      "Attention Walk (Loss=4.5397):  30%|██▉       | 59/200 [02:21<05:36,  2.39s/it]\n",
      "Attention Walk (Loss=4.5247):  30%|██▉       | 59/200 [02:23<05:36,  2.39s/it]\n",
      "Attention Walk (Loss=4.5247):  30%|███       | 60/200 [02:23<05:32,  2.37s/it]\n",
      "Attention Walk (Loss=4.511):  30%|███       | 60/200 [02:26<05:32,  2.37s/it] \n",
      "Attention Walk (Loss=4.511):  30%|███       | 61/200 [02:26<05:29,  2.37s/it]\n",
      "Attention Walk (Loss=4.4984):  30%|███       | 61/200 [02:28<05:29,  2.37s/it]\n",
      "Attention Walk (Loss=4.4984):  31%|███       | 62/200 [02:28<05:23,  2.35s/it]\n",
      "Attention Walk (Loss=4.4869):  31%|███       | 62/200 [02:30<05:23,  2.35s/it]\n",
      "Attention Walk (Loss=4.4869):  32%|███▏      | 63/200 [02:30<05:20,  2.34s/it]\n",
      "Attention Walk (Loss=4.4763):  32%|███▏      | 63/200 [02:33<05:20,  2.34s/it]\n",
      "Attention Walk (Loss=4.4763):  32%|███▏      | 64/200 [02:33<05:17,  2.33s/it]\n",
      "Attention Walk (Loss=4.4667):  32%|███▏      | 64/200 [02:35<05:17,  2.33s/it]\n",
      "Attention Walk (Loss=4.4667):  32%|███▎      | 65/200 [02:35<05:18,  2.36s/it]\n",
      "Attention Walk (Loss=4.4578):  32%|███▎      | 65/200 [02:37<05:18,  2.36s/it]\n",
      "Attention Walk (Loss=4.4578):  33%|███▎      | 66/200 [02:37<05:15,  2.35s/it]\n",
      "Attention Walk (Loss=4.4496):  33%|███▎      | 66/200 [02:40<05:15,  2.35s/it]\n",
      "Attention Walk (Loss=4.4496):  34%|███▎      | 67/200 [02:40<05:15,  2.37s/it]\n",
      "Attention Walk (Loss=4.442):  34%|███▎      | 67/200 [02:42<05:15,  2.37s/it] \n",
      "Attention Walk (Loss=4.442):  34%|███▍      | 68/200 [02:42<05:12,  2.36s/it]\n",
      "Attention Walk (Loss=4.435):  34%|███▍      | 68/200 [02:44<05:12,  2.36s/it]\n",
      "Attention Walk (Loss=4.435):  34%|███▍      | 69/200 [02:44<05:07,  2.35s/it]\n",
      "Attention Walk (Loss=4.4285):  34%|███▍      | 69/200 [02:47<05:07,  2.35s/it]\n",
      "Attention Walk (Loss=4.4285):  35%|███▌      | 70/200 [02:47<05:06,  2.36s/it]\n",
      "Attention Walk (Loss=4.4224):  35%|███▌      | 70/200 [02:49<05:06,  2.36s/it]\n",
      "Attention Walk (Loss=4.4224):  36%|███▌      | 71/200 [02:49<05:06,  2.37s/it]\n",
      "Attention Walk (Loss=4.4167):  36%|███▌      | 71/200 [02:51<05:06,  2.37s/it]\n",
      "Attention Walk (Loss=4.4167):  36%|███▌      | 72/200 [02:51<05:00,  2.35s/it]\n",
      "Attention Walk (Loss=4.4113):  36%|███▌      | 72/200 [02:54<05:00,  2.35s/it]\n",
      "Attention Walk (Loss=4.4113):  36%|███▋      | 73/200 [02:54<04:55,  2.33s/it]\n",
      "Attention Walk (Loss=4.4062):  36%|███▋      | 73/200 [02:56<04:55,  2.33s/it]\n",
      "Attention Walk (Loss=4.4062):  37%|███▋      | 74/200 [02:56<04:55,  2.34s/it]\n",
      "Attention Walk (Loss=4.4014):  37%|███▋      | 74/200 [02:59<04:55,  2.34s/it]\n",
      "Attention Walk (Loss=4.4014):  38%|███▊      | 75/200 [02:59<04:55,  2.37s/it]\n",
      "Attention Walk (Loss=4.3969):  38%|███▊      | 75/200 [03:01<04:55,  2.37s/it]\n",
      "Attention Walk (Loss=4.3969):  38%|███▊      | 76/200 [03:01<04:53,  2.36s/it]\n",
      "Attention Walk (Loss=4.3925):  38%|███▊      | 76/200 [03:03<04:53,  2.36s/it]\n",
      "Attention Walk (Loss=4.3925):  38%|███▊      | 77/200 [03:03<04:47,  2.34s/it]\n",
      "Attention Walk (Loss=4.3884):  38%|███▊      | 77/200 [03:06<04:47,  2.34s/it]\n",
      "Attention Walk (Loss=4.3884):  39%|███▉      | 78/200 [03:06<04:47,  2.36s/it]\n",
      "Attention Walk (Loss=4.3844):  39%|███▉      | 78/200 [03:08<04:47,  2.36s/it]\n",
      "Attention Walk (Loss=4.3844):  40%|███▉      | 79/200 [03:08<04:46,  2.37s/it]\n",
      "Attention Walk (Loss=4.3807):  40%|███▉      | 79/200 [03:10<04:46,  2.37s/it]\n",
      "Attention Walk (Loss=4.3807):  40%|████      | 80/200 [03:10<04:44,  2.37s/it]\n",
      "Attention Walk (Loss=4.3771):  40%|████      | 80/200 [03:13<04:44,  2.37s/it]\n",
      "Attention Walk (Loss=4.3771):  40%|████      | 81/200 [03:13<04:45,  2.40s/it]\n",
      "Attention Walk (Loss=4.3736):  40%|████      | 81/200 [03:15<04:45,  2.40s/it]\n",
      "Attention Walk (Loss=4.3736):  41%|████      | 82/200 [03:15<04:41,  2.39s/it]\n",
      "Attention Walk (Loss=4.3703):  41%|████      | 82/200 [03:17<04:41,  2.39s/it]\n",
      "Attention Walk (Loss=4.3703):  42%|████▏     | 83/200 [03:17<04:34,  2.35s/it]\n",
      "Attention Walk (Loss=4.3671):  42%|████▏     | 83/200 [03:20<04:34,  2.35s/it]\n",
      "Attention Walk (Loss=4.3671):  42%|████▏     | 84/200 [03:20<04:30,  2.34s/it]\n",
      "Attention Walk (Loss=4.3641):  42%|████▏     | 84/200 [03:22<04:30,  2.34s/it]\n",
      "Attention Walk (Loss=4.3641):  42%|████▎     | 85/200 [03:22<04:26,  2.32s/it]\n",
      "Attention Walk (Loss=4.3612):  42%|████▎     | 85/200 [03:24<04:26,  2.32s/it]\n",
      "Attention Walk (Loss=4.3612):  43%|████▎     | 86/200 [03:24<04:23,  2.31s/it]\n",
      "Attention Walk (Loss=4.3584):  43%|████▎     | 86/200 [03:27<04:23,  2.31s/it]\n",
      "Attention Walk (Loss=4.3584):  44%|████▎     | 87/200 [03:27<04:20,  2.31s/it]\n",
      "Attention Walk (Loss=4.3557):  44%|████▎     | 87/200 [03:29<04:20,  2.31s/it]\n",
      "Attention Walk (Loss=4.3557):  44%|████▍     | 88/200 [03:29<04:20,  2.33s/it]\n",
      "Attention Walk (Loss=4.3531):  44%|████▍     | 88/200 [03:31<04:20,  2.33s/it]\n",
      "Attention Walk (Loss=4.3531):  44%|████▍     | 89/200 [03:31<04:20,  2.35s/it]\n",
      "Attention Walk (Loss=4.3506):  44%|████▍     | 89/200 [03:34<04:20,  2.35s/it]\n",
      "Attention Walk (Loss=4.3506):  45%|████▌     | 90/200 [03:34<04:21,  2.38s/it]\n",
      "Attention Walk (Loss=4.3482):  45%|████▌     | 90/200 [03:36<04:21,  2.38s/it]\n",
      "Attention Walk (Loss=4.3482):  46%|████▌     | 91/200 [03:36<04:21,  2.40s/it]\n",
      "Attention Walk (Loss=4.3458):  46%|████▌     | 91/200 [03:39<04:21,  2.40s/it]\n",
      "Attention Walk (Loss=4.3458):  46%|████▌     | 92/200 [03:39<04:16,  2.37s/it]\n",
      "Attention Walk (Loss=4.3436):  46%|████▌     | 92/200 [03:41<04:16,  2.37s/it]\n",
      "Attention Walk (Loss=4.3436):  46%|████▋     | 93/200 [03:41<04:14,  2.37s/it]\n",
      "Attention Walk (Loss=4.3414):  46%|████▋     | 93/200 [03:43<04:14,  2.37s/it]\n",
      "Attention Walk (Loss=4.3414):  47%|████▋     | 94/200 [03:43<04:09,  2.36s/it]\n",
      "Attention Walk (Loss=4.3393):  47%|████▋     | 94/200 [03:46<04:09,  2.36s/it]\n",
      "Attention Walk (Loss=4.3393):  48%|████▊     | 95/200 [03:46<04:09,  2.37s/it]\n",
      "Attention Walk (Loss=4.3373):  48%|████▊     | 95/200 [03:48<04:09,  2.37s/it]\n",
      "Attention Walk (Loss=4.3373):  48%|████▊     | 96/200 [03:48<04:11,  2.42s/it]\n",
      "Attention Walk (Loss=4.3353):  48%|████▊     | 96/200 [03:51<04:11,  2.42s/it]\n",
      "Attention Walk (Loss=4.3353):  48%|████▊     | 97/200 [03:51<04:08,  2.41s/it]\n",
      "Attention Walk (Loss=4.3333):  48%|████▊     | 97/200 [03:53<04:08,  2.41s/it]\n",
      "Attention Walk (Loss=4.3333):  49%|████▉     | 98/200 [03:53<04:05,  2.40s/it]\n",
      "Attention Walk (Loss=4.3314):  49%|████▉     | 98/200 [03:55<04:05,  2.40s/it]\n",
      "Attention Walk (Loss=4.3314):  50%|████▉     | 99/200 [03:55<04:04,  2.42s/it]\n",
      "Attention Walk (Loss=4.3296):  50%|████▉     | 99/200 [03:58<04:04,  2.42s/it]\n",
      "Attention Walk (Loss=4.3296):  50%|█████     | 100/200 [03:58<04:03,  2.43s/it]\n",
      "Attention Walk (Loss=4.3278):  50%|█████     | 100/200 [04:00<04:03,  2.43s/it]\n",
      "Attention Walk (Loss=4.3278):  50%|█████     | 101/200 [04:00<04:01,  2.44s/it]\n",
      "Attention Walk (Loss=4.3261):  50%|█████     | 101/200 [04:03<04:01,  2.44s/it]\n",
      "Attention Walk (Loss=4.3261):  51%|█████     | 102/200 [04:03<03:59,  2.44s/it]\n",
      "Attention Walk (Loss=4.3244):  51%|█████     | 102/200 [04:05<03:59,  2.44s/it]\n",
      "Attention Walk (Loss=4.3244):  52%|█████▏    | 103/200 [04:05<03:52,  2.40s/it]\n",
      "Attention Walk (Loss=4.3227):  52%|█████▏    | 103/200 [04:08<03:52,  2.40s/it]\n",
      "Attention Walk (Loss=4.3227):  52%|█████▏    | 104/200 [04:08<03:49,  2.39s/it]\n",
      "Attention Walk (Loss=4.3211):  52%|█████▏    | 104/200 [04:10<03:49,  2.39s/it]\n",
      "Attention Walk (Loss=4.3211):  52%|█████▎    | 105/200 [04:10<03:47,  2.40s/it]\n",
      "Attention Walk (Loss=4.3196):  52%|█████▎    | 105/200 [04:12<03:47,  2.40s/it]\n",
      "Attention Walk (Loss=4.3196):  53%|█████▎    | 106/200 [04:12<03:44,  2.39s/it]\n",
      "Attention Walk (Loss=4.318):  53%|█████▎    | 106/200 [04:15<03:44,  2.39s/it] \n",
      "Attention Walk (Loss=4.318):  54%|█████▎    | 107/200 [04:15<03:38,  2.35s/it]\n",
      "Attention Walk (Loss=4.3166):  54%|█████▎    | 107/200 [04:17<03:38,  2.35s/it]\n",
      "Attention Walk (Loss=4.3166):  54%|█████▍    | 108/200 [04:17<03:34,  2.33s/it]\n",
      "Attention Walk (Loss=4.3151):  54%|█████▍    | 108/200 [04:19<03:34,  2.33s/it]\n",
      "Attention Walk (Loss=4.3151):  55%|█████▍    | 109/200 [04:19<03:31,  2.32s/it]\n",
      "Attention Walk (Loss=4.3137):  55%|█████▍    | 109/200 [04:21<03:31,  2.32s/it]\n",
      "Attention Walk (Loss=4.3137):  55%|█████▌    | 110/200 [04:21<03:27,  2.30s/it]\n",
      "Attention Walk (Loss=4.3123):  55%|█████▌    | 110/200 [04:24<03:27,  2.30s/it]\n",
      "Attention Walk (Loss=4.3123):  56%|█████▌    | 111/200 [04:24<03:26,  2.32s/it]\n",
      "Attention Walk (Loss=4.311):  56%|█████▌    | 111/200 [04:26<03:26,  2.32s/it] \n",
      "Attention Walk (Loss=4.311):  56%|█████▌    | 112/200 [04:26<03:26,  2.35s/it]\n",
      "Attention Walk (Loss=4.3097):  56%|█████▌    | 112/200 [04:29<03:26,  2.35s/it]\n",
      "Attention Walk (Loss=4.3097):  56%|█████▋    | 113/200 [04:29<03:25,  2.37s/it]\n",
      "Attention Walk (Loss=4.3084):  56%|█████▋    | 113/200 [04:31<03:25,  2.37s/it]\n",
      "Attention Walk (Loss=4.3084):  57%|█████▋    | 114/200 [04:31<03:22,  2.35s/it]\n",
      "Attention Walk (Loss=4.3072):  57%|█████▋    | 114/200 [04:33<03:22,  2.35s/it]\n",
      "Attention Walk (Loss=4.3072):  57%|█████▊    | 115/200 [04:33<03:20,  2.36s/it]\n",
      "Attention Walk (Loss=4.306):  57%|█████▊    | 115/200 [04:36<03:20,  2.36s/it] \n",
      "Attention Walk (Loss=4.306):  58%|█████▊    | 116/200 [04:36<03:17,  2.35s/it]\n",
      "Attention Walk (Loss=4.3048):  58%|█████▊    | 116/200 [04:38<03:17,  2.35s/it]\n",
      "Attention Walk (Loss=4.3048):  58%|█████▊    | 117/200 [04:38<03:14,  2.35s/it]\n",
      "Attention Walk (Loss=4.3036):  58%|█████▊    | 117/200 [04:40<03:14,  2.35s/it]\n",
      "Attention Walk (Loss=4.3036):  59%|█████▉    | 118/200 [04:40<03:11,  2.34s/it]\n",
      "Attention Walk (Loss=4.3025):  59%|█████▉    | 118/200 [04:43<03:11,  2.34s/it]\n",
      "Attention Walk (Loss=4.3025):  60%|█████▉    | 119/200 [04:43<03:10,  2.35s/it]\n",
      "Attention Walk (Loss=4.3014):  60%|█████▉    | 119/200 [04:45<03:10,  2.35s/it]\n",
      "Attention Walk (Loss=4.3014):  60%|██████    | 120/200 [04:45<03:10,  2.38s/it]\n",
      "Attention Walk (Loss=4.3003):  60%|██████    | 120/200 [04:48<03:10,  2.38s/it]\n",
      "Attention Walk (Loss=4.3003):  60%|██████    | 121/200 [04:48<03:09,  2.40s/it]\n",
      "Attention Walk (Loss=4.2992):  60%|██████    | 121/200 [04:50<03:09,  2.40s/it]\n",
      "Attention Walk (Loss=4.2992):  61%|██████    | 122/200 [04:50<03:07,  2.40s/it]\n",
      "Attention Walk (Loss=4.2982):  61%|██████    | 122/200 [04:52<03:07,  2.40s/it]\n",
      "Attention Walk (Loss=4.2982):  62%|██████▏   | 123/200 [04:52<03:05,  2.40s/it]\n",
      "Attention Walk (Loss=4.2972):  62%|██████▏   | 123/200 [04:55<03:05,  2.40s/it]\n",
      "Attention Walk (Loss=4.2972):  62%|██████▏   | 124/200 [04:55<03:01,  2.38s/it]\n",
      "Attention Walk (Loss=4.2962):  62%|██████▏   | 124/200 [04:57<03:01,  2.38s/it]\n",
      "Attention Walk (Loss=4.2962):  62%|██████▎   | 125/200 [04:57<02:57,  2.37s/it]\n",
      "Attention Walk (Loss=4.2952):  62%|██████▎   | 125/200 [04:59<02:57,  2.37s/it]\n",
      "Attention Walk (Loss=4.2952):  63%|██████▎   | 126/200 [04:59<02:54,  2.36s/it]\n",
      "Attention Walk (Loss=4.2943):  63%|██████▎   | 126/200 [05:02<02:54,  2.36s/it]\n",
      "Attention Walk (Loss=4.2943):  64%|██████▎   | 127/200 [05:02<02:49,  2.33s/it]\n",
      "Attention Walk (Loss=4.2934):  64%|██████▎   | 127/200 [05:04<02:49,  2.33s/it]\n",
      "Attention Walk (Loss=4.2934):  64%|██████▍   | 128/200 [05:04<02:46,  2.31s/it]\n",
      "Attention Walk (Loss=4.2924):  64%|██████▍   | 128/200 [05:06<02:46,  2.31s/it]\n",
      "Attention Walk (Loss=4.2924):  64%|██████▍   | 129/200 [05:06<02:44,  2.32s/it]\n",
      "Attention Walk (Loss=4.2916):  64%|██████▍   | 129/200 [05:09<02:44,  2.32s/it]\n",
      "Attention Walk (Loss=4.2916):  65%|██████▌   | 130/200 [05:09<02:42,  2.32s/it]\n",
      "Attention Walk (Loss=4.2907):  65%|██████▌   | 130/200 [05:11<02:42,  2.32s/it]\n",
      "Attention Walk (Loss=4.2907):  66%|██████▌   | 131/200 [05:11<02:39,  2.32s/it]\n",
      "Attention Walk (Loss=4.2898):  66%|██████▌   | 131/200 [05:13<02:39,  2.32s/it]\n",
      "Attention Walk (Loss=4.2898):  66%|██████▌   | 132/200 [05:13<02:38,  2.34s/it]\n",
      "Attention Walk (Loss=4.289):  66%|██████▌   | 132/200 [05:16<02:38,  2.34s/it] \n",
      "Attention Walk (Loss=4.289):  66%|██████▋   | 133/200 [05:16<02:38,  2.36s/it]\n",
      "Attention Walk (Loss=4.2882):  66%|██████▋   | 133/200 [05:18<02:38,  2.36s/it]\n",
      "Attention Walk (Loss=4.2882):  67%|██████▋   | 134/200 [05:18<02:38,  2.40s/it]\n",
      "Attention Walk (Loss=4.2874):  67%|██████▋   | 134/200 [05:21<02:38,  2.40s/it]\n",
      "Attention Walk (Loss=4.2874):  68%|██████▊   | 135/200 [05:21<02:36,  2.41s/it]\n",
      "Attention Walk (Loss=4.2866):  68%|██████▊   | 135/200 [05:23<02:36,  2.41s/it]\n",
      "Attention Walk (Loss=4.2866):  68%|██████▊   | 136/200 [05:23<02:32,  2.38s/it]\n",
      "Attention Walk (Loss=4.2858):  68%|██████▊   | 136/200 [05:25<02:32,  2.38s/it]\n",
      "Attention Walk (Loss=4.2858):  68%|██████▊   | 137/200 [05:25<02:30,  2.40s/it]\n",
      "Attention Walk (Loss=4.285):  68%|██████▊   | 137/200 [05:28<02:30,  2.40s/it] \n",
      "Attention Walk (Loss=4.285):  69%|██████▉   | 138/200 [05:28<02:27,  2.38s/it]\n",
      "Attention Walk (Loss=4.2843):  69%|██████▉   | 138/200 [05:30<02:27,  2.38s/it]\n",
      "Attention Walk (Loss=4.2843):  70%|██████▉   | 139/200 [05:30<02:23,  2.35s/it]\n",
      "Attention Walk (Loss=4.2836):  70%|██████▉   | 139/200 [05:32<02:23,  2.35s/it]\n",
      "Attention Walk (Loss=4.2836):  70%|███████   | 140/200 [05:32<02:20,  2.34s/it]\n",
      "Attention Walk (Loss=4.2829):  70%|███████   | 140/200 [05:35<02:20,  2.34s/it]\n",
      "Attention Walk (Loss=4.2829):  70%|███████   | 141/200 [05:35<02:19,  2.36s/it]\n",
      "Attention Walk (Loss=4.2821):  70%|███████   | 141/200 [05:37<02:19,  2.36s/it]\n",
      "Attention Walk (Loss=4.2821):  71%|███████   | 142/200 [05:37<02:16,  2.35s/it]\n",
      "Attention Walk (Loss=4.2815):  71%|███████   | 142/200 [05:39<02:16,  2.35s/it]\n",
      "Attention Walk (Loss=4.2815):  72%|███████▏  | 143/200 [05:39<02:12,  2.33s/it]\n",
      "Attention Walk (Loss=4.2808):  72%|███████▏  | 143/200 [05:42<02:12,  2.33s/it]\n",
      "Attention Walk (Loss=4.2808):  72%|███████▏  | 144/200 [05:42<02:10,  2.32s/it]\n",
      "Attention Walk (Loss=4.2801):  72%|███████▏  | 144/200 [05:44<02:10,  2.32s/it]\n",
      "Attention Walk (Loss=4.2801):  72%|███████▎  | 145/200 [05:44<02:07,  2.33s/it]\n",
      "Attention Walk (Loss=4.2795):  72%|███████▎  | 145/200 [05:46<02:07,  2.33s/it]\n",
      "Attention Walk (Loss=4.2795):  73%|███████▎  | 146/200 [05:46<02:08,  2.38s/it]\n",
      "Attention Walk (Loss=4.2788):  73%|███████▎  | 146/200 [05:49<02:08,  2.38s/it]\n",
      "Attention Walk (Loss=4.2788):  74%|███████▎  | 147/200 [05:49<02:06,  2.39s/it]\n",
      "Attention Walk (Loss=4.2782):  74%|███████▎  | 147/200 [05:51<02:06,  2.39s/it]\n",
      "Attention Walk (Loss=4.2782):  74%|███████▍  | 148/200 [05:51<02:05,  2.41s/it]\n",
      "Attention Walk (Loss=4.2776):  74%|███████▍  | 148/200 [05:54<02:05,  2.41s/it]\n",
      "Attention Walk (Loss=4.2776):  74%|███████▍  | 149/200 [05:54<02:03,  2.41s/it]\n",
      "Attention Walk (Loss=4.277):  74%|███████▍  | 149/200 [05:56<02:03,  2.41s/it] \n",
      "Attention Walk (Loss=4.277):  75%|███████▌  | 150/200 [05:56<02:00,  2.41s/it]\n",
      "Attention Walk (Loss=4.2764):  75%|███████▌  | 150/200 [05:59<02:00,  2.41s/it]\n",
      "Attention Walk (Loss=4.2764):  76%|███████▌  | 151/200 [05:59<01:57,  2.39s/it]\n",
      "Attention Walk (Loss=4.2758):  76%|███████▌  | 151/200 [06:01<01:57,  2.39s/it]\n",
      "Attention Walk (Loss=4.2758):  76%|███████▌  | 152/200 [06:01<01:54,  2.38s/it]\n",
      "Attention Walk (Loss=4.2752):  76%|███████▌  | 152/200 [06:03<01:54,  2.38s/it]\n",
      "Attention Walk (Loss=4.2752):  76%|███████▋  | 153/200 [06:03<01:51,  2.37s/it]\n",
      "Attention Walk (Loss=4.2746):  76%|███████▋  | 153/200 [06:06<01:51,  2.37s/it]\n",
      "Attention Walk (Loss=4.2746):  77%|███████▋  | 154/200 [06:06<01:48,  2.36s/it]\n",
      "Attention Walk (Loss=4.2741):  77%|███████▋  | 154/200 [06:08<01:48,  2.36s/it]\n",
      "Attention Walk (Loss=4.2741):  78%|███████▊  | 155/200 [06:08<01:46,  2.37s/it]\n",
      "Attention Walk (Loss=4.2735):  78%|███████▊  | 155/200 [06:10<01:46,  2.37s/it]\n",
      "Attention Walk (Loss=4.2735):  78%|███████▊  | 156/200 [06:10<01:44,  2.38s/it]\n",
      "Attention Walk (Loss=4.273):  78%|███████▊  | 156/200 [06:13<01:44,  2.38s/it] \n",
      "Attention Walk (Loss=4.273):  78%|███████▊  | 157/200 [06:13<01:42,  2.38s/it]\n",
      "Attention Walk (Loss=4.2724):  78%|███████▊  | 157/200 [06:15<01:42,  2.38s/it]\n",
      "Attention Walk (Loss=4.2724):  79%|███████▉  | 158/200 [06:15<01:40,  2.39s/it]\n",
      "Attention Walk (Loss=4.2719):  79%|███████▉  | 158/200 [06:18<01:40,  2.39s/it]\n",
      "Attention Walk (Loss=4.2719):  80%|███████▉  | 159/200 [06:18<01:38,  2.39s/it]\n",
      "Attention Walk (Loss=4.2714):  80%|███████▉  | 159/200 [06:20<01:38,  2.39s/it]\n",
      "Attention Walk (Loss=4.2714):  80%|████████  | 160/200 [06:20<01:34,  2.37s/it]\n",
      "Attention Walk (Loss=4.2709):  80%|████████  | 160/200 [06:22<01:34,  2.37s/it]\n",
      "Attention Walk (Loss=4.2709):  80%|████████  | 161/200 [06:22<01:33,  2.39s/it]\n",
      "Attention Walk (Loss=4.2704):  80%|████████  | 161/200 [06:25<01:33,  2.39s/it]\n",
      "Attention Walk (Loss=4.2704):  81%|████████  | 162/200 [06:25<01:30,  2.37s/it]\n",
      "Attention Walk (Loss=4.2699):  81%|████████  | 162/200 [06:27<01:30,  2.37s/it]\n",
      "Attention Walk (Loss=4.2699):  82%|████████▏ | 163/200 [06:27<01:28,  2.38s/it]\n",
      "Attention Walk (Loss=4.2694):  82%|████████▏ | 163/200 [06:29<01:28,  2.38s/it]\n",
      "Attention Walk (Loss=4.2694):  82%|████████▏ | 164/200 [06:29<01:25,  2.39s/it]\n",
      "Attention Walk (Loss=4.2689):  82%|████████▏ | 164/200 [06:32<01:25,  2.39s/it]\n",
      "Attention Walk (Loss=4.2689):  82%|████████▎ | 165/200 [06:32<01:23,  2.38s/it]\n",
      "Attention Walk (Loss=4.2685):  82%|████████▎ | 165/200 [06:34<01:23,  2.38s/it]\n",
      "Attention Walk (Loss=4.2685):  83%|████████▎ | 166/200 [06:34<01:21,  2.39s/it]\n",
      "Attention Walk (Loss=4.268):  83%|████████▎ | 166/200 [06:37<01:21,  2.39s/it] \n",
      "Attention Walk (Loss=4.268):  84%|████████▎ | 167/200 [06:37<01:18,  2.38s/it]\n",
      "Attention Walk (Loss=4.2675):  84%|████████▎ | 167/200 [06:39<01:18,  2.38s/it]\n",
      "Attention Walk (Loss=4.2675):  84%|████████▍ | 168/200 [06:39<01:15,  2.36s/it]\n",
      "Attention Walk (Loss=4.2671):  84%|████████▍ | 168/200 [06:41<01:15,  2.36s/it]\n",
      "Attention Walk (Loss=4.2671):  84%|████████▍ | 169/200 [06:41<01:13,  2.36s/it]\n",
      "Attention Walk (Loss=4.2667):  84%|████████▍ | 169/200 [06:44<01:13,  2.36s/it]\n",
      "Attention Walk (Loss=4.2667):  85%|████████▌ | 170/200 [06:44<01:10,  2.35s/it]\n",
      "Attention Walk (Loss=4.2662):  85%|████████▌ | 170/200 [06:46<01:10,  2.35s/it]\n",
      "Attention Walk (Loss=4.2662):  86%|████████▌ | 171/200 [06:46<01:08,  2.35s/it]\n",
      "Attention Walk (Loss=4.2658):  86%|████████▌ | 171/200 [06:48<01:08,  2.35s/it]\n",
      "Attention Walk (Loss=4.2658):  86%|████████▌ | 172/200 [06:48<01:05,  2.35s/it]\n",
      "Attention Walk (Loss=4.2654):  86%|████████▌ | 172/200 [06:51<01:05,  2.35s/it]\n",
      "Attention Walk (Loss=4.2654):  86%|████████▋ | 173/200 [06:51<01:04,  2.39s/it]\n",
      "Attention Walk (Loss=4.2649):  86%|████████▋ | 173/200 [06:53<01:04,  2.39s/it]\n",
      "Attention Walk (Loss=4.2649):  87%|████████▋ | 174/200 [06:53<01:02,  2.39s/it]\n",
      "Attention Walk (Loss=4.2645):  87%|████████▋ | 174/200 [06:55<01:02,  2.39s/it]\n",
      "Attention Walk (Loss=4.2645):  88%|████████▊ | 175/200 [06:55<00:59,  2.38s/it]\n",
      "Attention Walk (Loss=4.2641):  88%|████████▊ | 175/200 [06:58<00:59,  2.38s/it]\n",
      "Attention Walk (Loss=4.2641):  88%|████████▊ | 176/200 [06:58<00:56,  2.36s/it]\n",
      "Attention Walk (Loss=4.2637):  88%|████████▊ | 176/200 [07:00<00:56,  2.36s/it]\n",
      "Attention Walk (Loss=4.2637):  88%|████████▊ | 177/200 [07:00<00:53,  2.35s/it]\n",
      "Attention Walk (Loss=4.2633):  88%|████████▊ | 177/200 [07:02<00:53,  2.35s/it]\n",
      "Attention Walk (Loss=4.2633):  89%|████████▉ | 178/200 [07:02<00:51,  2.34s/it]\n",
      "Attention Walk (Loss=4.2629):  89%|████████▉ | 178/200 [07:05<00:51,  2.34s/it]\n",
      "Attention Walk (Loss=4.2629):  90%|████████▉ | 179/200 [07:05<00:49,  2.36s/it]\n",
      "Attention Walk (Loss=4.2625):  90%|████████▉ | 179/200 [07:07<00:49,  2.36s/it]\n",
      "Attention Walk (Loss=4.2625):  90%|█████████ | 180/200 [07:07<00:47,  2.36s/it]\n",
      "Attention Walk (Loss=4.2621):  90%|█████████ | 180/200 [07:10<00:47,  2.36s/it]\n",
      "Attention Walk (Loss=4.2621):  90%|█████████ | 181/200 [07:10<00:45,  2.38s/it]\n",
      "Attention Walk (Loss=4.2617):  90%|█████████ | 181/200 [07:12<00:45,  2.38s/it]\n",
      "Attention Walk (Loss=4.2617):  91%|█████████ | 182/200 [07:12<00:42,  2.39s/it]\n",
      "Attention Walk (Loss=4.2614):  91%|█████████ | 182/200 [07:14<00:42,  2.39s/it]\n",
      "Attention Walk (Loss=4.2614):  92%|█████████▏| 183/200 [07:14<00:40,  2.40s/it]\n",
      "Attention Walk (Loss=4.261):  92%|█████████▏| 183/200 [07:17<00:40,  2.40s/it] \n",
      "Attention Walk (Loss=4.261):  92%|█████████▏| 184/200 [07:17<00:37,  2.37s/it]\n",
      "Attention Walk (Loss=4.2606):  92%|█████████▏| 184/200 [07:19<00:37,  2.37s/it]\n",
      "Attention Walk (Loss=4.2606):  92%|█████████▎| 185/200 [07:19<00:35,  2.37s/it]\n",
      "Attention Walk (Loss=4.2603):  92%|█████████▎| 185/200 [07:21<00:35,  2.37s/it]\n",
      "Attention Walk (Loss=4.2603):  93%|█████████▎| 186/200 [07:21<00:32,  2.35s/it]\n",
      "Attention Walk (Loss=4.2599):  93%|█████████▎| 186/200 [07:24<00:32,  2.35s/it]\n",
      "Attention Walk (Loss=4.2599):  94%|█████████▎| 187/200 [07:24<00:30,  2.33s/it]\n",
      "Attention Walk (Loss=4.2595):  94%|█████████▎| 187/200 [07:26<00:30,  2.33s/it]\n",
      "Attention Walk (Loss=4.2595):  94%|█████████▍| 188/200 [07:26<00:27,  2.32s/it]\n",
      "Attention Walk (Loss=4.2592):  94%|█████████▍| 188/200 [07:28<00:27,  2.32s/it]\n",
      "Attention Walk (Loss=4.2592):  94%|█████████▍| 189/200 [07:28<00:25,  2.34s/it]\n",
      "Attention Walk (Loss=4.2588):  94%|█████████▍| 189/200 [07:31<00:25,  2.34s/it]\n",
      "Attention Walk (Loss=4.2588):  95%|█████████▌| 190/200 [07:31<00:23,  2.34s/it]\n",
      "Attention Walk (Loss=4.2585):  95%|█████████▌| 190/200 [07:33<00:23,  2.34s/it]\n",
      "Attention Walk (Loss=4.2585):  96%|█████████▌| 191/200 [07:33<00:21,  2.36s/it]\n",
      "Attention Walk (Loss=4.2581):  96%|█████████▌| 191/200 [07:36<00:21,  2.36s/it]\n",
      "Attention Walk (Loss=4.2581):  96%|█████████▌| 192/200 [07:36<00:19,  2.38s/it]\n",
      "Attention Walk (Loss=4.2578):  96%|█████████▌| 192/200 [07:38<00:19,  2.38s/it]\n",
      "Attention Walk (Loss=4.2578):  96%|█████████▋| 193/200 [07:38<00:16,  2.39s/it]\n",
      "Attention Walk (Loss=4.2575):  96%|█████████▋| 193/200 [07:40<00:16,  2.39s/it]\n",
      "Attention Walk (Loss=4.2575):  97%|█████████▋| 194/200 [07:40<00:14,  2.40s/it]\n",
      "Attention Walk (Loss=4.2571):  97%|█████████▋| 194/200 [07:43<00:14,  2.40s/it]\n",
      "Attention Walk (Loss=4.2571):  98%|█████████▊| 195/200 [07:43<00:12,  2.44s/it]\n",
      "Attention Walk (Loss=4.2568):  98%|█████████▊| 195/200 [07:45<00:12,  2.44s/it]\n",
      "Attention Walk (Loss=4.2568):  98%|█████████▊| 196/200 [07:45<00:09,  2.39s/it]\n",
      "Attention Walk (Loss=4.2565):  98%|█████████▊| 196/200 [07:48<00:09,  2.39s/it]\n",
      "Attention Walk (Loss=4.2565):  98%|█████████▊| 197/200 [07:48<00:07,  2.38s/it]\n",
      "Attention Walk (Loss=4.2561):  98%|█████████▊| 197/200 [07:50<00:07,  2.38s/it]\n",
      "Attention Walk (Loss=4.2561):  99%|█████████▉| 198/200 [07:50<00:04,  2.36s/it]\n",
      "Attention Walk (Loss=4.2558):  99%|█████████▉| 198/200 [07:52<00:04,  2.36s/it]\n",
      "Attention Walk (Loss=4.2558): 100%|█████████▉| 199/200 [07:52<00:02,  2.35s/it]\n",
      "Attention Walk (Loss=4.2555): 100%|█████████▉| 199/200 [07:55<00:02,  2.35s/it]\n",
      "Attention Walk (Loss=4.2555): 100%|██████████| 200/200 [07:55<00:00,  2.35s/it]\n",
      "Attention Walk (Loss=4.2555): 100%|██████████| 200/200 [07:55<00:00,  2.38s/it]\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/spam/spam.csv --embedding-path ../../result/embeddings_spam_AW_128.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a558d7",
   "metadata": {},
   "source": [
    "# GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd gae && python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95228f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd gae/gae && python train.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
