{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d121cbd5",
   "metadata": {
    "id": "d121cbd5"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PietroVolpato/lfn_project/blob/main/src/LFN_project_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358080be-e62b-4a82-b8d3-e5becb180aac",
   "metadata": {
    "id": "358080be-e62b-4a82-b8d3-e5becb180aac"
   },
   "source": [
    "# Learning from networks project\n",
    "### Evaluation of different Node Embedding algorithms\n",
    "Members:<br>\n",
    "- D'Emilio Filippo, id : 2120931\n",
    "- Volpato Pietro, id : 2120825\n",
    "\n",
    "### Information about the notebook (have a look at the report for details)\n",
    "This notebook is responsable of computing the embeddings for every embedding technique and for every selected graph.<br>\n",
    "Each computed embedding is saved to file as a numpy array (extension .npy), in the directory /embeddings. In this way that once an embedding is computed, it won't be lost when the runtime of the notebook is terminated.<br>\n",
    "We can then efficiently load the embeddings in the \"test\" notebook, and evaluate the quality of the embeddings.<br>\n",
    "Selected embedding techniques:\n",
    "- Node2Vec\n",
    "- Line\n",
    "- ...\n",
    "\n",
    "For information about the graphs, se cells below.<br>\n",
    "*NOTE*: by implementation choice, the computation of each embedding is computed separately (e.g. there are no function to coincisely compute all embeddings).<br>\n",
    "This choice comes from the fact that computing embeddings is computationally intensive, and we might want to compute only a specific\n",
    "embedding strategy for a specific graph, in order to update only this entry in the folder containing the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d3b60-f774-4fc7-b0bc-7617057d6459",
   "metadata": {
    "id": "bb1d3b60-f774-4fc7-b0bc-7617057d6459"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93987c53-ecce-41d6-948b-4a4b7ac4978a",
   "metadata": {
    "id": "93987c53-ecce-41d6-948b-4a4b7ac4978a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gzip\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b64818-de78-4c30-945e-b95279c7f1c9",
   "metadata": {},
   "source": [
    "# configuration\n",
    "Here you can properly configure the names of the graphs and the names of the embedding strategies. Use meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b26d0-84d0-46ad-83dd-ac826faac4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_keys = [\"facebook\",\"citation\",\"biological\",\"CL\",\"COX2\"]\n",
    "embedding_keys = [\"LINE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6ca3e-b5f5-4869-a3ef-47fb66cd7c56",
   "metadata": {
    "id": "79f6ca3e-b5f5-4869-a3ef-47fb66cd7c56"
   },
   "source": [
    "# Loading the graphs\n",
    "Selected graphs:\n",
    "- Facebook_combined    https://snap.stanford.edu/data/ego-Facebook.html          \n",
    "- cit-Helpth           https://networkrepository.com/cit-HepTh.php             \n",
    "- bio-CE-CX            https://networkrepository.com/bio-CE-CX.php             \n",
    "- CL-100K-1d8-L9       https://networkrepository.com/CL-100K-1d8-L9.php ---- the graph has node labels\n",
    "- COX2-MD              https://networkrepository.com/COX2-MD.php  ---- the graph has node labels\n",
    "\n",
    "To run this notebook, adjust the paths to match where the files are saved in your PC.<br>\n",
    "To keep paths as they are, create a \"data\" folder inside the directory of this notebook, and store the files there.<br><br>\n",
    "\n",
    "Graphs are stored as a dictionary: the key is the graph name, the value is the corresponding netowrkx graph.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "TtpPrXlsIUgJ",
   "metadata": {
    "id": "TtpPrXlsIUgJ"
   },
   "outputs": [],
   "source": [
    "facebook_path = 'data/facebook_combined.txt.gz'\n",
    "citation_path = 'data/cit-HepTh.edges'\n",
    "biological_path = 'data/bio-CE-CX.edges'\n",
    "CL_path = \"data/CL-100K-1d8-L9/CL-100K-1d8-L9.edges\"\n",
    "COX2_path = \"data/COX2-MD/COX2-MD.edges\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f7ed1-5bbd-4e15-8d0c-1a93e9802783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(path):\n",
    "    \"\"\"\n",
    "    For files with extension .edges\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('%'):  # Skip comment lines\n",
    "                continue\n",
    "            # Split the line based on spaces or commas\n",
    "            data = re.split(r'[,\\s]+', line.strip())\n",
    "            if len(data) < 2:  # Skip lines that don't have at least two columns\n",
    "                continue\n",
    "            # Extract the first two columns (nodes)\n",
    "            node1, node2 = int(data[0]), int(data[1])\n",
    "            G.add_edge(node1, node2)\n",
    "    G = nx.convert_node_labels_to_integers(G)  # Relabel nodes to integers\n",
    "    return G\n",
    "\n",
    "def load_graph_with_gz(path):\n",
    "    \"\"\"\n",
    "    For files with extension .txt.gz\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            node1, node2 = map(int, line.strip().split())\n",
    "            G.add_edge(node1, node2)\n",
    "    G = nx.convert_node_labels_to_integers(G)  # Relabel nodes to integers\n",
    "    return G\n",
    "\n",
    "def print_graphs_info(graphs):\n",
    "    for k in graph_keys:\n",
    "        G = graphs[k]\n",
    "        print(f\"{k}: |V|={len(G.nodes)}, |E|={len(G.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ab6207-3fe1-44b0-bf8e-636a7d9f9f2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2ab6207-3fe1-44b0-bf8e-636a7d9f9f2b",
    "outputId": "b810991c-26ec-47d1-b2c3-248de16db6fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook graph: |V|=4039, |E|=88234\n",
      "citation graph: |V|=22908, |E|=2444798\n",
      "biological graph: |V|=15229, |E|=245952\n",
      "CL graph: |V|=92482, |E|=436611\n",
      "COX2 graph: |V|=7962, |E|=101542\n"
     ]
    }
   ],
   "source": [
    "graphs = {}\n",
    "\n",
    "# facebook graph is the only one .tar.gz        \n",
    "graphs[graph_keys[0]] = load_graph_with_gz(facebook_path)  # relabeling nodes to integer\n",
    "graphs[graph_keys[1]] = load_graph(citation_path)\n",
    "graphs[graph_keys[2]] = load_graph(biological_path)\n",
    "graphs[graph_keys[3]] = load_graph(CL_path)  # node labeled\n",
    "graphs[graph_keys[4]] = load_graph(COX2_path)  # node labeled\n",
    "\n",
    "print_graphs_info(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ab17a-9328-47b5-bc28-15dbd59deabf",
   "metadata": {},
   "source": [
    "# Functions and declarations for the embeddings\n",
    "Embedding data structure is defined as following:<br>\n",
    "- The first index refer to the graph (e.g. embeddings[\"facebook\"] contains the embeddings of the facebook graph for every embedding technique).<br>\n",
    "- The second index refer to the embedding technique (e.g. embeddings[\"facebook\"][\"LINE\"] cointans the embedding of facebook graph computed using LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e7e55ae-2aa3-4989-8903-0ccc3898309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(emb, path):\n",
    "    np.save(f\"embeddings/{path}.npy\", emb)\n",
    "    print(f\"Successfully saved the embeddings in embeddings/{path}.npy\")\n",
    "\n",
    "# dictionaries to store the embeddings, obtained by several techniques, for each graph\n",
    "embeddings = {}\n",
    "for k in graph_keys:\n",
    "    embeddings[k] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0aada-e8df-421a-bf3a-d40c09885e40",
   "metadata": {
    "id": "66b0aada-e8df-421a-bf3a-d40c09885e40",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Spiegazione sui parametri di node2vec:<br>\n",
    "- G (required): The graph on which to run Node2Vec. Must be an undirected networkx.Graph object.\n",
    "- dimensions (default = 128): The dimensionality of the node embeddings. Higher dimensions allow for capturing more information but increase computational cost.\n",
    "- walk_length (default = 80): The number of steps for each random walk. A larger walk_length captures more of the network structure.\n",
    "- num_walks (default = 10): The number of random walks to start per node. Increasing this can improve the representation at the cost of additional computation.\n",
    "- workers (default = 1): The number of CPU cores to use for parallel processing. If you're running this on a multi-core machine, increasing this can speed up the computation.\n",
    "- p (return parameter): p<1: Increases the likelihood of revisiting a node (DFS-like behavior). p>1: Discourages revisiting nodes, encouraging exploration (BFS-like behavior).\n",
    "- q (in-out parameter): q<1: Encourages walks to nodes further away from the starting node (BFS-like).q>1: Biases walks to nodes closer to the starting node (DFS-like).\n",
    "\n",
    "Spiegazione di : model = node2vec.fit(window=5, min_count=1, batch_words=4)<br>\n",
    "This trains a Word2Vec model (from the gensim library) using the random walks. Let’s go over the parameters:<br>\n",
    "\n",
    "- window (default = 10): The maximum distance between the current and predicted nodes in the random walk sequence. Larger windows capture more context but require more computation.\n",
    "\n",
    "- min_count (default = 1): Minimum frequency for a node to be considered in the embedding. Since most graphs are sparse, this is often set to 1.\n",
    "\n",
    "- batch_words (default = 4): The number of words (or nodes) processed in each training batch. Adjust this for performance depending on your hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7649b02-023b-4ec3-b1e7-4d051776d0f2",
   "metadata": {
    "id": "d7649b02-023b-4ec3-b1e7-4d051776d0f2"
   },
   "source": [
    "# Node2Vec\n",
    "- pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c54d5a3a-a0ff-4044-8ba4-808588927dba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405,
     "referenced_widgets": [
      "08a2aa42a02244cca504a2553ee3a9d3",
      "121ff0f0d9a940ab9ebf233013e0526a",
      "8a997cdf9e1f4141a65e806dcf974c65",
      "4d686c59f72c4a07a244a1d9183e9855",
      "3b98a185484e4302abe56263ada93980",
      "e587efe0fc234ea4b413cb1ab80bf9ac",
      "255e4240da824b8ab6c035a659407984",
      "385aa92c3d8541dc9de0ef77007a3bfa",
      "1800b26997924888a4fbfea29ceaf1d6",
      "341b42b7c516425bae603d44be7e8aa8",
      "465fcc5fe5cb44189b3647adc27d29d2"
     ]
    },
    "id": "c54d5a3a-a0ff-4044-8ba4-808588927dba",
    "outputId": "866852e5-e669-4cb7-ec76-c02effbe9815"
   },
   "outputs": [],
   "source": [
    "def get_node2vec_embeddings(G, dimensions=128, walk_length=10, num_walks=20, p=1, q=1, workers=1):\n",
    "    \"\"\"\n",
    "    Generate node embeddings for a graph using the Node2Vec algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph): \n",
    "            The input graph for which embeddings are to be generated. \n",
    "            The graph should have nodes labeled as integers, ideally sequentially starting from 0.\n",
    "\n",
    "        dimensions (int, optional): \n",
    "            The dimensionality of the embedding space. Default is 128.\n",
    "\n",
    "        walk_length (int, optional): \n",
    "            The length of each random walk. Default is 10.\n",
    "\n",
    "        num_walks (int, optional): \n",
    "            The number of random walks to start from each node. Default is 20.\n",
    "\n",
    "        p (float, optional): \n",
    "            The return parameter, controlling the likelihood of immediately revisiting a node in the walk. \n",
    "            A higher value makes it more likely to backtrack. Default is 1.\n",
    "\n",
    "        q (float, optional): \n",
    "            The in-out parameter, controlling the likelihood of exploring outward from the starting node. \n",
    "            A higher value makes it more likely to move outward. Default is 1.\n",
    "\n",
    "        workers (int, optional): \n",
    "            The number of parallel workers for random walk generation and model training. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            A NumPy array where each row represents the embedding of a node.\n",
    "            The row index corresponds to the node ID, and each row has `dimensions` elements.\n",
    "    \"\"\"\n",
    "    # Initialize Node2Vec model\n",
    "    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, p=p, q=q, workers=workers)\n",
    "    \n",
    "    # Fit the Node2Vec model and generate embeddings\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    \n",
    "    # Convert embeddings to a NumPy array\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    embeddings = np.zeros((num_nodes, dimensions))  # Preallocate array\n",
    "    for node in G.nodes:\n",
    "        embeddings[node] = model.wv[node]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535202c-086c-42ae-a7a0-2bce62d42e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = \"node2vec\"\n",
    "embeddings[graph_keys[0][curr]] = get_node2vec_embeddings(graphs[graph_keys[0]])\n",
    "save(embeddings[graph_keys[0][curr]], f\"embeddings_{graph_keys[0]}_{curr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c4f4c-6f35-4c15-bff0-837960ef3411",
   "metadata": {},
   "source": [
    "# LINE : Large-scale information network embedding\n",
    "installation guide:\n",
    "- git clone https://github.com/VahidooX/LINE.git\n",
    "- !pip install keras\n",
    "- !pip install tensorflow\n",
    "- adjust the sys.path to where you downloaded line repository\n",
    "\n",
    "*NOTE*: it was necessary to modify utils.py to adapt it at current version of keras. Some elements were deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e6cc917-5939-45b8-aca5-d192cb0fd1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'C:\\Users\\oppil\\OneDrive\\Desktop\\Universita\\magistrale\\2_1\\LFN\\LINE')\n",
    "\n",
    "from model import create_model\n",
    "from utils import batchgen_train\n",
    "\n",
    "def get_LINE_embeddings(G, embedding_dim=128, batch_size=1024, negative_ratio=5, epochs=10, negative_sampling=\"UNIFORM\"):\n",
    "    \"\"\"\n",
    "    Generate LINE embeddings for a given graph.\n",
    "\n",
    "    Parameters:\n",
    "        G (nx.Graph): The graph for which embeddings are computed.\n",
    "        embedding_dim (int): Dimensionality of the embeddings.\n",
    "        batch_size (int): Batch size for training.\n",
    "        negative_ratio (int): Ratio of negative to positive samples.\n",
    "        epochs (int): Number of training epochs.\n",
    "        negative_sampling (str): Negative sampling strategy (\"UNIFORM\" or \"NON-UNIFORM\").\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Node embeddings (shape: [num_nodes, embedding_dim]).\n",
    "    \"\"\"\n",
    "    num_nodes = G.number_of_nodes()\n",
    "\n",
    "    # Convert networkx.Graph to adj_list (edge list as 2D numpy array)\n",
    "    adj_list = np.array(list(G.edges()), dtype=np.int32)\n",
    "\n",
    "    # Create LINE model\n",
    "    model, embed_generator = create_model(num_nodes, embedding_dim)\n",
    "\n",
    "    # Generate training batches\n",
    "    train_gen = batchgen_train(adj_list, num_nodes, batch_size, negative_ratio, negative_sampling)\n",
    "\n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    model.fit(train_gen, steps_per_epoch=500, epochs=epochs)\n",
    "\n",
    "    # Extract embeddings\n",
    "    node_ids = np.arange(num_nodes)  # Sequential node IDs\n",
    "    embeddings = embed_generator.predict_on_batch(node_ids)\n",
    "\n",
    "    print(\"Node Embeddings Shape:\", embeddings[0].shape)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ae91dd-e0ae-47ae-b50b-42de4dfe3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = \"LINE\"\n",
    "embeddings[graph_keys[0][curr]] = get_LINE_embeddings(graphs[graph_keys[0]])\n",
    "save(embeddings[graph_keys[0][curr]], f\"embeddings_{graph_keys[0]}_{curr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8781445-1246-42e1-ac00-0cab10620d6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings_biological[\"LINE\"] = get_LINE_embeddings(G_biological)\n",
    "save(embeddings_biological[\"LINE\"],\"embeddings_biological_LINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e6cb38-7b60-44a0-a208-e32f5e4f47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_CL[\"LINE\"] = get_LINE_embeddings(G_CL)\n",
    "save(embeddings_CL[\"LINE\"],\"embeddings_CL_LINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44b0134c-db53-454d-a56a-931969197e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_citation[\"LINE\"] = get_LINE_embeddings(G_citation)\n",
    "save(embeddings_citation[\"LINE\"],\"embeddings_citation_LINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d14519-2db5-404a-a52d-45645752aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_COX2[\"LINE\"] = get_LINE_embeddings(G_COX2)\n",
    "save(embeddings_COX2[\"LINE\"],\"embeddings_COX2_LINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc4a49",
   "metadata": {},
   "source": [
    "# AttentionWalk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f540988",
   "metadata": {},
   "source": [
    "git clone https://github.com/benedekrozemberczki/AttentionWalk.git\n",
    "pip install texttable==1.6.7\n",
    "\n",
    "Richiede solo file .csv in input quindi usare il grafo di facebook con estensione .csv. Per farlo partire usando quel grafo e non quello di default bisogna specificare il percorso del file dopo aver chiamato il file python. Per salvare i risultati bisogna specificare il percorso del file\n",
    "\n",
    "Esempio: python src/main.py --edge-path ../facebook_edges.csv --ebmedding-path ../facebook_embedding.csv\n",
    "Io lo ho testato con tutto il resto di default che trovi nella tabella alla fine dell'esecuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b872e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'AttentionWalk'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/benedekrozemberczki/AttentionWalk.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting texttable==1.6.7\n",
      "  Downloading texttable-1.6.7-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: texttable\n",
      "Successfully installed texttable-1.6.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install texttable==1.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fc24b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------------------------------+\n",
      "| Attention path |      ../../result/facebook_attention.csv       |\n",
      "+================+================================================+\n",
      "| Beta           | 0.500                                          |\n",
      "+----------------+------------------------------------------------+\n",
      "| Dimensions     | 128                                            |\n",
      "+----------------+------------------------------------------------+\n",
      "| Edge path      | ../../data/facebook_edges.csv                  |\n",
      "+----------------+------------------------------------------------+\n",
      "| Embedding path | ../../result/facebook_embeddings_attention.csv |\n",
      "+----------------+------------------------------------------------+\n",
      "| Epochs         | 200                                            |\n",
      "+----------------+------------------------------------------------+\n",
      "| Gamma          | 0.500                                          |\n",
      "+----------------+------------------------------------------------+\n",
      "| Learning rate  | 0.010                                          |\n",
      "+----------------+------------------------------------------------+\n",
      "| Num of walks   | 80                                             |\n",
      "+----------------+------------------------------------------------+\n",
      "| Window size    | 5                                              |\n",
      "+----------------+------------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n",
      "\n",
      "Training the model.\n",
      "\n",
      "\n",
      "Saving the model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjacency matrix powers:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:  25%|██▌       | 1/4 [00:00<00:00,  7.79it/s]\n",
      "Adjacency matrix powers:  50%|█████     | 2/4 [00:00<00:01,  1.81it/s]\n",
      "Adjacency matrix powers:  75%|███████▌  | 3/4 [00:02<00:01,  1.14s/it]\n",
      "Adjacency matrix powers: 100%|██████████| 4/4 [00:06<00:00,  1.98s/it]\n",
      "Adjacency matrix powers: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n",
      "\n",
      "Loss:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Attention Walk (Loss=48.3153):   0%|          | 0/200 [00:01<?, ?it/s]\n",
      "Attention Walk (Loss=48.3153):   0%|          | 1/200 [00:01<04:10,  1.26s/it]\n",
      "Attention Walk (Loss=48.3048):   0%|          | 1/200 [00:02<04:10,  1.26s/it]\n",
      "Attention Walk (Loss=48.3048):   1%|          | 2/200 [00:02<03:59,  1.21s/it]\n",
      "Attention Walk (Loss=48.1288):   1%|          | 2/200 [00:03<03:59,  1.21s/it]\n",
      "Attention Walk (Loss=48.1288):   2%|▏         | 3/200 [00:03<03:56,  1.20s/it]\n",
      "Attention Walk (Loss=47.8296):   2%|▏         | 3/200 [00:04<03:56,  1.20s/it]\n",
      "Attention Walk (Loss=47.8296):   2%|▏         | 4/200 [00:04<03:52,  1.18s/it]\n",
      "Attention Walk (Loss=47.3242):   2%|▏         | 4/200 [00:05<03:52,  1.18s/it]\n",
      "Attention Walk (Loss=47.3242):   2%|▎         | 5/200 [00:05<03:39,  1.12s/it]\n",
      "Attention Walk (Loss=46.5763):   2%|▎         | 5/200 [00:06<03:39,  1.12s/it]\n",
      "Attention Walk (Loss=46.5763):   3%|▎         | 6/200 [00:06<03:33,  1.10s/it]\n",
      "Attention Walk (Loss=45.5699):   3%|▎         | 6/200 [00:07<03:33,  1.10s/it]\n",
      "Attention Walk (Loss=45.5699):   4%|▎         | 7/200 [00:07<03:28,  1.08s/it]\n",
      "Attention Walk (Loss=44.293):   4%|▎         | 7/200 [00:08<03:28,  1.08s/it] \n",
      "Attention Walk (Loss=44.293):   4%|▍         | 8/200 [00:08<03:25,  1.07s/it]\n",
      "Attention Walk (Loss=42.7393):   4%|▍         | 8/200 [00:09<03:25,  1.07s/it]\n",
      "Attention Walk (Loss=42.7393):   4%|▍         | 9/200 [00:09<03:22,  1.06s/it]\n",
      "Attention Walk (Loss=40.9115):   4%|▍         | 9/200 [00:11<03:22,  1.06s/it]\n",
      "Attention Walk (Loss=40.9115):   5%|▌         | 10/200 [00:11<03:21,  1.06s/it]\n",
      "Attention Walk (Loss=38.823):   5%|▌         | 10/200 [00:12<03:21,  1.06s/it] \n",
      "Attention Walk (Loss=38.823):   6%|▌         | 11/200 [00:12<03:19,  1.06s/it]\n",
      "Attention Walk (Loss=36.4983):   6%|▌         | 11/200 [00:13<03:19,  1.06s/it]\n",
      "Attention Walk (Loss=36.4983):   6%|▌         | 12/200 [00:13<03:18,  1.06s/it]\n",
      "Attention Walk (Loss=33.9725):   6%|▌         | 12/200 [00:14<03:18,  1.06s/it]\n",
      "Attention Walk (Loss=33.9725):   6%|▋         | 13/200 [00:14<03:18,  1.06s/it]\n",
      "Attention Walk (Loss=31.2918):   6%|▋         | 13/200 [00:15<03:18,  1.06s/it]\n",
      "Attention Walk (Loss=31.2918):   7%|▋         | 14/200 [00:15<03:16,  1.06s/it]\n",
      "Attention Walk (Loss=28.512):   7%|▋         | 14/200 [00:16<03:16,  1.06s/it] \n",
      "Attention Walk (Loss=28.512):   8%|▊         | 15/200 [00:16<03:13,  1.05s/it]\n",
      "Attention Walk (Loss=25.6964):   8%|▊         | 15/200 [00:17<03:13,  1.05s/it]\n",
      "Attention Walk (Loss=25.6964):   8%|▊         | 16/200 [00:17<03:12,  1.05s/it]\n",
      "Attention Walk (Loss=22.9124):   8%|▊         | 16/200 [00:18<03:12,  1.05s/it]\n",
      "Attention Walk (Loss=22.9124):   8%|▊         | 17/200 [00:18<03:10,  1.04s/it]\n",
      "Attention Walk (Loss=20.2265):   8%|▊         | 17/200 [00:19<03:10,  1.04s/it]\n",
      "Attention Walk (Loss=20.2265):   9%|▉         | 18/200 [00:19<03:08,  1.04s/it]\n",
      "Attention Walk (Loss=17.699):   9%|▉         | 18/200 [00:20<03:08,  1.04s/it] \n",
      "Attention Walk (Loss=17.699):  10%|▉         | 19/200 [00:20<03:07,  1.04s/it]\n",
      "Attention Walk (Loss=15.3788):  10%|▉         | 19/200 [00:21<03:07,  1.04s/it]\n",
      "Attention Walk (Loss=15.3788):  10%|█         | 20/200 [00:21<03:05,  1.03s/it]\n",
      "Attention Walk (Loss=13.299):  10%|█         | 20/200 [00:22<03:05,  1.03s/it] \n",
      "Attention Walk (Loss=13.299):  10%|█         | 21/200 [00:22<03:05,  1.04s/it]\n",
      "Attention Walk (Loss=11.4756):  10%|█         | 21/200 [00:23<03:05,  1.04s/it]\n",
      "Attention Walk (Loss=11.4756):  11%|█         | 22/200 [00:23<03:04,  1.04s/it]\n",
      "Attention Walk (Loss=9.9082):  11%|█         | 22/200 [00:24<03:04,  1.04s/it] \n",
      "Attention Walk (Loss=9.9082):  12%|█▏        | 23/200 [00:24<03:03,  1.04s/it]\n",
      "Attention Walk (Loss=8.5829):  12%|█▏        | 23/200 [00:25<03:03,  1.04s/it]\n",
      "Attention Walk (Loss=8.5829):  12%|█▏        | 24/200 [00:25<03:02,  1.04s/it]\n",
      "Attention Walk (Loss=7.4769):  12%|█▏        | 24/200 [00:26<03:02,  1.04s/it]\n",
      "Attention Walk (Loss=7.4769):  12%|█▎        | 25/200 [00:26<03:00,  1.03s/it]\n",
      "Attention Walk (Loss=6.5626):  12%|█▎        | 25/200 [00:27<03:00,  1.03s/it]\n",
      "Attention Walk (Loss=6.5626):  13%|█▎        | 26/200 [00:27<02:58,  1.02s/it]\n",
      "Attention Walk (Loss=5.8114):  13%|█▎        | 26/200 [00:28<02:58,  1.02s/it]\n",
      "Attention Walk (Loss=5.8114):  14%|█▎        | 27/200 [00:28<02:56,  1.02s/it]\n",
      "Attention Walk (Loss=5.1962):  14%|█▎        | 27/200 [00:29<02:56,  1.02s/it]\n",
      "Attention Walk (Loss=5.1962):  14%|█▍        | 28/200 [00:29<02:55,  1.02s/it]\n",
      "Attention Walk (Loss=4.6929):  14%|█▍        | 28/200 [00:30<02:55,  1.02s/it]\n",
      "Attention Walk (Loss=4.6929):  14%|█▍        | 29/200 [00:30<02:53,  1.01s/it]\n",
      "Attention Walk (Loss=4.281):  14%|█▍        | 29/200 [00:31<02:53,  1.01s/it] \n",
      "Attention Walk (Loss=4.281):  15%|█▌        | 30/200 [00:31<02:52,  1.01s/it]\n",
      "Attention Walk (Loss=3.9432):  15%|█▌        | 30/200 [00:32<02:52,  1.01s/it]\n",
      "Attention Walk (Loss=3.9432):  16%|█▌        | 31/200 [00:32<02:51,  1.01s/it]\n",
      "Attention Walk (Loss=3.6656):  16%|█▌        | 31/200 [00:33<02:51,  1.01s/it]\n",
      "Attention Walk (Loss=3.6656):  16%|█▌        | 32/200 [00:33<02:50,  1.01s/it]\n",
      "Attention Walk (Loss=3.4368):  16%|█▌        | 32/200 [00:34<02:50,  1.01s/it]\n",
      "Attention Walk (Loss=3.4368):  16%|█▋        | 33/200 [00:34<02:49,  1.02s/it]\n",
      "Attention Walk (Loss=3.2477):  16%|█▋        | 33/200 [00:35<02:49,  1.02s/it]\n",
      "Attention Walk (Loss=3.2477):  17%|█▋        | 34/200 [00:35<02:48,  1.02s/it]\n",
      "Attention Walk (Loss=3.0908):  17%|█▋        | 34/200 [00:36<02:48,  1.02s/it]\n",
      "Attention Walk (Loss=3.0908):  18%|█▊        | 35/200 [00:36<02:48,  1.02s/it]\n",
      "Attention Walk (Loss=2.9603):  18%|█▊        | 35/200 [00:37<02:48,  1.02s/it]\n",
      "Attention Walk (Loss=2.9603):  18%|█▊        | 36/200 [00:37<02:46,  1.02s/it]\n",
      "Attention Walk (Loss=2.8513):  18%|█▊        | 36/200 [00:38<02:46,  1.02s/it]\n",
      "Attention Walk (Loss=2.8513):  18%|█▊        | 37/200 [00:38<02:45,  1.02s/it]\n",
      "Attention Walk (Loss=2.7599):  18%|█▊        | 37/200 [00:39<02:45,  1.02s/it]\n",
      "Attention Walk (Loss=2.7599):  19%|█▉        | 38/200 [00:39<02:46,  1.03s/it]\n",
      "Attention Walk (Loss=2.6829):  19%|█▉        | 38/200 [00:40<02:46,  1.03s/it]\n",
      "Attention Walk (Loss=2.6829):  20%|█▉        | 39/200 [00:40<02:46,  1.03s/it]\n",
      "Attention Walk (Loss=2.6179):  20%|█▉        | 39/200 [00:41<02:46,  1.03s/it]\n",
      "Attention Walk (Loss=2.6179):  20%|██        | 40/200 [00:41<02:44,  1.03s/it]\n",
      "Attention Walk (Loss=2.5626):  20%|██        | 40/200 [00:42<02:44,  1.03s/it]\n",
      "Attention Walk (Loss=2.5626):  20%|██        | 41/200 [00:42<02:45,  1.04s/it]\n",
      "Attention Walk (Loss=2.5153):  20%|██        | 41/200 [00:44<02:45,  1.04s/it]\n",
      "Attention Walk (Loss=2.5153):  21%|██        | 42/200 [00:44<02:48,  1.07s/it]\n",
      "Attention Walk (Loss=2.4746):  21%|██        | 42/200 [00:45<02:48,  1.07s/it]\n",
      "Attention Walk (Loss=2.4746):  22%|██▏       | 43/200 [00:45<02:46,  1.06s/it]\n",
      "Attention Walk (Loss=2.4394):  22%|██▏       | 43/200 [00:46<02:46,  1.06s/it]\n",
      "Attention Walk (Loss=2.4394):  22%|██▏       | 44/200 [00:46<02:44,  1.06s/it]\n",
      "Attention Walk (Loss=2.4087):  22%|██▏       | 44/200 [00:47<02:44,  1.06s/it]\n",
      "Attention Walk (Loss=2.4087):  22%|██▎       | 45/200 [00:47<02:41,  1.05s/it]\n",
      "Attention Walk (Loss=2.3816):  22%|██▎       | 45/200 [00:48<02:41,  1.05s/it]\n",
      "Attention Walk (Loss=2.3816):  23%|██▎       | 46/200 [00:48<02:41,  1.05s/it]\n",
      "Attention Walk (Loss=2.3576):  23%|██▎       | 46/200 [00:49<02:41,  1.05s/it]\n",
      "Attention Walk (Loss=2.3576):  24%|██▎       | 47/200 [00:49<02:39,  1.04s/it]\n",
      "Attention Walk (Loss=2.3362):  24%|██▎       | 47/200 [00:50<02:39,  1.04s/it]\n",
      "Attention Walk (Loss=2.3362):  24%|██▍       | 48/200 [00:50<02:37,  1.04s/it]\n",
      "Attention Walk (Loss=2.3168):  24%|██▍       | 48/200 [00:51<02:37,  1.04s/it]\n",
      "Attention Walk (Loss=2.3168):  24%|██▍       | 49/200 [00:51<02:37,  1.04s/it]\n",
      "Attention Walk (Loss=2.2992):  24%|██▍       | 49/200 [00:52<02:37,  1.04s/it]\n",
      "Attention Walk (Loss=2.2992):  25%|██▌       | 50/200 [00:52<02:36,  1.04s/it]\n",
      "Attention Walk (Loss=2.283):  25%|██▌       | 50/200 [00:53<02:36,  1.04s/it] \n",
      "Attention Walk (Loss=2.283):  26%|██▌       | 51/200 [00:53<02:36,  1.05s/it]\n",
      "Attention Walk (Loss=2.2681):  26%|██▌       | 51/200 [00:54<02:36,  1.05s/it]\n",
      "Attention Walk (Loss=2.2681):  26%|██▌       | 52/200 [00:54<02:34,  1.04s/it]\n",
      "Attention Walk (Loss=2.2542):  26%|██▌       | 52/200 [00:55<02:34,  1.04s/it]\n",
      "Attention Walk (Loss=2.2542):  26%|██▋       | 53/200 [00:55<02:33,  1.04s/it]\n",
      "Attention Walk (Loss=2.2412):  26%|██▋       | 53/200 [00:56<02:33,  1.04s/it]\n",
      "Attention Walk (Loss=2.2412):  27%|██▋       | 54/200 [00:56<02:32,  1.04s/it]\n",
      "Attention Walk (Loss=2.229):  27%|██▋       | 54/200 [00:57<02:32,  1.04s/it] \n",
      "Attention Walk (Loss=2.229):  28%|██▊       | 55/200 [00:57<02:31,  1.04s/it]\n",
      "Attention Walk (Loss=2.2174):  28%|██▊       | 55/200 [00:58<02:31,  1.04s/it]\n",
      "Attention Walk (Loss=2.2174):  28%|██▊       | 56/200 [00:58<02:30,  1.04s/it]\n",
      "Attention Walk (Loss=2.2064):  28%|██▊       | 56/200 [00:59<02:30,  1.04s/it]\n",
      "Attention Walk (Loss=2.2064):  28%|██▊       | 57/200 [00:59<02:28,  1.04s/it]\n",
      "Attention Walk (Loss=2.196):  28%|██▊       | 57/200 [01:00<02:28,  1.04s/it] \n",
      "Attention Walk (Loss=2.196):  29%|██▉       | 58/200 [01:00<02:26,  1.04s/it]\n",
      "Attention Walk (Loss=2.1859):  29%|██▉       | 58/200 [01:01<02:26,  1.04s/it]\n",
      "Attention Walk (Loss=2.1859):  30%|██▉       | 59/200 [01:01<02:25,  1.03s/it]\n",
      "Attention Walk (Loss=2.1763):  30%|██▉       | 59/200 [01:02<02:25,  1.03s/it]\n",
      "Attention Walk (Loss=2.1763):  30%|███       | 60/200 [01:02<02:25,  1.04s/it]\n",
      "Attention Walk (Loss=2.1671):  30%|███       | 60/200 [01:03<02:25,  1.04s/it]\n",
      "Attention Walk (Loss=2.1671):  30%|███       | 61/200 [01:03<02:24,  1.04s/it]\n",
      "Attention Walk (Loss=2.1583):  30%|███       | 61/200 [01:04<02:24,  1.04s/it]\n",
      "Attention Walk (Loss=2.1583):  31%|███       | 62/200 [01:04<02:23,  1.04s/it]\n",
      "Attention Walk (Loss=2.1497):  31%|███       | 62/200 [01:05<02:23,  1.04s/it]\n",
      "Attention Walk (Loss=2.1497):  32%|███▏      | 63/200 [01:05<02:22,  1.04s/it]\n",
      "Attention Walk (Loss=2.1415):  32%|███▏      | 63/200 [01:06<02:22,  1.04s/it]\n",
      "Attention Walk (Loss=2.1415):  32%|███▏      | 64/200 [01:06<02:20,  1.04s/it]\n",
      "Attention Walk (Loss=2.1336):  32%|███▏      | 64/200 [01:08<02:20,  1.04s/it]\n",
      "Attention Walk (Loss=2.1336):  32%|███▎      | 65/200 [01:08<02:19,  1.04s/it]\n",
      "Attention Walk (Loss=2.1259):  32%|███▎      | 65/200 [01:09<02:19,  1.04s/it]\n",
      "Attention Walk (Loss=2.1259):  33%|███▎      | 66/200 [01:09<02:19,  1.04s/it]\n",
      "Attention Walk (Loss=2.1185):  33%|███▎      | 66/200 [01:10<02:19,  1.04s/it]\n",
      "Attention Walk (Loss=2.1185):  34%|███▎      | 67/200 [01:10<02:17,  1.04s/it]\n",
      "Attention Walk (Loss=2.1113):  34%|███▎      | 67/200 [01:11<02:17,  1.04s/it]\n",
      "Attention Walk (Loss=2.1113):  34%|███▍      | 68/200 [01:11<02:16,  1.04s/it]\n",
      "Attention Walk (Loss=2.1044):  34%|███▍      | 68/200 [01:12<02:16,  1.04s/it]\n",
      "Attention Walk (Loss=2.1044):  34%|███▍      | 69/200 [01:12<02:15,  1.04s/it]\n",
      "Attention Walk (Loss=2.0977):  34%|███▍      | 69/200 [01:13<02:15,  1.04s/it]\n",
      "Attention Walk (Loss=2.0977):  35%|███▌      | 70/200 [01:13<02:15,  1.04s/it]\n",
      "Attention Walk (Loss=2.0912):  35%|███▌      | 70/200 [01:14<02:15,  1.04s/it]\n",
      "Attention Walk (Loss=2.0912):  36%|███▌      | 71/200 [01:14<02:14,  1.04s/it]\n",
      "Attention Walk (Loss=2.0849):  36%|███▌      | 71/200 [01:15<02:14,  1.04s/it]\n",
      "Attention Walk (Loss=2.0849):  36%|███▌      | 72/200 [01:15<02:18,  1.08s/it]\n",
      "Attention Walk (Loss=2.0788):  36%|███▌      | 72/200 [01:16<02:18,  1.08s/it]\n",
      "Attention Walk (Loss=2.0788):  36%|███▋      | 73/200 [01:16<02:18,  1.09s/it]\n",
      "Attention Walk (Loss=2.0729):  36%|███▋      | 73/200 [01:17<02:18,  1.09s/it]\n",
      "Attention Walk (Loss=2.0729):  37%|███▋      | 74/200 [01:17<02:16,  1.09s/it]\n",
      "Attention Walk (Loss=2.0671):  37%|███▋      | 74/200 [01:18<02:16,  1.09s/it]\n",
      "Attention Walk (Loss=2.0671):  38%|███▊      | 75/200 [01:18<02:14,  1.08s/it]\n",
      "Attention Walk (Loss=2.0616):  38%|███▊      | 75/200 [01:19<02:14,  1.08s/it]\n",
      "Attention Walk (Loss=2.0616):  38%|███▊      | 76/200 [01:19<02:13,  1.08s/it]\n",
      "Attention Walk (Loss=2.0562):  38%|███▊      | 76/200 [01:20<02:13,  1.08s/it]\n",
      "Attention Walk (Loss=2.0562):  38%|███▊      | 77/200 [01:20<02:11,  1.07s/it]\n",
      "Attention Walk (Loss=2.0509):  38%|███▊      | 77/200 [01:21<02:11,  1.07s/it]\n",
      "Attention Walk (Loss=2.0509):  39%|███▉      | 78/200 [01:21<02:10,  1.07s/it]\n",
      "Attention Walk (Loss=2.0458):  39%|███▉      | 78/200 [01:22<02:10,  1.07s/it]\n",
      "Attention Walk (Loss=2.0458):  40%|███▉      | 79/200 [01:22<02:07,  1.05s/it]\n",
      "Attention Walk (Loss=2.0409):  40%|███▉      | 79/200 [01:23<02:07,  1.05s/it]\n",
      "Attention Walk (Loss=2.0409):  40%|████      | 80/200 [01:23<02:05,  1.05s/it]\n",
      "Attention Walk (Loss=2.0361):  40%|████      | 80/200 [01:24<02:05,  1.05s/it]\n",
      "Attention Walk (Loss=2.0361):  40%|████      | 81/200 [01:24<02:04,  1.04s/it]\n",
      "Attention Walk (Loss=2.0314):  40%|████      | 81/200 [01:26<02:04,  1.04s/it]\n",
      "Attention Walk (Loss=2.0314):  41%|████      | 82/200 [01:26<02:03,  1.04s/it]\n",
      "Attention Walk (Loss=2.0268):  41%|████      | 82/200 [01:27<02:03,  1.04s/it]\n",
      "Attention Walk (Loss=2.0268):  42%|████▏     | 83/200 [01:27<02:01,  1.04s/it]\n",
      "Attention Walk (Loss=2.0224):  42%|████▏     | 83/200 [01:28<02:01,  1.04s/it]\n",
      "Attention Walk (Loss=2.0224):  42%|████▏     | 84/200 [01:28<02:00,  1.04s/it]\n",
      "Attention Walk (Loss=2.018):  42%|████▏     | 84/200 [01:29<02:00,  1.04s/it] \n",
      "Attention Walk (Loss=2.018):  42%|████▎     | 85/200 [01:29<01:59,  1.04s/it]\n",
      "Attention Walk (Loss=2.0138):  42%|████▎     | 85/200 [01:30<01:59,  1.04s/it]\n",
      "Attention Walk (Loss=2.0138):  43%|████▎     | 86/200 [01:30<01:58,  1.04s/it]\n",
      "Attention Walk (Loss=2.0097):  43%|████▎     | 86/200 [01:31<01:58,  1.04s/it]\n",
      "Attention Walk (Loss=2.0097):  44%|████▎     | 87/200 [01:31<01:56,  1.03s/it]\n",
      "Attention Walk (Loss=2.0057):  44%|████▎     | 87/200 [01:32<01:56,  1.03s/it]\n",
      "Attention Walk (Loss=2.0057):  44%|████▍     | 88/200 [01:32<01:56,  1.04s/it]\n",
      "Attention Walk (Loss=2.0018):  44%|████▍     | 88/200 [01:33<01:56,  1.04s/it]\n",
      "Attention Walk (Loss=2.0018):  44%|████▍     | 89/200 [01:33<01:54,  1.03s/it]\n",
      "Attention Walk (Loss=1.998):  44%|████▍     | 89/200 [01:34<01:54,  1.03s/it] \n",
      "Attention Walk (Loss=1.998):  45%|████▌     | 90/200 [01:34<01:54,  1.04s/it]\n",
      "Attention Walk (Loss=1.9942):  45%|████▌     | 90/200 [01:35<01:54,  1.04s/it]\n",
      "Attention Walk (Loss=1.9942):  46%|████▌     | 91/200 [01:35<01:52,  1.03s/it]\n",
      "Attention Walk (Loss=1.9906):  46%|████▌     | 91/200 [01:36<01:52,  1.03s/it]\n",
      "Attention Walk (Loss=1.9906):  46%|████▌     | 92/200 [01:36<01:52,  1.04s/it]\n",
      "Attention Walk (Loss=1.987):  46%|████▌     | 92/200 [01:37<01:52,  1.04s/it] \n",
      "Attention Walk (Loss=1.987):  46%|████▋     | 93/200 [01:37<01:51,  1.04s/it]\n",
      "Attention Walk (Loss=1.9835):  46%|████▋     | 93/200 [01:38<01:51,  1.04s/it]\n",
      "Attention Walk (Loss=1.9835):  47%|████▋     | 94/200 [01:38<01:49,  1.03s/it]\n",
      "Attention Walk (Loss=1.9801):  47%|████▋     | 94/200 [01:39<01:49,  1.03s/it]\n",
      "Attention Walk (Loss=1.9801):  48%|████▊     | 95/200 [01:39<01:48,  1.03s/it]\n",
      "Attention Walk (Loss=1.9768):  48%|████▊     | 95/200 [01:40<01:48,  1.03s/it]\n",
      "Attention Walk (Loss=1.9768):  48%|████▊     | 96/200 [01:40<01:47,  1.03s/it]\n",
      "Attention Walk (Loss=1.9735):  48%|████▊     | 96/200 [01:41<01:47,  1.03s/it]\n",
      "Attention Walk (Loss=1.9735):  48%|████▊     | 97/200 [01:41<01:46,  1.03s/it]\n",
      "Attention Walk (Loss=1.9703):  48%|████▊     | 97/200 [01:42<01:46,  1.03s/it]\n",
      "Attention Walk (Loss=1.9703):  49%|████▉     | 98/200 [01:42<01:45,  1.03s/it]\n",
      "Attention Walk (Loss=1.9672):  49%|████▉     | 98/200 [01:43<01:45,  1.03s/it]\n",
      "Attention Walk (Loss=1.9672):  50%|████▉     | 99/200 [01:43<01:44,  1.03s/it]\n",
      "Attention Walk (Loss=1.9641):  50%|████▉     | 99/200 [01:44<01:44,  1.03s/it]\n",
      "Attention Walk (Loss=1.9641):  50%|█████     | 100/200 [01:44<01:42,  1.03s/it]\n",
      "Attention Walk (Loss=1.961):  50%|█████     | 100/200 [01:45<01:42,  1.03s/it] \n",
      "Attention Walk (Loss=1.961):  50%|█████     | 101/200 [01:45<01:42,  1.03s/it]\n",
      "Attention Walk (Loss=1.9581):  50%|█████     | 101/200 [01:46<01:42,  1.03s/it]\n",
      "Attention Walk (Loss=1.9581):  51%|█████     | 102/200 [01:46<01:41,  1.03s/it]\n",
      "Attention Walk (Loss=1.9551):  51%|█████     | 102/200 [01:47<01:41,  1.03s/it]\n",
      "Attention Walk (Loss=1.9551):  52%|█████▏    | 103/200 [01:47<01:39,  1.03s/it]\n",
      "Attention Walk (Loss=1.9523):  52%|█████▏    | 103/200 [01:48<01:39,  1.03s/it]\n",
      "Attention Walk (Loss=1.9523):  52%|█████▏    | 104/200 [01:48<01:38,  1.03s/it]\n",
      "Attention Walk (Loss=1.9495):  52%|█████▏    | 104/200 [01:49<01:38,  1.03s/it]\n",
      "Attention Walk (Loss=1.9495):  52%|█████▎    | 105/200 [01:49<01:38,  1.03s/it]\n",
      "Attention Walk (Loss=1.9467):  52%|█████▎    | 105/200 [01:50<01:38,  1.03s/it]\n",
      "Attention Walk (Loss=1.9467):  53%|█████▎    | 106/200 [01:50<01:37,  1.04s/it]\n",
      "Attention Walk (Loss=1.944):  53%|█████▎    | 106/200 [01:51<01:37,  1.04s/it] \n",
      "Attention Walk (Loss=1.944):  54%|█████▎    | 107/200 [01:51<01:36,  1.03s/it]\n",
      "Attention Walk (Loss=1.9413):  54%|█████▎    | 107/200 [01:52<01:36,  1.03s/it]\n",
      "Attention Walk (Loss=1.9413):  54%|█████▍    | 108/200 [01:52<01:35,  1.04s/it]\n",
      "Attention Walk (Loss=1.9387):  54%|█████▍    | 108/200 [01:53<01:35,  1.04s/it]\n",
      "Attention Walk (Loss=1.9387):  55%|█████▍    | 109/200 [01:53<01:34,  1.04s/it]\n",
      "Attention Walk (Loss=1.9361):  55%|█████▍    | 109/200 [01:54<01:34,  1.04s/it]\n",
      "Attention Walk (Loss=1.9361):  55%|█████▌    | 110/200 [01:54<01:33,  1.03s/it]\n",
      "Attention Walk (Loss=1.9336):  55%|█████▌    | 110/200 [01:56<01:33,  1.03s/it]\n",
      "Attention Walk (Loss=1.9336):  56%|█████▌    | 111/200 [01:56<01:31,  1.03s/it]\n",
      "Attention Walk (Loss=1.9311):  56%|█████▌    | 111/200 [01:57<01:31,  1.03s/it]\n",
      "Attention Walk (Loss=1.9311):  56%|█████▌    | 112/200 [01:57<01:30,  1.03s/it]\n",
      "Attention Walk (Loss=1.9286):  56%|█████▌    | 112/200 [01:58<01:30,  1.03s/it]\n",
      "Attention Walk (Loss=1.9286):  56%|█████▋    | 113/200 [01:58<01:29,  1.03s/it]\n",
      "Attention Walk (Loss=1.9262):  56%|█████▋    | 113/200 [01:59<01:29,  1.03s/it]\n",
      "Attention Walk (Loss=1.9262):  57%|█████▋    | 114/200 [01:59<01:29,  1.04s/it]\n",
      "Attention Walk (Loss=1.9238):  57%|█████▋    | 114/200 [02:00<01:29,  1.04s/it]\n",
      "Attention Walk (Loss=1.9238):  57%|█████▊    | 115/200 [02:00<01:28,  1.04s/it]\n",
      "Attention Walk (Loss=1.9215):  57%|█████▊    | 115/200 [02:01<01:28,  1.04s/it]\n",
      "Attention Walk (Loss=1.9215):  58%|█████▊    | 116/200 [02:01<01:27,  1.04s/it]\n",
      "Attention Walk (Loss=1.9192):  58%|█████▊    | 116/200 [02:02<01:27,  1.04s/it]\n",
      "Attention Walk (Loss=1.9192):  58%|█████▊    | 117/200 [02:02<01:26,  1.04s/it]\n",
      "Attention Walk (Loss=1.9169):  58%|█████▊    | 117/200 [02:03<01:26,  1.04s/it]\n",
      "Attention Walk (Loss=1.9169):  59%|█████▉    | 118/200 [02:03<01:25,  1.04s/it]\n",
      "Attention Walk (Loss=1.9147):  59%|█████▉    | 118/200 [02:04<01:25,  1.04s/it]\n",
      "Attention Walk (Loss=1.9147):  60%|█████▉    | 119/200 [02:04<01:24,  1.04s/it]\n",
      "Attention Walk (Loss=1.9125):  60%|█████▉    | 119/200 [02:05<01:24,  1.04s/it]\n",
      "Attention Walk (Loss=1.9125):  60%|██████    | 120/200 [02:05<01:22,  1.04s/it]\n",
      "Attention Walk (Loss=1.9103):  60%|██████    | 120/200 [02:06<01:22,  1.04s/it]\n",
      "Attention Walk (Loss=1.9103):  60%|██████    | 121/200 [02:06<01:21,  1.03s/it]\n",
      "Attention Walk (Loss=1.9082):  60%|██████    | 121/200 [02:07<01:21,  1.03s/it]\n",
      "Attention Walk (Loss=1.9082):  61%|██████    | 122/200 [02:07<01:21,  1.04s/it]\n",
      "Attention Walk (Loss=1.9061):  61%|██████    | 122/200 [02:08<01:21,  1.04s/it]\n",
      "Attention Walk (Loss=1.9061):  62%|██████▏   | 123/200 [02:08<01:19,  1.03s/it]\n",
      "Attention Walk (Loss=1.9041):  62%|██████▏   | 123/200 [02:09<01:19,  1.03s/it]\n",
      "Attention Walk (Loss=1.9041):  62%|██████▏   | 124/200 [02:09<01:18,  1.03s/it]\n",
      "Attention Walk (Loss=1.902):  62%|██████▏   | 124/200 [02:10<01:18,  1.03s/it] \n",
      "Attention Walk (Loss=1.902):  62%|██████▎   | 125/200 [02:10<01:17,  1.03s/it]\n",
      "Attention Walk (Loss=1.9001):  62%|██████▎   | 125/200 [02:11<01:17,  1.03s/it]\n",
      "Attention Walk (Loss=1.9001):  63%|██████▎   | 126/200 [02:11<01:16,  1.03s/it]\n",
      "Attention Walk (Loss=1.8981):  63%|██████▎   | 126/200 [02:12<01:16,  1.03s/it]\n",
      "Attention Walk (Loss=1.8981):  64%|██████▎   | 127/200 [02:12<01:15,  1.03s/it]\n",
      "Attention Walk (Loss=1.8962):  64%|██████▎   | 127/200 [02:13<01:15,  1.03s/it]\n",
      "Attention Walk (Loss=1.8962):  64%|██████▍   | 128/200 [02:13<01:13,  1.02s/it]\n",
      "Attention Walk (Loss=1.8943):  64%|██████▍   | 128/200 [02:14<01:13,  1.02s/it]\n",
      "Attention Walk (Loss=1.8943):  64%|██████▍   | 129/200 [02:14<01:12,  1.02s/it]\n",
      "Attention Walk (Loss=1.8924):  64%|██████▍   | 129/200 [02:15<01:12,  1.02s/it]\n",
      "Attention Walk (Loss=1.8924):  65%|██████▌   | 130/200 [02:15<01:11,  1.02s/it]\n",
      "Attention Walk (Loss=1.8905):  65%|██████▌   | 130/200 [02:16<01:11,  1.02s/it]\n",
      "Attention Walk (Loss=1.8905):  66%|██████▌   | 131/200 [02:16<01:10,  1.02s/it]\n",
      "Attention Walk (Loss=1.8887):  66%|██████▌   | 131/200 [02:17<01:10,  1.02s/it]\n",
      "Attention Walk (Loss=1.8887):  66%|██████▌   | 132/200 [02:17<01:09,  1.02s/it]\n",
      "Attention Walk (Loss=1.8869):  66%|██████▌   | 132/200 [02:18<01:09,  1.02s/it]\n",
      "Attention Walk (Loss=1.8869):  66%|██████▋   | 133/200 [02:18<01:08,  1.02s/it]\n",
      "Attention Walk (Loss=1.8852):  66%|██████▋   | 133/200 [02:19<01:08,  1.02s/it]\n",
      "Attention Walk (Loss=1.8852):  67%|██████▋   | 134/200 [02:19<01:07,  1.02s/it]\n",
      "Attention Walk (Loss=1.8834):  67%|██████▋   | 134/200 [02:20<01:07,  1.02s/it]\n",
      "Attention Walk (Loss=1.8834):  68%|██████▊   | 135/200 [02:20<01:06,  1.02s/it]\n",
      "Attention Walk (Loss=1.8817):  68%|██████▊   | 135/200 [02:21<01:06,  1.02s/it]\n",
      "Attention Walk (Loss=1.8817):  68%|██████▊   | 136/200 [02:21<01:05,  1.02s/it]\n",
      "Attention Walk (Loss=1.88):  68%|██████▊   | 136/200 [02:22<01:05,  1.02s/it]  \n",
      "Attention Walk (Loss=1.88):  68%|██████▊   | 137/200 [02:22<01:03,  1.01s/it]\n",
      "Attention Walk (Loss=1.8784):  68%|██████▊   | 137/200 [02:23<01:03,  1.01s/it]\n",
      "Attention Walk (Loss=1.8784):  69%|██████▉   | 138/200 [02:23<01:02,  1.01s/it]\n",
      "Attention Walk (Loss=1.8767):  69%|██████▉   | 138/200 [02:24<01:02,  1.01s/it]\n",
      "Attention Walk (Loss=1.8767):  70%|██████▉   | 139/200 [02:24<01:01,  1.01s/it]\n",
      "Attention Walk (Loss=1.8751):  70%|██████▉   | 139/200 [02:25<01:01,  1.01s/it]\n",
      "Attention Walk (Loss=1.8751):  70%|███████   | 140/200 [02:25<01:00,  1.01s/it]\n",
      "Attention Walk (Loss=1.8735):  70%|███████   | 140/200 [02:26<01:00,  1.01s/it]\n",
      "Attention Walk (Loss=1.8735):  70%|███████   | 141/200 [02:26<00:59,  1.01s/it]\n",
      "Attention Walk (Loss=1.872):  70%|███████   | 141/200 [02:27<00:59,  1.01s/it] \n",
      "Attention Walk (Loss=1.872):  71%|███████   | 142/200 [02:27<00:59,  1.02s/it]\n",
      "Attention Walk (Loss=1.8704):  71%|███████   | 142/200 [02:28<00:59,  1.02s/it]\n",
      "Attention Walk (Loss=1.8704):  72%|███████▏  | 143/200 [02:28<00:58,  1.03s/it]\n",
      "Attention Walk (Loss=1.8689):  72%|███████▏  | 143/200 [02:29<00:58,  1.03s/it]\n",
      "Attention Walk (Loss=1.8689):  72%|███████▏  | 144/200 [02:29<00:57,  1.03s/it]\n",
      "Attention Walk (Loss=1.8674):  72%|███████▏  | 144/200 [02:30<00:57,  1.03s/it]\n",
      "Attention Walk (Loss=1.8674):  72%|███████▎  | 145/200 [02:30<00:57,  1.04s/it]\n",
      "Attention Walk (Loss=1.8659):  72%|███████▎  | 145/200 [02:32<00:57,  1.04s/it]\n",
      "Attention Walk (Loss=1.8659):  73%|███████▎  | 146/200 [02:32<00:56,  1.05s/it]\n",
      "Attention Walk (Loss=1.8645):  73%|███████▎  | 146/200 [02:33<00:56,  1.05s/it]\n",
      "Attention Walk (Loss=1.8645):  74%|███████▎  | 147/200 [02:33<00:56,  1.06s/it]\n",
      "Attention Walk (Loss=1.863):  74%|███████▎  | 147/200 [02:34<00:56,  1.06s/it] \n",
      "Attention Walk (Loss=1.863):  74%|███████▍  | 148/200 [02:34<00:55,  1.06s/it]\n",
      "Attention Walk (Loss=1.8616):  74%|███████▍  | 148/200 [02:35<00:55,  1.06s/it]\n",
      "Attention Walk (Loss=1.8616):  74%|███████▍  | 149/200 [02:35<00:53,  1.05s/it]\n",
      "Attention Walk (Loss=1.8602):  74%|███████▍  | 149/200 [02:36<00:53,  1.05s/it]\n",
      "Attention Walk (Loss=1.8602):  75%|███████▌  | 150/200 [02:36<00:52,  1.04s/it]\n",
      "Attention Walk (Loss=1.8588):  75%|███████▌  | 150/200 [02:37<00:52,  1.04s/it]\n",
      "Attention Walk (Loss=1.8588):  76%|███████▌  | 151/200 [02:37<00:52,  1.06s/it]\n",
      "Attention Walk (Loss=1.8575):  76%|███████▌  | 151/200 [02:38<00:52,  1.06s/it]\n",
      "Attention Walk (Loss=1.8575):  76%|███████▌  | 152/200 [02:38<00:50,  1.06s/it]\n",
      "Attention Walk (Loss=1.8561):  76%|███████▌  | 152/200 [02:39<00:50,  1.06s/it]\n",
      "Attention Walk (Loss=1.8561):  76%|███████▋  | 153/200 [02:39<00:49,  1.05s/it]\n",
      "Attention Walk (Loss=1.8548):  76%|███████▋  | 153/200 [02:40<00:49,  1.05s/it]\n",
      "Attention Walk (Loss=1.8548):  77%|███████▋  | 154/200 [02:40<00:48,  1.05s/it]\n",
      "Attention Walk (Loss=1.8535):  77%|███████▋  | 154/200 [02:41<00:48,  1.05s/it]\n",
      "Attention Walk (Loss=1.8535):  78%|███████▊  | 155/200 [02:41<00:48,  1.07s/it]\n",
      "Attention Walk (Loss=1.8522):  78%|███████▊  | 155/200 [02:42<00:48,  1.07s/it]\n",
      "Attention Walk (Loss=1.8522):  78%|███████▊  | 156/200 [02:42<00:48,  1.11s/it]\n",
      "Attention Walk (Loss=1.851):  78%|███████▊  | 156/200 [02:43<00:48,  1.11s/it] \n",
      "Attention Walk (Loss=1.851):  78%|███████▊  | 157/200 [02:43<00:47,  1.10s/it]\n",
      "Attention Walk (Loss=1.8497):  78%|███████▊  | 157/200 [02:44<00:47,  1.10s/it]\n",
      "Attention Walk (Loss=1.8497):  79%|███████▉  | 158/200 [02:44<00:45,  1.09s/it]\n",
      "Attention Walk (Loss=1.8485):  79%|███████▉  | 158/200 [02:46<00:45,  1.09s/it]\n",
      "Attention Walk (Loss=1.8485):  80%|███████▉  | 159/200 [02:46<00:45,  1.10s/it]\n",
      "Attention Walk (Loss=1.8473):  80%|███████▉  | 159/200 [02:47<00:45,  1.10s/it]\n",
      "Attention Walk (Loss=1.8473):  80%|████████  | 160/200 [02:47<00:44,  1.11s/it]\n",
      "Attention Walk (Loss=1.8461):  80%|████████  | 160/200 [02:48<00:44,  1.11s/it]\n",
      "Attention Walk (Loss=1.8461):  80%|████████  | 161/200 [02:48<00:42,  1.09s/it]\n",
      "Attention Walk (Loss=1.8449):  80%|████████  | 161/200 [02:49<00:42,  1.09s/it]\n",
      "Attention Walk (Loss=1.8449):  81%|████████  | 162/200 [02:49<00:41,  1.08s/it]\n",
      "Attention Walk (Loss=1.8437):  81%|████████  | 162/200 [02:50<00:41,  1.08s/it]\n",
      "Attention Walk (Loss=1.8437):  82%|████████▏ | 163/200 [02:50<00:39,  1.08s/it]\n",
      "Attention Walk (Loss=1.8425):  82%|████████▏ | 163/200 [02:51<00:39,  1.08s/it]\n",
      "Attention Walk (Loss=1.8425):  82%|████████▏ | 164/200 [02:51<00:40,  1.12s/it]\n",
      "Attention Walk (Loss=1.8414):  82%|████████▏ | 164/200 [02:52<00:40,  1.12s/it]\n",
      "Attention Walk (Loss=1.8414):  82%|████████▎ | 165/200 [02:52<00:41,  1.18s/it]\n",
      "Attention Walk (Loss=1.8403):  82%|████████▎ | 165/200 [02:54<00:41,  1.18s/it]\n",
      "Attention Walk (Loss=1.8403):  83%|████████▎ | 166/200 [02:54<00:40,  1.20s/it]\n",
      "Attention Walk (Loss=1.8392):  83%|████████▎ | 166/200 [02:55<00:40,  1.20s/it]\n",
      "Attention Walk (Loss=1.8392):  84%|████████▎ | 167/200 [02:55<00:42,  1.30s/it]\n",
      "Attention Walk (Loss=1.8381):  84%|████████▎ | 167/200 [02:57<00:42,  1.30s/it]\n",
      "Attention Walk (Loss=1.8381):  84%|████████▍ | 168/200 [02:57<00:42,  1.34s/it]\n",
      "Attention Walk (Loss=1.837):  84%|████████▍ | 168/200 [02:58<00:42,  1.34s/it] \n",
      "Attention Walk (Loss=1.837):  84%|████████▍ | 169/200 [02:58<00:41,  1.34s/it]\n",
      "Attention Walk (Loss=1.8359):  84%|████████▍ | 169/200 [02:59<00:41,  1.34s/it]\n",
      "Attention Walk (Loss=1.8359):  85%|████████▌ | 170/200 [02:59<00:40,  1.33s/it]\n",
      "Attention Walk (Loss=1.8349):  85%|████████▌ | 170/200 [03:01<00:40,  1.33s/it]\n",
      "Attention Walk (Loss=1.8349):  86%|████████▌ | 171/200 [03:01<00:38,  1.33s/it]\n",
      "Attention Walk (Loss=1.8338):  86%|████████▌ | 171/200 [03:02<00:38,  1.33s/it]\n",
      "Attention Walk (Loss=1.8338):  86%|████████▌ | 172/200 [03:02<00:36,  1.31s/it]\n",
      "Attention Walk (Loss=1.8328):  86%|████████▌ | 172/200 [03:03<00:36,  1.31s/it]\n",
      "Attention Walk (Loss=1.8328):  86%|████████▋ | 173/200 [03:03<00:34,  1.28s/it]\n",
      "Attention Walk (Loss=1.8318):  86%|████████▋ | 173/200 [03:04<00:34,  1.28s/it]\n",
      "Attention Walk (Loss=1.8318):  87%|████████▋ | 174/200 [03:04<00:32,  1.26s/it]\n",
      "Attention Walk (Loss=1.8308):  87%|████████▋ | 174/200 [03:06<00:32,  1.26s/it]\n",
      "Attention Walk (Loss=1.8308):  88%|████████▊ | 175/200 [03:06<00:31,  1.26s/it]\n",
      "Attention Walk (Loss=1.8298):  88%|████████▊ | 175/200 [03:07<00:31,  1.26s/it]\n",
      "Attention Walk (Loss=1.8298):  88%|████████▊ | 176/200 [03:07<00:30,  1.26s/it]\n",
      "Attention Walk (Loss=inf):  88%|████████▊ | 176/200 [03:08<00:30,  1.26s/it]   \n",
      "Attention Walk (Loss=inf):  88%|████████▊ | 177/200 [03:08<00:28,  1.26s/it]\n",
      "Attention Walk (Loss=nan):  88%|████████▊ | 177/200 [03:09<00:28,  1.26s/it]\n",
      "Attention Walk (Loss=nan):  89%|████████▉ | 178/200 [03:09<00:27,  1.26s/it]\n",
      "Attention Walk (Loss=nan):  89%|████████▉ | 178/200 [03:11<00:27,  1.26s/it]\n",
      "Attention Walk (Loss=nan):  90%|████████▉ | 179/200 [03:11<00:26,  1.25s/it]\n",
      "Attention Walk (Loss=nan):  90%|████████▉ | 179/200 [03:12<00:26,  1.25s/it]\n",
      "Attention Walk (Loss=nan):  90%|█████████ | 180/200 [03:12<00:24,  1.22s/it]\n",
      "Attention Walk (Loss=nan):  90%|█████████ | 180/200 [03:13<00:24,  1.22s/it]\n",
      "Attention Walk (Loss=nan):  90%|█████████ | 181/200 [03:13<00:22,  1.20s/it]\n",
      "Attention Walk (Loss=nan):  90%|█████████ | 181/200 [03:14<00:22,  1.20s/it]\n",
      "Attention Walk (Loss=nan):  91%|█████████ | 182/200 [03:14<00:21,  1.18s/it]\n",
      "Attention Walk (Loss=nan):  91%|█████████ | 182/200 [03:15<00:21,  1.18s/it]\n",
      "Attention Walk (Loss=nan):  92%|█████████▏| 183/200 [03:15<00:19,  1.17s/it]\n",
      "Attention Walk (Loss=nan):  92%|█████████▏| 183/200 [03:16<00:19,  1.17s/it]\n",
      "Attention Walk (Loss=nan):  92%|█████████▏| 184/200 [03:16<00:18,  1.15s/it]\n",
      "Attention Walk (Loss=nan):  92%|█████████▏| 184/200 [03:17<00:18,  1.15s/it]\n",
      "Attention Walk (Loss=nan):  92%|█████████▎| 185/200 [03:17<00:17,  1.14s/it]\n",
      "Attention Walk (Loss=nan):  92%|█████████▎| 185/200 [03:18<00:17,  1.14s/it]\n",
      "Attention Walk (Loss=nan):  93%|█████████▎| 186/200 [03:18<00:15,  1.13s/it]\n",
      "Attention Walk (Loss=nan):  93%|█████████▎| 186/200 [03:20<00:15,  1.13s/it]\n",
      "Attention Walk (Loss=nan):  94%|█████████▎| 187/200 [03:20<00:14,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  94%|█████████▎| 187/200 [03:21<00:14,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  94%|█████████▍| 188/200 [03:21<00:13,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  94%|█████████▍| 188/200 [03:22<00:13,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  94%|█████████▍| 189/200 [03:22<00:12,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  94%|█████████▍| 189/200 [03:23<00:12,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  95%|█████████▌| 190/200 [03:23<00:11,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  95%|█████████▌| 190/200 [03:24<00:11,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  96%|█████████▌| 191/200 [03:24<00:10,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  96%|█████████▌| 191/200 [03:25<00:10,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  96%|█████████▌| 192/200 [03:25<00:09,  1.14s/it]\n",
      "Attention Walk (Loss=nan):  96%|█████████▌| 192/200 [03:26<00:09,  1.14s/it]\n",
      "Attention Walk (Loss=nan):  96%|█████████▋| 193/200 [03:26<00:07,  1.13s/it]\n",
      "Attention Walk (Loss=nan):  96%|█████████▋| 193/200 [03:27<00:07,  1.13s/it]\n",
      "Attention Walk (Loss=nan):  97%|█████████▋| 194/200 [03:27<00:06,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  97%|█████████▋| 194/200 [03:29<00:06,  1.12s/it]\n",
      "Attention Walk (Loss=nan):  98%|█████████▊| 195/200 [03:29<00:05,  1.14s/it]\n",
      "Attention Walk (Loss=nan):  98%|█████████▊| 195/200 [03:30<00:05,  1.14s/it]\n",
      "Attention Walk (Loss=nan):  98%|█████████▊| 196/200 [03:30<00:04,  1.13s/it]\n",
      "Attention Walk (Loss=nan):  98%|█████████▊| 196/200 [03:31<00:04,  1.13s/it]\n",
      "Attention Walk (Loss=nan):  98%|█████████▊| 197/200 [03:31<00:03,  1.14s/it]\n",
      "Attention Walk (Loss=nan):  98%|█████████▊| 197/200 [03:32<00:03,  1.14s/it]\n",
      "Attention Walk (Loss=nan):  99%|█████████▉| 198/200 [03:32<00:02,  1.15s/it]\n",
      "Attention Walk (Loss=nan):  99%|█████████▉| 198/200 [03:33<00:02,  1.15s/it]\n",
      "Attention Walk (Loss=nan): 100%|█████████▉| 199/200 [03:33<00:01,  1.15s/it]\n",
      "Attention Walk (Loss=nan): 100%|█████████▉| 199/200 [03:34<00:01,  1.15s/it]\n",
      "Attention Walk (Loss=nan): 100%|██████████| 200/200 [03:34<00:00,  1.15s/it]\n",
      "Attention Walk (Loss=nan): 100%|██████████| 200/200 [03:34<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/facebook_edges.csv --embedding-path ../../result/facebook_embeddings_attention.csv --attention-path ../../result/facebook_attention.csv"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08a2aa42a02244cca504a2553ee3a9d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_121ff0f0d9a940ab9ebf233013e0526a",
       "IPY_MODEL_8a997cdf9e1f4141a65e806dcf974c65",
       "IPY_MODEL_4d686c59f72c4a07a244a1d9183e9855"
      ],
      "layout": "IPY_MODEL_3b98a185484e4302abe56263ada93980"
     }
    },
    "121ff0f0d9a940ab9ebf233013e0526a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e587efe0fc234ea4b413cb1ab80bf9ac",
      "placeholder": "​",
      "style": "IPY_MODEL_255e4240da824b8ab6c035a659407984",
      "value": "Computing transition probabilities: 100%"
     }
    },
    "1800b26997924888a4fbfea29ceaf1d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "255e4240da824b8ab6c035a659407984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "341b42b7c516425bae603d44be7e8aa8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "385aa92c3d8541dc9de0ef77007a3bfa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b98a185484e4302abe56263ada93980": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "465fcc5fe5cb44189b3647adc27d29d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d686c59f72c4a07a244a1d9183e9855": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_341b42b7c516425bae603d44be7e8aa8",
      "placeholder": "​",
      "style": "IPY_MODEL_465fcc5fe5cb44189b3647adc27d29d2",
      "value": " 15229/15229 [03:51&lt;00:00, 2713.44it/s]"
     }
    },
    "8a997cdf9e1f4141a65e806dcf974c65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_385aa92c3d8541dc9de0ef77007a3bfa",
      "max": 15229,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1800b26997924888a4fbfea29ceaf1d6",
      "value": 15229
     }
    },
    "e587efe0fc234ea4b413cb1ab80bf9ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
