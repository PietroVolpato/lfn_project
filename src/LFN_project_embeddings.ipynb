{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d121cbd5",
   "metadata": {
    "id": "d121cbd5"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PietroVolpato/lfn_project/blob/main/src/LFN_project_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358080be-e62b-4a82-b8d3-e5becb180aac",
   "metadata": {
    "id": "358080be-e62b-4a82-b8d3-e5becb180aac"
   },
   "source": [
    "# Learning from networks project\n",
    "### Evaluation of different Node Embedding algorithms\n",
    "Members:<br>\n",
    "- D'Emilio Filippo, id : 2120931\n",
    "- Volpato Pietro, id : 2120825\n",
    "\n",
    "### Information about the notebook (have a look at the report for details)\n",
    "This notebook are computed the embeddings for every selected using 3 different embedding algorithms.<br>\n",
    "Each embedding is saved to file as a numpy array (extension .npy), in the directory /results. In this way once an embedding is computed, it won't be lost when the runtime of the notebook is terminated.<br>\n",
    "We can then efficiently load the embeddings in the \"test\" notebook, and evaluate the quality of the embeddings.<br>\n",
    "Selected embedding techniques:\n",
    "- Node2Vec\n",
    "- Line\n",
    "- Attention Walk\n",
    "\n",
    "For information about the graphs, se cells below.<br>\n",
    "*NOTE*: by implementation choice, the computation of each embedding is computed separately (there are no function to coincisely compute all embeddings).<br>\n",
    "This choice comes from the fact that computing embeddings is computationally intensive, and we might want to compute only a specific embedding strategy for a specific graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d3b60-f774-4fc7-b0bc-7617057d6459",
   "metadata": {
    "id": "bb1d3b60-f774-4fc7-b0bc-7617057d6459"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93987c53-ecce-41d6-948b-4a4b7ac4978a",
   "metadata": {
    "id": "93987c53-ecce-41d6-948b-4a4b7ac4978a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gzip\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b64818-de78-4c30-945e-b95279c7f1c9",
   "metadata": {
    "id": "d6b64818-de78-4c30-945e-b95279c7f1c9"
   },
   "source": [
    "# configuration\n",
    "Here you can properly configure the names of the graphs and the names of the embedding strategies. Use meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e5b26d0-84d0-46ad-83dd-ac826faac4fe",
   "metadata": {
    "id": "6e5b26d0-84d0-46ad-83dd-ac826faac4fe"
   },
   "outputs": [],
   "source": [
    "graph_keys = [\"facebook\",\"citation\",\"biological\",\"proteins\", \"spam\"]\n",
    "embedding_keys = [\"LINE\", \"node2vec\", \"AW\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6ca3e-b5f5-4869-a3ef-47fb66cd7c56",
   "metadata": {
    "id": "79f6ca3e-b5f5-4869-a3ef-47fb66cd7c56"
   },
   "source": [
    "# Loading the graphs\n",
    "Selected graphs:\n",
    "- Facebook_combined    https://snap.stanford.edu/data/ego-Facebook.html          \n",
    "- cit-Helpth           https://networkrepository.com/cit-HepTh.php             \n",
    "- bio-CE-CX            https://networkrepository.com/bio-CE-CX.php             \n",
    "- proteins-full        https://networkrepository.com/PROTEINS-full.php ---- the graph has node labels\n",
    "- spam                 https://networkrepository.com/web-spam-detection.php  ---- the graph has node labels\n",
    "\n",
    "To run this notebook, adjust the paths to match where the files are saved in your PC.<br>\n",
    "To keep paths as they are, create a \"data\" folder inside the directory of this notebook, and store the files there.<br>\n",
    "Graphs are stored as a dictionary: the key is the graph name, the value is the corresponding netowrkx graph.<br>\n",
    "\n",
    "When it is created a networkX graph from a text file the node are renamed as integers form 0 to |V|-1, so that we can store the embeddings\n",
    "on a matrix, and each row index corresponds to the embedding vector of the corrisponding node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "TtpPrXlsIUgJ",
   "metadata": {
    "id": "TtpPrXlsIUgJ"
   },
   "outputs": [],
   "source": [
    "facebook_path = '../data/facebook/facebook_combined.txt.gz'\n",
    "citation_path = '../data/citation/cit-HepTh.edges'\n",
    "biological_path = '../data/biological/bio-CE-CX.edges'\n",
    "proteins_path = \"../data/proteins/PROTEINS-full.edges\"\n",
    "spam_path = \"../data/spam/web-spam-detection.edges\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e83f7ed1-5bbd-4e15-8d0c-1a93e9802783",
   "metadata": {
    "id": "e83f7ed1-5bbd-4e15-8d0c-1a93e9802783"
   },
   "outputs": [],
   "source": [
    "def load_graph(path):\n",
    "    \"\"\"\n",
    "    For files with extension .edges\n",
    "    nodes are renamed as integers, starting from 0\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('%'):  # Skip comment lines\n",
    "                continue\n",
    "            # Split the line based on spaces or commas\n",
    "            data = re.split(r'[,\\s]+', line.strip())\n",
    "            if len(data) < 2:  # Skip lines that don't have at least two columns\n",
    "                continue\n",
    "            # Extract the first two columns (nodes)\n",
    "            node1, node2 = int(data[0]), int(data[1])\n",
    "            G.add_edge(node1, node2)\n",
    "    mapping = {node : i for i,node in enumerate(G.nodes)} # mappoing original : relabeled\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    return G\n",
    "\n",
    "def load_graph_with_gz(path):\n",
    "    \"\"\"\n",
    "    For files with extension .txt.gz\n",
    "    nodes are renamed as integers, starting from 0\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            node1, node2 = map(int, line.strip().split())\n",
    "            G.add_edge(node1, node2)\n",
    "    mapping = {node : i for i,node in enumerate(G.nodes)} # mappoing original : relabeled\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    return G\n",
    "\n",
    "def print_graphs_info(graphs):\n",
    "    for k in graph_keys:\n",
    "        G = graphs[k]\n",
    "        print(f\"{k}: |V|={len(G.nodes)}, |E|={len(G.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2ab6207-3fe1-44b0-bf8e-636a7d9f9f2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2ab6207-3fe1-44b0-bf8e-636a7d9f9f2b",
    "outputId": "b810991c-26ec-47d1-b2c3-248de16db6fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook: |V|=4039, |E|=88234\n",
      "citation: |V|=22908, |E|=2444798\n",
      "biological: |V|=15229, |E|=245952\n",
      "proteins: |V|=43471, |E|=81049\n",
      "spam: |V|=9072, |E|=473854\n"
     ]
    }
   ],
   "source": [
    "graphs = {}\n",
    "\n",
    "# facebook graph is the only one .tar.gz\n",
    "graphs[graph_keys[0]] = load_graph_with_gz(facebook_path)  # relabeling nodes to integer\n",
    "graphs[graph_keys[1]] = load_graph(citation_path)\n",
    "graphs[graph_keys[2]] = load_graph(biological_path)\n",
    "graphs[graph_keys[3]] = load_graph(proteins_path)  # node labeled\n",
    "graphs[graph_keys[4]] = load_graph(spam_path)  # node labeled\n",
    "\n",
    "print_graphs_info(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8tGjO8jcn7P5",
   "metadata": {
    "id": "8tGjO8jcn7P5"
   },
   "source": [
    "# Download the dataset from the GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pBB2zmeGoCXe",
   "metadata": {
    "id": "pBB2zmeGoCXe"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/PietroVolpato/lfn_project/main/data/\"\n",
    "filename = \"bio-CE-CX_edges.csv\"\n",
    "\n",
    "response = requests.get(url + filename)\n",
    "with open(filename, \"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ab17a-9328-47b5-bc28-15dbd59deabf",
   "metadata": {
    "id": "f14ab17a-9328-47b5-bc28-15dbd59deabf"
   },
   "source": [
    "# Functions and declarations for the embeddings\n",
    "Embedding data structure is defined as following:<br>\n",
    "- The first index refer to the graph (e.g. embeddings[\"facebook\"] contains the embeddings of the facebook graph for every embedding technique).<br>\n",
    "- The second index refer to the embedding technique (e.g. embeddings[\"facebook\"][\"LINE\"] cointans the embedding of facebook graph computed using LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e7e55ae-2aa3-4989-8903-0ccc3898309f",
   "metadata": {
    "id": "0e7e55ae-2aa3-4989-8903-0ccc3898309f"
   },
   "outputs": [],
   "source": [
    "def save(emb, graph_key, embedding_key, emb_dim):\n",
    "    path = f\"../result/embeddings_{graph_key}_{embedding_key}_{emb_dim}.npy\"\n",
    "    np.save(path, emb)\n",
    "    print(f\"Successfully saved the embeddings in {path}\")\n",
    "\n",
    "# dictionaries to store the embeddings, obtained by several techniques, for each graph\n",
    "embeddings = {}\n",
    "for k in graph_keys:\n",
    "    embeddings[k] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7649b02-023b-4ec3-b1e7-4d051776d0f2",
   "metadata": {
    "id": "d7649b02-023b-4ec3-b1e7-4d051776d0f2"
   },
   "source": [
    "# Node2Vec\n",
    "- pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c54d5a3a-a0ff-4044-8ba4-808588927dba",
   "metadata": {
    "id": "c54d5a3a-a0ff-4044-8ba4-808588927dba"
   },
   "outputs": [],
   "source": [
    "def get_node2vec_embeddings(G, dimensions=128, walk_length=50, num_walks=40, neighborhood_size = 10, p=0.5, q=2):\n",
    "    \"\"\"\n",
    "    Generate node embeddings for a graph using the Node2Vec algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph):The input graph for which embeddings are to be generated.\n",
    "            The graph should have nodes labeled as integers, ideally sequentially starting from 0.\n",
    "        dimensions (int, optional): The dimensionality of the embedding space. Default is 128.\n",
    "        walk_length (int, optional): The length of each random walk. Default is 10.\n",
    "        num_walks (int, optional): The number of random walks to start from each node. Default is 20.\n",
    "        p (float, optional):\n",
    "            The return parameter, controlling the likelihood of immediately revisiting a node in the walk.\n",
    "            A higher value makes it more likely to backtrack. Default is 1.\n",
    "        q (float, optional):\n",
    "            The in-out parameter, controlling the likelihood of exploring outward from the starting node.\n",
    "            A higher value makes it more likely to move outward. Default is 1.\n",
    "        workers (int, optional): The number of parallel workers for random walk generation and model training. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array where each row represents the embedding of a node.\n",
    "            The row index corresponds to the node ID, and each row has `dimensions` elements.\n",
    "    \"\"\"\n",
    "    # Initialize Node2Vec model\n",
    "    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, p=p, q=q, workers=1)\n",
    "\n",
    "    # Fit the Node2Vec model and generate embeddings\n",
    "    model = node2vec.fit(window=neighborhood_size, min_count=1, batch_words=4)\n",
    "\n",
    "    # Convert embeddings to a NumPy array\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    embeddings = np.zeros((num_nodes, dimensions))  # Preallocate array\n",
    "    for node in G.nodes:\n",
    "        embeddings[node] = model.wv[str(node)]  # in the vocabulary node names are converted always to strings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d9eac-2b78-4283-bea5-354f1f324e49",
   "metadata": {},
   "source": [
    "## Produce the embeddings with node2vec\n",
    "here you can easily produce the embeddings for any of the loaded graphs using node2vec.<br>\n",
    "Adjust the variable curr_graph_key with the key of the graph you want to compute the embeddings for.<br>\n",
    "The embeddings are saved to file (look output to get path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f11c92-dd61-476b-9c80-e152704a2e58",
   "metadata": {
    "id": "2535202c-086c-42ae-a7a0-2bce62d42e02"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7627024875dc4a369426ff18e05336ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/15229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1):  22%|████████████▍                                           | 4/18 [00:43<02:44, 11.72s/it]"
     ]
    }
   ],
   "source": [
    "# graph_keys[0] = facebook\n",
    "# graph_keys[1] = citation \n",
    "# graph_keys[2] = biological\n",
    "# graph_keys[3] = proteins\n",
    "# graph_keys[4] = spam\n",
    "curr_graph_key = \"biological\"   # chose the graph\n",
    "emb_dim = 128\n",
    "\n",
    "# DENSE GRAPH (G, dimensions=emb_dim, walk_length=20, num_walks=20, p=1, q=0.5, epochs = 10)\n",
    "# PAPER SETTINGS: (G, dimensions=128, walk_length=80, num_walks=10, p=1, q=0.5)\n",
    "# PAPER SETTINGS BLOG GRAPH 10k, 333k:  (G, dimensions=128, walk_length=80, num_walks=18, p=0.25, q=0.25)\n",
    "\n",
    "# facebook : (walk_length=80, num_walks=18, neighborhood_size = 16, p=0.25, q=0.25)\n",
    "# spam : (walk_length=80, num_walks=18, neighborhood_size = 16, p=0.25, q=0.25)\n",
    "# biological: (walk_length=80, num_walks=18, neighborhood_size = 16, p=0.25, q=0.25)\n",
    "# proteins: G, (walk_length=20, num_walks=20, neighborhood_size = 8, p=0.25, q=1)\n",
    "\n",
    "# facebook community detection: walk_length=60, num_walks=15, neighborhood_size=10, p=2, q=0.5\n",
    "\n",
    "G = graphs[curr_graph_key]\n",
    "embeddings[curr_graph_key][\"node2vec\"] = get_node2vec_embeddings(\n",
    "    G, dimensions=emb_dim, walk_length=80, num_walks=18, neighborhood_size = 16, p=0.25, q=0.25\n",
    ")\n",
    "save(embeddings[curr_graph_key][\"node2vec\"], curr_graph_key, \"node2vec\", emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c4f4c-6f35-4c15-bff0-837960ef3411",
   "metadata": {
    "id": "900c4f4c-6f35-4c15-bff0-837960ef3411"
   },
   "source": [
    "# LINE : Large-scale information network embedding\n",
    "installation guide:\n",
    "- git clone https://github.com/VahidooX/LINE.git\n",
    "- !pip install keras\n",
    "- !pip install tensorflow\n",
    "- adjust the sys.path to where you downloaded LINE repository\n",
    "\n",
    "*NOTE*: it was necessary to modify utils.py to adapt it at current version of keras because some elements were deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e6cc917-5939-45b8-aca5-d192cb0fd1d9",
   "metadata": {
    "id": "3e6cc917-5939-45b8-aca5-d192cb0fd1d9"
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"LINE\")\n",
    "\n",
    "from model import create_model\n",
    "from utils import batchgen_train\n",
    "\n",
    "def get_LINE_embeddings(G, embedding_dim=128, batch_size=1024, negative_ratio=5, epochs=20, negative_sampling=\"UNIFORM\"):\n",
    "    \"\"\"\n",
    "    Generate LINE embeddings for a given graph.\n",
    "\n",
    "    Parameters:\n",
    "        G (nx.Graph): The graph for which embeddings are computed.\n",
    "        embedding_dim (int): Dimensionality of the embeddings.\n",
    "        batch_size (int): Batch size for training.\n",
    "        negative_ratio (int): Ratio of negative to positive samples.\n",
    "        epochs (int): Number of training epochs.\n",
    "        negative_sampling (str): Negative sampling strategy (\"UNIFORM\" or \"NON-UNIFORM\").\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Node embeddings (shape: [num_nodes, embedding_dim]).\n",
    "    \"\"\"\n",
    "    num_nodes = G.number_of_nodes()\n",
    "\n",
    "    # Convert networkx.Graph to adj_list (edge list as 2D numpy array)\n",
    "    adj_list = np.array(list(G.edges()), dtype=np.int32)\n",
    "\n",
    "    # Create LINE model\n",
    "    model, embed_generator = create_model(num_nodes, embedding_dim)\n",
    "\n",
    "    # Generate training batches\n",
    "    train_gen = batchgen_train(adj_list, num_nodes, batch_size, negative_ratio, negative_sampling)\n",
    "\n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    model.fit(train_gen, steps_per_epoch=500, epochs=epochs)\n",
    "\n",
    "    # Extract embeddings\n",
    "    node_ids = np.arange(num_nodes)  # Sequential node IDs\n",
    "    embeddings = embed_generator.predict_on_batch(node_ids)\n",
    "\n",
    "    print(\"Node Embeddings Shape:\", embeddings[0].shape)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94620d7e-7d58-494a-a968-acf7f5b20b4f",
   "metadata": {},
   "source": [
    "## Produce the embeddings with LINE\n",
    "here you can easily produce the embeddings for any of the loaded graphs using LINE.<br>\n",
    "Adjust the variable curr_graph_key with the key of the graph you want to compute the embeddings for.<br>\n",
    "The embeddings are saved to file (look output to get path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2535202c-086c-42ae-a7a0-2bce62d42e02",
   "metadata": {
    "id": "2535202c-086c-42ae-a7a0-2bce62d42e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 97s 193ms/step - loss: 1.3136\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 97s 195ms/step - loss: 0.7341\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 91s 181ms/step - loss: 0.5351\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 92s 184ms/step - loss: 0.4505\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 94s 189ms/step - loss: 0.4097\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 91s 181ms/step - loss: 0.3787\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 90s 180ms/step - loss: 0.3579\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 94s 188ms/step - loss: 0.3472\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 99s 197ms/step - loss: 0.3285\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 89s 179ms/step - loss: 0.3239\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 89s 178ms/step - loss: 0.3014\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 93s 186ms/step - loss: 0.2886\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 92s 183ms/step - loss: 0.2785\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 91s 183ms/step - loss: 0.2685\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 99s 197ms/step - loss: 0.2620\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 89s 178ms/step - loss: 0.2541\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 89s 178ms/step - loss: 0.2466\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 93s 185ms/step - loss: 0.2451\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 100s 201ms/step - loss: 0.2375\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 87s 174ms/step - loss: 0.2400\n",
      "Node Embeddings Shape: (128,)\n",
      "Successfully saved the embeddings in ../result/embeddings_spam_LINE_128.npy\n"
     ]
    }
   ],
   "source": [
    "curr_graph_key = \"spam\"   # chose the graph\n",
    "emb_dim = 128\n",
    "\n",
    "embeddings[curr_graph_key][\"LINE\"] = get_LINE_embeddings(graphs[curr_graph_key], epochs = 20)\n",
    "save(embeddings[curr_graph_key][\"LINE\"], curr_graph_key, \"LINE\", emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc4a49",
   "metadata": {
    "id": "5abc4a49"
   },
   "source": [
    "# AttentionWalk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f540988",
   "metadata": {
    "id": "5f540988"
   },
   "source": [
    "## Installation guide\n",
    "<ol>\n",
    "<li>git clone https://github.com/benedekrozemberczki/AttentionWalk.git</li>\n",
    "<li>pip install texttable</li>\n",
    "</ol>\n",
    "\n",
    "It requires that the input file is a .csv, so first we have implemented a function that converts the .txt.gz and the .edges files to a .csv to be given as input to the AttentionWalk algorithm.<br>\n",
    "For starting the algorithm you have to enter to the AttentionWalk folder after having cloned it from the Github repository and then set the arguments as described in the README.md file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b872e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3b872e0",
    "outputId": "44fa23c4-3512-4e6c-e2e7-4459f560804d"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/benedekrozemberczki/AttentionWalk.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33c074",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d33c074",
    "outputId": "bdb2c444-b76b-4596-af4b-f60315b0747e"
   },
   "outputs": [],
   "source": [
    "!pip install texttable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b0bbb",
   "metadata": {
    "id": "d01b0bbb"
   },
   "source": [
    "## Test with the facebook network\n",
    "Save the embeddings in the result folder<br>\n",
    "Time: 1m 50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98fc24b7",
   "metadata": {
    "collapsed": true,
    "id": "98fc24b7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "80702d2f-f99e-402c-a944-b6dcc8ab5146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------------------------------+\n",
      "| Attention path |     ./output/chameleon_AW_attention.csv     |\n",
      "+================+=============================================+\n",
      "| Beta           | 0.500                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Dimensions     | 256                                         |\n",
      "+----------------+---------------------------------------------+\n",
      "| Edge path      | ../../data/facebook/facebook_combined.csv   |\n",
      "+----------------+---------------------------------------------+\n",
      "| Embedding path | ../../result/embeddings_facebook_AW_256.csv |\n",
      "+----------------+---------------------------------------------+\n",
      "| Epochs         | 200                                         |\n",
      "+----------------+---------------------------------------------+\n",
      "| Gamma          | 0.500                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Learning rate  | 0.010                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Num of walks   | 80                                          |\n",
      "+----------------+---------------------------------------------+\n",
      "| Window size    | 5                                           |\n",
      "+----------------+---------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n",
      "\n",
      "Training the model.\n",
      "\n",
      "Loss contains NaN or Inf. Stopping training.\n",
      "\n",
      "Saving the model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjacency matrix powers:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:  50%|#####     | 2/4 [00:00<00:00,  5.19it/s]\n",
      "Adjacency matrix powers:  75%|#######5  | 3/4 [00:01<00:00,  2.26it/s]\n",
      "Adjacency matrix powers: 100%|##########| 4/4 [00:02<00:00,  1.24it/s]\n",
      "Adjacency matrix powers: 100%|##########| 4/4 [00:02<00:00,  1.55it/s]\n",
      "\n",
      "Loss:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Attention Walk (Loss=48.3145):   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Attention Walk (Loss=48.3145):   0%|          | 1/200 [00:00<02:52,  1.16it/s]\n",
      "Attention Walk (Loss=48.3035):   0%|          | 1/200 [00:01<02:52,  1.16it/s]\n",
      "Attention Walk (Loss=48.3035):   1%|1         | 2/200 [00:01<03:17,  1.00it/s]\n",
      "Attention Walk (Loss=48.1262):   1%|1         | 2/200 [00:02<03:17,  1.00it/s]\n",
      "Attention Walk (Loss=48.1262):   2%|1         | 3/200 [00:02<03:13,  1.02it/s]\n",
      "Attention Walk (Loss=47.8291):   2%|1         | 3/200 [00:03<03:13,  1.02it/s]\n",
      "Attention Walk (Loss=47.8291):   2%|2         | 4/200 [00:03<03:02,  1.07it/s]\n",
      "Attention Walk (Loss=47.328):   2%|2         | 4/200 [00:04<03:02,  1.07it/s] \n",
      "Attention Walk (Loss=47.328):   2%|2         | 5/200 [00:04<02:58,  1.09it/s]\n",
      "Attention Walk (Loss=46.587):   2%|2         | 5/200 [00:05<02:58,  1.09it/s]\n",
      "Attention Walk (Loss=46.587):   3%|3         | 6/200 [00:05<03:12,  1.01it/s]\n",
      "Attention Walk (Loss=45.5891):   3%|3         | 6/200 [00:06<03:12,  1.01it/s]\n",
      "Attention Walk (Loss=45.5891):   4%|3         | 7/200 [00:06<03:20,  1.04s/it]\n",
      "Attention Walk (Loss=44.3205):   4%|3         | 7/200 [00:08<03:20,  1.04s/it]\n",
      "Attention Walk (Loss=44.3205):   4%|4         | 8/200 [00:08<03:26,  1.07s/it]\n",
      "Attention Walk (Loss=42.774):   4%|4         | 8/200 [00:09<03:26,  1.07s/it] \n",
      "Attention Walk (Loss=42.774):   4%|4         | 9/200 [00:09<03:24,  1.07s/it]\n",
      "Attention Walk (Loss=40.9521):   4%|4         | 9/200 [00:10<03:24,  1.07s/it]\n",
      "Attention Walk (Loss=40.9521):   5%|5         | 10/200 [00:10<03:14,  1.03s/it]\n",
      "Attention Walk (Loss=38.8682):   5%|5         | 10/200 [00:11<03:14,  1.03s/it]\n",
      "Attention Walk (Loss=38.8682):   6%|5         | 11/200 [00:11<03:12,  1.02s/it]\n",
      "Attention Walk (Loss=36.5465):   6%|5         | 11/200 [00:12<03:12,  1.02s/it]\n",
      "Attention Walk (Loss=36.5465):   6%|6         | 12/200 [00:12<03:15,  1.04s/it]\n",
      "Attention Walk (Loss=34.0226):   6%|6         | 12/200 [00:13<03:15,  1.04s/it]\n",
      "Attention Walk (Loss=34.0226):   6%|6         | 13/200 [00:13<03:11,  1.02s/it]\n",
      "Attention Walk (Loss=31.3426):   6%|6         | 13/200 [00:14<03:11,  1.02s/it]\n",
      "Attention Walk (Loss=31.3426):   7%|7         | 14/200 [00:14<03:12,  1.04s/it]\n",
      "Attention Walk (Loss=28.5625):   7%|7         | 14/200 [00:15<03:12,  1.04s/it]\n",
      "Attention Walk (Loss=28.5625):   8%|7         | 15/200 [00:15<03:06,  1.01s/it]\n",
      "Attention Walk (Loss=25.746):   8%|7         | 15/200 [00:16<03:06,  1.01s/it] \n",
      "Attention Walk (Loss=25.746):   8%|8         | 16/200 [00:16<03:04,  1.00s/it]\n",
      "Attention Walk (Loss=22.9605):   8%|8         | 16/200 [00:17<03:04,  1.00s/it]\n",
      "Attention Walk (Loss=22.9605):   8%|8         | 17/200 [00:17<02:59,  1.02it/s]\n",
      "Attention Walk (Loss=20.2726):   8%|8         | 17/200 [00:18<02:59,  1.02it/s]\n",
      "Attention Walk (Loss=20.2726):   9%|9         | 18/200 [00:18<02:58,  1.02it/s]\n",
      "Attention Walk (Loss=17.7427):   9%|9         | 18/200 [00:19<02:58,  1.02it/s]\n",
      "Attention Walk (Loss=17.7427):  10%|9         | 19/200 [00:19<03:08,  1.04s/it]\n",
      "Attention Walk (Loss=15.4195):  10%|9         | 19/200 [00:20<03:08,  1.04s/it]\n",
      "Attention Walk (Loss=15.4195):  10%|#         | 20/200 [00:20<02:56,  1.02it/s]\n",
      "Attention Walk (Loss=13.3363):  10%|#         | 20/200 [00:20<02:56,  1.02it/s]\n",
      "Attention Walk (Loss=13.3363):  10%|#         | 21/200 [00:20<02:51,  1.04it/s]\n",
      "Attention Walk (Loss=11.5089):  10%|#         | 21/200 [00:22<02:51,  1.04it/s]\n",
      "Attention Walk (Loss=11.5089):  11%|#1        | 22/200 [00:22<02:55,  1.01it/s]\n",
      "Attention Walk (Loss=9.9369):  11%|#1        | 22/200 [00:22<02:55,  1.01it/s] \n",
      "Attention Walk (Loss=9.9369):  12%|#1        | 23/200 [00:22<02:52,  1.03it/s]\n",
      "Attention Walk (Loss=8.6065):  12%|#1        | 23/200 [00:23<02:52,  1.03it/s]\n",
      "Attention Walk (Loss=8.6065):  12%|#2        | 24/200 [00:23<02:46,  1.06it/s]\n",
      "Attention Walk (Loss=7.4948):  12%|#2        | 24/200 [00:24<02:46,  1.06it/s]\n",
      "Attention Walk (Loss=7.4948):  12%|#2        | 25/200 [00:24<02:48,  1.04it/s]\n",
      "Attention Walk (Loss=6.5744):  12%|#2        | 25/200 [00:25<02:48,  1.04it/s]\n",
      "Attention Walk (Loss=6.5744):  13%|#3        | 26/200 [00:25<02:46,  1.04it/s]\n",
      "Attention Walk (Loss=5.8168):  13%|#3        | 26/200 [00:26<02:46,  1.04it/s]\n",
      "Attention Walk (Loss=5.8168):  14%|#3        | 27/200 [00:26<02:40,  1.08it/s]\n",
      "Attention Walk (Loss=5.1951):  14%|#3        | 27/200 [00:27<02:40,  1.08it/s]\n",
      "Attention Walk (Loss=5.1951):  14%|#4        | 28/200 [00:27<02:41,  1.07it/s]\n",
      "Attention Walk (Loss=4.6853):  14%|#4        | 28/200 [00:28<02:41,  1.07it/s]\n",
      "Attention Walk (Loss=4.6853):  14%|#4        | 29/200 [00:28<02:30,  1.14it/s]\n",
      "Attention Walk (Loss=4.2671):  14%|#4        | 29/200 [00:29<02:30,  1.14it/s]\n",
      "Attention Walk (Loss=4.2671):  15%|#5        | 30/200 [00:29<02:28,  1.14it/s]\n",
      "Attention Walk (Loss=3.9234):  15%|#5        | 30/200 [00:30<02:28,  1.14it/s]\n",
      "Attention Walk (Loss=3.9234):  16%|#5        | 31/200 [00:30<02:32,  1.11it/s]\n",
      "Attention Walk (Loss=3.6402):  16%|#5        | 31/200 [00:31<02:32,  1.11it/s]\n",
      "Attention Walk (Loss=3.6402):  16%|#6        | 32/200 [00:31<02:33,  1.09it/s]\n",
      "Attention Walk (Loss=3.4064):  16%|#6        | 32/200 [00:31<02:33,  1.09it/s]\n",
      "Attention Walk (Loss=3.4064):  16%|#6        | 33/200 [00:31<02:28,  1.13it/s]\n",
      "Attention Walk (Loss=3.2128):  16%|#6        | 33/200 [00:32<02:28,  1.13it/s]\n",
      "Attention Walk (Loss=3.2128):  17%|#7        | 34/200 [00:32<02:25,  1.14it/s]\n",
      "Attention Walk (Loss=3.0521):  17%|#7        | 34/200 [00:33<02:25,  1.14it/s]\n",
      "Attention Walk (Loss=3.0521):  18%|#7        | 35/200 [00:33<02:21,  1.16it/s]\n",
      "Attention Walk (Loss=2.9183):  18%|#7        | 35/200 [00:34<02:21,  1.16it/s]\n",
      "Attention Walk (Loss=2.9183):  18%|#8        | 36/200 [00:34<02:23,  1.14it/s]\n",
      "Attention Walk (Loss=2.8065):  18%|#8        | 36/200 [00:35<02:23,  1.14it/s]\n",
      "Attention Walk (Loss=2.8065):  18%|#8        | 37/200 [00:35<02:16,  1.19it/s]\n",
      "Attention Walk (Loss=2.713):  18%|#8        | 37/200 [00:36<02:16,  1.19it/s] \n",
      "Attention Walk (Loss=2.713):  19%|#9        | 38/200 [00:36<02:21,  1.15it/s]\n",
      "Attention Walk (Loss=2.6343):  19%|#9        | 38/200 [00:37<02:21,  1.15it/s]\n",
      "Attention Walk (Loss=2.6343):  20%|#9        | 39/200 [00:37<02:18,  1.16it/s]\n",
      "Attention Walk (Loss=2.568):  20%|#9        | 39/200 [00:38<02:18,  1.16it/s] \n",
      "Attention Walk (Loss=2.568):  20%|##        | 40/200 [00:38<02:20,  1.14it/s]\n",
      "Attention Walk (Loss=2.5118):  20%|##        | 40/200 [00:38<02:20,  1.14it/s]\n",
      "Attention Walk (Loss=2.5118):  20%|##        | 41/200 [00:38<02:14,  1.18it/s]\n",
      "Attention Walk (Loss=2.4639):  20%|##        | 41/200 [00:39<02:14,  1.18it/s]\n",
      "Attention Walk (Loss=2.4639):  21%|##1       | 42/200 [00:39<02:19,  1.14it/s]\n",
      "Attention Walk (Loss=2.423):  21%|##1       | 42/200 [00:40<02:19,  1.14it/s] \n",
      "Attention Walk (Loss=2.423):  22%|##1       | 43/200 [00:40<02:24,  1.09it/s]\n",
      "Attention Walk (Loss=2.3877):  22%|##1       | 43/200 [00:41<02:24,  1.09it/s]\n",
      "Attention Walk (Loss=2.3877):  22%|##2       | 44/200 [00:41<02:22,  1.10it/s]\n",
      "Attention Walk (Loss=2.357):  22%|##2       | 44/200 [00:42<02:22,  1.10it/s] \n",
      "Attention Walk (Loss=2.357):  22%|##2       | 45/200 [00:42<02:18,  1.12it/s]\n",
      "Attention Walk (Loss=2.3302):  22%|##2       | 45/200 [00:43<02:18,  1.12it/s]\n",
      "Attention Walk (Loss=2.3302):  23%|##3       | 46/200 [00:43<02:24,  1.06it/s]\n",
      "Attention Walk (Loss=2.3066):  23%|##3       | 46/200 [00:44<02:24,  1.06it/s]\n",
      "Attention Walk (Loss=2.3066):  24%|##3       | 47/200 [00:44<02:22,  1.07it/s]\n",
      "Attention Walk (Loss=2.2856):  24%|##3       | 47/200 [00:45<02:22,  1.07it/s]\n",
      "Attention Walk (Loss=2.2856):  24%|##4       | 48/200 [00:45<02:21,  1.08it/s]\n",
      "Attention Walk (Loss=2.2668):  24%|##4       | 48/200 [00:46<02:21,  1.08it/s]\n",
      "Attention Walk (Loss=2.2668):  24%|##4       | 49/200 [00:46<02:23,  1.05it/s]\n",
      "Attention Walk (Loss=2.2497):  24%|##4       | 49/200 [00:47<02:23,  1.05it/s]\n",
      "Attention Walk (Loss=2.2497):  25%|##5       | 50/200 [00:47<02:20,  1.07it/s]\n",
      "Attention Walk (Loss=2.2341):  25%|##5       | 50/200 [00:48<02:20,  1.07it/s]\n",
      "Attention Walk (Loss=2.2341):  26%|##5       | 51/200 [00:48<02:21,  1.05it/s]\n",
      "Attention Walk (Loss=2.2198):  26%|##5       | 51/200 [00:49<02:21,  1.05it/s]\n",
      "Attention Walk (Loss=2.2198):  26%|##6       | 52/200 [00:49<02:25,  1.02it/s]\n",
      "Attention Walk (Loss=2.2065):  26%|##6       | 52/200 [00:50<02:25,  1.02it/s]\n",
      "Attention Walk (Loss=2.2065):  26%|##6       | 53/200 [00:50<02:21,  1.04it/s]\n",
      "Attention Walk (Loss=2.1941):  26%|##6       | 53/200 [00:51<02:21,  1.04it/s]\n",
      "Attention Walk (Loss=2.1941):  27%|##7       | 54/200 [00:51<02:28,  1.02s/it]\n",
      "Attention Walk (Loss=2.1825):  27%|##7       | 54/200 [00:52<02:28,  1.02s/it]\n",
      "Attention Walk (Loss=2.1825):  28%|##7       | 55/200 [00:52<02:26,  1.01s/it]\n",
      "Attention Walk (Loss=2.1714):  28%|##7       | 55/200 [00:53<02:26,  1.01s/it]\n",
      "Attention Walk (Loss=2.1714):  28%|##8       | 56/200 [00:53<02:24,  1.00s/it]\n",
      "Attention Walk (Loss=2.161):  28%|##8       | 56/200 [00:54<02:24,  1.00s/it] \n",
      "Attention Walk (Loss=2.161):  28%|##8       | 57/200 [00:54<02:26,  1.02s/it]\n",
      "Attention Walk (Loss=2.151):  28%|##8       | 57/200 [00:55<02:26,  1.02s/it]\n",
      "Attention Walk (Loss=2.151):  29%|##9       | 58/200 [00:55<02:23,  1.01s/it]\n",
      "Attention Walk (Loss=2.1414):  29%|##9       | 58/200 [00:56<02:23,  1.01s/it]\n",
      "Attention Walk (Loss=2.1414):  30%|##9       | 59/200 [00:56<02:22,  1.01s/it]\n",
      "Attention Walk (Loss=2.1323):  30%|##9       | 59/200 [00:57<02:22,  1.01s/it]\n",
      "Attention Walk (Loss=2.1323):  30%|###       | 60/200 [00:57<02:21,  1.01s/it]\n",
      "Attention Walk (Loss=2.1235):  30%|###       | 60/200 [00:58<02:21,  1.01s/it]\n",
      "Attention Walk (Loss=2.1235):  30%|###       | 61/200 [00:58<02:21,  1.01s/it]\n",
      "Attention Walk (Loss=2.115):  30%|###       | 61/200 [00:59<02:21,  1.01s/it] \n",
      "Attention Walk (Loss=2.115):  31%|###1      | 62/200 [00:59<02:18,  1.01s/it]\n",
      "Attention Walk (Loss=2.1068):  31%|###1      | 62/200 [01:00<02:18,  1.01s/it]\n",
      "Attention Walk (Loss=2.1068):  32%|###1      | 63/200 [01:00<02:15,  1.01it/s]\n",
      "Attention Walk (Loss=2.0989):  32%|###1      | 63/200 [01:01<02:15,  1.01it/s]\n",
      "Attention Walk (Loss=2.0989):  32%|###2      | 64/200 [01:01<02:10,  1.04it/s]\n",
      "Attention Walk (Loss=2.0913):  32%|###2      | 64/200 [01:02<02:10,  1.04it/s]\n",
      "Attention Walk (Loss=2.0913):  32%|###2      | 65/200 [01:02<02:08,  1.05it/s]\n",
      "Attention Walk (Loss=2.0839):  32%|###2      | 65/200 [01:03<02:08,  1.05it/s]\n",
      "Attention Walk (Loss=2.0839):  33%|###3      | 66/200 [01:03<02:05,  1.07it/s]\n",
      "Attention Walk (Loss=2.0767):  33%|###3      | 66/200 [01:03<02:05,  1.07it/s]\n",
      "Attention Walk (Loss=2.0767):  34%|###3      | 67/200 [01:03<02:01,  1.10it/s]\n",
      "Attention Walk (Loss=2.0698):  34%|###3      | 67/200 [01:04<02:01,  1.10it/s]\n",
      "Attention Walk (Loss=2.0698):  34%|###4      | 68/200 [01:04<01:55,  1.14it/s]\n",
      "Attention Walk (Loss=2.063):  34%|###4      | 68/200 [01:05<01:55,  1.14it/s] \n",
      "Attention Walk (Loss=2.063):  34%|###4      | 69/200 [01:05<01:53,  1.16it/s]\n",
      "Attention Walk (Loss=2.0565):  34%|###4      | 69/200 [01:06<01:53,  1.16it/s]\n",
      "Attention Walk (Loss=2.0565):  35%|###5      | 70/200 [01:06<01:52,  1.15it/s]\n",
      "Attention Walk (Loss=2.0501):  35%|###5      | 70/200 [01:07<01:52,  1.15it/s]\n",
      "Attention Walk (Loss=2.0501):  36%|###5      | 71/200 [01:07<01:50,  1.17it/s]\n",
      "Attention Walk (Loss=2.044):  36%|###5      | 71/200 [01:08<01:50,  1.17it/s] \n",
      "Attention Walk (Loss=2.044):  36%|###6      | 72/200 [01:08<01:51,  1.15it/s]\n",
      "Attention Walk (Loss=2.038):  36%|###6      | 72/200 [01:09<01:51,  1.15it/s]\n",
      "Attention Walk (Loss=2.038):  36%|###6      | 73/200 [01:09<01:50,  1.15it/s]\n",
      "Attention Walk (Loss=2.0322):  36%|###6      | 73/200 [01:09<01:50,  1.15it/s]\n",
      "Attention Walk (Loss=2.0322):  37%|###7      | 74/200 [01:09<01:48,  1.16it/s]\n",
      "Attention Walk (Loss=2.0265):  37%|###7      | 74/200 [01:10<01:48,  1.16it/s]\n",
      "Attention Walk (Loss=2.0265):  38%|###7      | 75/200 [01:10<01:46,  1.17it/s]\n",
      "Attention Walk (Loss=2.021):  38%|###7      | 75/200 [01:11<01:46,  1.17it/s] \n",
      "Attention Walk (Loss=2.021):  38%|###8      | 76/200 [01:11<01:44,  1.19it/s]\n",
      "Attention Walk (Loss=2.0156):  38%|###8      | 76/200 [01:12<01:44,  1.19it/s]\n",
      "Attention Walk (Loss=2.0156):  38%|###8      | 77/200 [01:12<01:47,  1.15it/s]\n",
      "Attention Walk (Loss=2.0104):  38%|###8      | 77/200 [01:13<01:47,  1.15it/s]\n",
      "Attention Walk (Loss=2.0104):  39%|###9      | 78/200 [01:13<01:47,  1.13it/s]\n",
      "Attention Walk (Loss=2.0053):  39%|###9      | 78/200 [01:14<01:47,  1.13it/s]\n",
      "Attention Walk (Loss=2.0053):  40%|###9      | 79/200 [01:14<01:45,  1.15it/s]\n",
      "Attention Walk (Loss=2.0004):  40%|###9      | 79/200 [01:15<01:45,  1.15it/s]\n",
      "Attention Walk (Loss=2.0004):  40%|####      | 80/200 [01:15<01:42,  1.17it/s]\n",
      "Attention Walk (Loss=1.9956):  40%|####      | 80/200 [01:15<01:42,  1.17it/s]\n",
      "Attention Walk (Loss=1.9956):  40%|####      | 81/200 [01:15<01:39,  1.19it/s]\n",
      "Attention Walk (Loss=1.9909):  40%|####      | 81/200 [01:16<01:39,  1.19it/s]\n",
      "Attention Walk (Loss=1.9909):  41%|####1     | 82/200 [01:16<01:38,  1.19it/s]\n",
      "Attention Walk (Loss=1.9863):  41%|####1     | 82/200 [01:17<01:38,  1.19it/s]\n",
      "Attention Walk (Loss=1.9863):  42%|####1     | 83/200 [01:17<01:42,  1.14it/s]\n",
      "Attention Walk (Loss=1.9819):  42%|####1     | 83/200 [01:18<01:42,  1.14it/s]\n",
      "Attention Walk (Loss=1.9819):  42%|####2     | 84/200 [01:18<01:44,  1.11it/s]\n",
      "Attention Walk (Loss=1.9775):  42%|####2     | 84/200 [01:19<01:44,  1.11it/s]\n",
      "Attention Walk (Loss=1.9775):  42%|####2     | 85/200 [01:19<01:47,  1.07it/s]\n",
      "Attention Walk (Loss=1.9733):  42%|####2     | 85/200 [01:20<01:47,  1.07it/s]\n",
      "Attention Walk (Loss=1.9733):  43%|####3     | 86/200 [01:20<01:44,  1.09it/s]\n",
      "Attention Walk (Loss=1.9691):  43%|####3     | 86/200 [01:21<01:44,  1.09it/s]\n",
      "Attention Walk (Loss=1.9691):  44%|####3     | 87/200 [01:21<01:46,  1.06it/s]\n",
      "Attention Walk (Loss=1.9651):  44%|####3     | 87/200 [01:22<01:46,  1.06it/s]\n",
      "Attention Walk (Loss=1.9651):  44%|####4     | 88/200 [01:22<01:46,  1.05it/s]\n",
      "Attention Walk (Loss=1.9611):  44%|####4     | 88/200 [01:23<01:46,  1.05it/s]\n",
      "Attention Walk (Loss=1.9611):  44%|####4     | 89/200 [01:23<01:50,  1.01it/s]\n",
      "Attention Walk (Loss=1.9572):  44%|####4     | 89/200 [01:24<01:50,  1.01it/s]\n",
      "Attention Walk (Loss=1.9572):  45%|####5     | 90/200 [01:24<01:46,  1.03it/s]\n",
      "Attention Walk (Loss=1.9534):  45%|####5     | 90/200 [01:25<01:46,  1.03it/s]\n",
      "Attention Walk (Loss=1.9534):  46%|####5     | 91/200 [01:25<01:42,  1.07it/s]\n",
      "Attention Walk (Loss=1.9497):  46%|####5     | 91/200 [01:26<01:42,  1.07it/s]\n",
      "Attention Walk (Loss=1.9497):  46%|####6     | 92/200 [01:26<01:36,  1.12it/s]\n",
      "Attention Walk (Loss=1.9461):  46%|####6     | 92/200 [01:27<01:36,  1.12it/s]\n",
      "Attention Walk (Loss=1.9461):  46%|####6     | 93/200 [01:27<01:34,  1.13it/s]\n",
      "Attention Walk (Loss=1.9425):  46%|####6     | 93/200 [01:27<01:34,  1.13it/s]\n",
      "Attention Walk (Loss=1.9425):  47%|####6     | 94/200 [01:27<01:35,  1.11it/s]\n",
      "Attention Walk (Loss=1.9391):  47%|####6     | 94/200 [01:28<01:35,  1.11it/s]\n",
      "Attention Walk (Loss=1.9391):  48%|####7     | 95/200 [01:28<01:35,  1.10it/s]\n",
      "Attention Walk (Loss=1.9356):  48%|####7     | 95/200 [01:29<01:35,  1.10it/s]\n",
      "Attention Walk (Loss=1.9356):  48%|####8     | 96/200 [01:29<01:33,  1.11it/s]\n",
      "Attention Walk (Loss=1.9323):  48%|####8     | 96/200 [01:30<01:33,  1.11it/s]\n",
      "Attention Walk (Loss=1.9323):  48%|####8     | 97/200 [01:30<01:31,  1.12it/s]\n",
      "Attention Walk (Loss=1.929):  48%|####8     | 97/200 [01:31<01:31,  1.12it/s] \n",
      "Attention Walk (Loss=1.929):  49%|####9     | 98/200 [01:31<01:31,  1.12it/s]\n",
      "Attention Walk (Loss=1.9258):  49%|####9     | 98/200 [01:32<01:31,  1.12it/s]\n",
      "Attention Walk (Loss=1.9258):  50%|####9     | 99/200 [01:32<01:33,  1.08it/s]\n",
      "Attention Walk (Loss=1.9226):  50%|####9     | 99/200 [01:33<01:33,  1.08it/s]\n",
      "Attention Walk (Loss=1.9226):  50%|#####     | 100/200 [01:33<01:34,  1.06it/s]\n",
      "Attention Walk (Loss=1.9195):  50%|#####     | 100/200 [01:34<01:34,  1.06it/s]\n",
      "Attention Walk (Loss=1.9195):  50%|#####     | 101/200 [01:34<01:36,  1.03it/s]\n",
      "Attention Walk (Loss=1.9165):  50%|#####     | 101/200 [01:35<01:36,  1.03it/s]\n",
      "Attention Walk (Loss=1.9165):  51%|#####1    | 102/200 [01:35<01:34,  1.04it/s]\n",
      "Attention Walk (Loss=1.9135):  51%|#####1    | 102/200 [01:36<01:34,  1.04it/s]\n",
      "Attention Walk (Loss=1.9135):  52%|#####1    | 103/200 [01:36<01:34,  1.02it/s]\n",
      "Attention Walk (Loss=1.9105):  52%|#####1    | 103/200 [01:37<01:34,  1.02it/s]\n",
      "Attention Walk (Loss=1.9105):  52%|#####2    | 104/200 [01:37<01:33,  1.03it/s]\n",
      "Attention Walk (Loss=1.9076):  52%|#####2    | 104/200 [01:38<01:33,  1.03it/s]\n",
      "Attention Walk (Loss=1.9076):  52%|#####2    | 105/200 [01:38<01:30,  1.05it/s]\n",
      "Attention Walk (Loss=1.9048):  52%|#####2    | 105/200 [01:39<01:30,  1.05it/s]\n",
      "Attention Walk (Loss=1.9048):  53%|#####3    | 106/200 [01:39<01:30,  1.04it/s]\n",
      "Attention Walk (Loss=1.902):  53%|#####3    | 106/200 [01:40<01:30,  1.04it/s] \n",
      "Attention Walk (Loss=1.902):  54%|#####3    | 107/200 [01:40<01:33,  1.01s/it]\n",
      "Attention Walk (Loss=1.8992):  54%|#####3    | 107/200 [01:41<01:33,  1.01s/it]\n",
      "Attention Walk (Loss=1.8992):  54%|#####4    | 108/200 [01:41<01:32,  1.00s/it]\n",
      "Attention Walk (Loss=1.8965):  54%|#####4    | 108/200 [01:42<01:32,  1.00s/it]\n",
      "Attention Walk (Loss=1.8965):  55%|#####4    | 109/200 [01:42<01:30,  1.01it/s]\n",
      "Attention Walk (Loss=1.8938):  55%|#####4    | 109/200 [01:43<01:30,  1.01it/s]\n",
      "Attention Walk (Loss=1.8938):  55%|#####5    | 110/200 [01:43<01:28,  1.02it/s]\n",
      "Attention Walk (Loss=1.8912):  55%|#####5    | 110/200 [01:44<01:28,  1.02it/s]\n",
      "Attention Walk (Loss=1.8912):  56%|#####5    | 111/200 [01:44<01:26,  1.03it/s]\n",
      "Attention Walk (Loss=1.8886):  56%|#####5    | 111/200 [01:45<01:26,  1.03it/s]\n",
      "Attention Walk (Loss=1.8886):  56%|#####6    | 112/200 [01:45<01:25,  1.03it/s]\n",
      "Attention Walk (Loss=1.8861):  56%|#####6    | 112/200 [01:46<01:25,  1.03it/s]\n",
      "Attention Walk (Loss=1.8861):  56%|#####6    | 113/200 [01:46<01:23,  1.04it/s]\n",
      "Attention Walk (Loss=1.8836):  56%|#####6    | 113/200 [01:47<01:23,  1.04it/s]\n",
      "Attention Walk (Loss=1.8836):  57%|#####6    | 114/200 [01:47<01:25,  1.01it/s]\n",
      "Attention Walk (Loss=1.8811):  57%|#####6    | 114/200 [01:48<01:25,  1.01it/s]\n",
      "Attention Walk (Loss=1.8811):  57%|#####7    | 115/200 [01:48<01:25,  1.01s/it]\n",
      "Attention Walk (Loss=1.8787):  57%|#####7    | 115/200 [01:49<01:25,  1.01s/it]\n",
      "Attention Walk (Loss=1.8787):  58%|#####8    | 116/200 [01:49<01:23,  1.00it/s]\n",
      "Attention Walk (Loss=1.8763):  58%|#####8    | 116/200 [01:50<01:23,  1.00it/s]\n",
      "Attention Walk (Loss=1.8763):  58%|#####8    | 117/200 [01:50<01:25,  1.03s/it]\n",
      "Attention Walk (Loss=1.874):  58%|#####8    | 117/200 [01:51<01:25,  1.03s/it] \n",
      "Attention Walk (Loss=1.874):  59%|#####8    | 118/200 [01:51<01:27,  1.07s/it]\n",
      "Attention Walk (Loss=1.8717):  59%|#####8    | 118/200 [01:52<01:27,  1.07s/it]\n",
      "Attention Walk (Loss=1.8717):  60%|#####9    | 119/200 [01:52<01:23,  1.04s/it]\n",
      "Attention Walk (Loss=1.8694):  60%|#####9    | 119/200 [01:53<01:23,  1.04s/it]\n",
      "Attention Walk (Loss=1.8694):  60%|######    | 120/200 [01:53<01:21,  1.02s/it]\n",
      "Attention Walk (Loss=1.8672):  60%|######    | 120/200 [01:54<01:21,  1.02s/it]\n",
      "Attention Walk (Loss=1.8672):  60%|######    | 121/200 [01:54<01:19,  1.00s/it]\n",
      "Attention Walk (Loss=1.865):  60%|######    | 121/200 [01:55<01:19,  1.00s/it] \n",
      "Attention Walk (Loss=1.865):  61%|######1   | 122/200 [01:55<01:17,  1.01it/s]\n",
      "Attention Walk (Loss=1.8628):  61%|######1   | 122/200 [01:56<01:17,  1.01it/s]\n",
      "Attention Walk (Loss=1.8628):  62%|######1   | 123/200 [01:56<01:14,  1.03it/s]\n",
      "Attention Walk (Loss=1.8607):  62%|######1   | 123/200 [01:57<01:14,  1.03it/s]\n",
      "Attention Walk (Loss=1.8607):  62%|######2   | 124/200 [01:57<01:12,  1.05it/s]\n",
      "Attention Walk (Loss=1.8586):  62%|######2   | 124/200 [01:58<01:12,  1.05it/s]\n",
      "Attention Walk (Loss=1.8586):  62%|######2   | 125/200 [01:58<01:10,  1.07it/s]\n",
      "Attention Walk (Loss=1.8565):  62%|######2   | 125/200 [01:59<01:10,  1.07it/s]\n",
      "Attention Walk (Loss=1.8565):  63%|######3   | 126/200 [01:59<01:09,  1.06it/s]\n",
      "Attention Walk (Loss=1.8545):  63%|######3   | 126/200 [02:00<01:09,  1.06it/s]\n",
      "Attention Walk (Loss=1.8545):  64%|######3   | 127/200 [02:00<01:09,  1.05it/s]\n",
      "Attention Walk (Loss=1.8525):  64%|######3   | 127/200 [02:01<01:09,  1.05it/s]\n",
      "Attention Walk (Loss=1.8525):  64%|######4   | 128/200 [02:01<01:09,  1.03it/s]\n",
      "Attention Walk (Loss=1.8505):  64%|######4   | 128/200 [02:02<01:09,  1.03it/s]\n",
      "Attention Walk (Loss=1.8505):  64%|######4   | 129/200 [02:02<01:08,  1.03it/s]\n",
      "Attention Walk (Loss=1.8486):  64%|######4   | 129/200 [02:03<01:08,  1.03it/s]\n",
      "Attention Walk (Loss=1.8486):  65%|######5   | 130/200 [02:03<01:07,  1.04it/s]\n",
      "Attention Walk (Loss=1.8466):  65%|######5   | 130/200 [02:04<01:07,  1.04it/s]\n",
      "Attention Walk (Loss=1.8466):  66%|######5   | 131/200 [02:04<01:06,  1.04it/s]\n",
      "Attention Walk (Loss=1.8448):  66%|######5   | 131/200 [02:05<01:06,  1.04it/s]\n",
      "Attention Walk (Loss=1.8448):  66%|######6   | 132/200 [02:05<01:06,  1.02it/s]\n",
      "Attention Walk (Loss=1.8429):  66%|######6   | 132/200 [02:06<01:06,  1.02it/s]\n",
      "Attention Walk (Loss=1.8429):  66%|######6   | 133/200 [02:06<01:06,  1.00it/s]\n",
      "Attention Walk (Loss=1.8411):  66%|######6   | 133/200 [02:06<01:06,  1.00it/s]\n",
      "Attention Walk (Loss=1.8411):  67%|######7   | 134/200 [02:06<01:03,  1.04it/s]\n",
      "Attention Walk (Loss=1.8393):  67%|######7   | 134/200 [02:07<01:03,  1.04it/s]\n",
      "Attention Walk (Loss=1.8393):  68%|######7   | 135/200 [02:07<01:00,  1.08it/s]\n",
      "Attention Walk (Loss=1.8375):  68%|######7   | 135/200 [02:08<01:00,  1.08it/s]\n",
      "Attention Walk (Loss=1.8375):  68%|######8   | 136/200 [02:08<00:57,  1.11it/s]\n",
      "Attention Walk (Loss=1.8358):  68%|######8   | 136/200 [02:09<00:57,  1.11it/s]\n",
      "Attention Walk (Loss=1.8358):  68%|######8   | 137/200 [02:09<00:55,  1.13it/s]\n",
      "Attention Walk (Loss=1.834):  68%|######8   | 137/200 [02:10<00:55,  1.13it/s] \n",
      "Attention Walk (Loss=1.834):  69%|######9   | 138/200 [02:10<00:53,  1.17it/s]\n",
      "Attention Walk (Loss=1.8323):  69%|######9   | 138/200 [02:11<00:53,  1.17it/s]\n",
      "Attention Walk (Loss=1.8323):  70%|######9   | 139/200 [02:11<00:51,  1.19it/s]\n",
      "Attention Walk (Loss=1.8307):  70%|######9   | 139/200 [02:12<00:51,  1.19it/s]\n",
      "Attention Walk (Loss=1.8307):  70%|#######   | 140/200 [02:12<00:53,  1.12it/s]\n",
      "Attention Walk (Loss=1.829):  70%|#######   | 140/200 [02:13<00:53,  1.12it/s] \n",
      "Attention Walk (Loss=1.829):  70%|#######   | 141/200 [02:13<00:54,  1.09it/s]\n",
      "Attention Walk (Loss=1.8274):  70%|#######   | 141/200 [02:14<00:54,  1.09it/s]\n",
      "Attention Walk (Loss=1.8274):  71%|#######1  | 142/200 [02:14<00:55,  1.04it/s]\n",
      "Attention Walk (Loss=1.8258):  71%|#######1  | 142/200 [02:14<00:55,  1.04it/s]\n",
      "Attention Walk (Loss=1.8258):  72%|#######1  | 143/200 [02:14<00:53,  1.08it/s]\n",
      "Attention Walk (Loss=1.8242):  72%|#######1  | 143/200 [02:16<00:53,  1.08it/s]\n",
      "Attention Walk (Loss=1.8242):  72%|#######2  | 144/200 [02:16<00:53,  1.04it/s]\n",
      "Attention Walk (Loss=1.8227):  72%|#######2  | 144/200 [02:17<00:53,  1.04it/s]\n",
      "Attention Walk (Loss=1.8227):  72%|#######2  | 145/200 [02:17<00:53,  1.02it/s]\n",
      "Attention Walk (Loss=1.8211):  72%|#######2  | 145/200 [02:18<00:53,  1.02it/s]\n",
      "Attention Walk (Loss=1.8211):  73%|#######3  | 146/200 [02:18<00:53,  1.02it/s]\n",
      "Attention Walk (Loss=1.8196):  73%|#######3  | 146/200 [02:19<00:53,  1.02it/s]\n",
      "Attention Walk (Loss=1.8196):  74%|#######3  | 147/200 [02:19<00:54,  1.03s/it]\n",
      "Attention Walk (Loss=1.8181):  74%|#######3  | 147/200 [02:20<00:54,  1.03s/it]\n",
      "Attention Walk (Loss=1.8181):  74%|#######4  | 148/200 [02:20<00:51,  1.02it/s]\n",
      "Attention Walk (Loss=1.8167):  74%|#######4  | 148/200 [02:20<00:51,  1.02it/s]\n",
      "Attention Walk (Loss=1.8167):  74%|#######4  | 149/200 [02:20<00:48,  1.05it/s]\n",
      "Attention Walk (Loss=1.8152):  74%|#######4  | 149/200 [02:21<00:48,  1.05it/s]\n",
      "Attention Walk (Loss=1.8152):  75%|#######5  | 150/200 [02:21<00:47,  1.05it/s]\n",
      "Attention Walk (Loss=1.8138):  75%|#######5  | 150/200 [02:22<00:47,  1.05it/s]\n",
      "Attention Walk (Loss=1.8138):  76%|#######5  | 151/200 [02:22<00:46,  1.04it/s]\n",
      "Attention Walk (Loss=1.8124):  76%|#######5  | 151/200 [02:23<00:46,  1.04it/s]\n",
      "Attention Walk (Loss=1.8124):  76%|#######6  | 152/200 [02:23<00:44,  1.08it/s]\n",
      "Attention Walk (Loss=1.811):  76%|#######6  | 152/200 [02:24<00:44,  1.08it/s] \n",
      "Attention Walk (Loss=1.811):  76%|#######6  | 153/200 [02:24<00:43,  1.08it/s]\n",
      "Attention Walk (Loss=1.8096):  76%|#######6  | 153/200 [02:25<00:43,  1.08it/s]\n",
      "Attention Walk (Loss=1.8096):  77%|#######7  | 154/200 [02:25<00:41,  1.12it/s]\n",
      "Attention Walk (Loss=1.8083):  77%|#######7  | 154/200 [02:26<00:41,  1.12it/s]\n",
      "Attention Walk (Loss=1.8083):  78%|#######7  | 155/200 [02:26<00:40,  1.12it/s]\n",
      "Attention Walk (Loss=1.807):  78%|#######7  | 155/200 [02:27<00:40,  1.12it/s] \n",
      "Attention Walk (Loss=1.807):  78%|#######8  | 156/200 [02:27<00:42,  1.04it/s]\n",
      "Attention Walk (Loss=1.8056):  78%|#######8  | 156/200 [02:28<00:42,  1.04it/s]\n",
      "Attention Walk (Loss=1.8056):  78%|#######8  | 157/200 [02:28<00:43,  1.01s/it]\n",
      "Attention Walk (Loss=1.8043):  78%|#######8  | 157/200 [02:29<00:43,  1.01s/it]\n",
      "Attention Walk (Loss=1.8043):  79%|#######9  | 158/200 [02:29<00:43,  1.03s/it]\n",
      "Attention Walk (Loss=1.8031):  79%|#######9  | 158/200 [02:30<00:43,  1.03s/it]\n",
      "Attention Walk (Loss=1.8031):  80%|#######9  | 159/200 [02:30<00:41,  1.00s/it]\n",
      "Attention Walk (Loss=1.8018):  80%|#######9  | 159/200 [02:31<00:41,  1.00s/it]\n",
      "Attention Walk (Loss=1.8018):  80%|########  | 160/200 [02:31<00:40,  1.02s/it]\n",
      "Attention Walk (Loss=1.8006):  80%|########  | 160/200 [02:32<00:40,  1.02s/it]\n",
      "Attention Walk (Loss=1.8006):  80%|########  | 161/200 [02:32<00:38,  1.01it/s]\n",
      "Attention Walk (Loss=1.7993):  80%|########  | 161/200 [02:33<00:38,  1.01it/s]\n",
      "Attention Walk (Loss=1.7993):  81%|########1 | 162/200 [02:33<00:37,  1.03it/s]\n",
      "Attention Walk (Loss=1.7981):  81%|########1 | 162/200 [02:34<00:37,  1.03it/s]\n",
      "Attention Walk (Loss=1.7981):  82%|########1 | 163/200 [02:34<00:35,  1.05it/s]\n",
      "Attention Walk (Loss=1.7969):  82%|########1 | 163/200 [02:35<00:35,  1.05it/s]\n",
      "Attention Walk (Loss=1.7969):  82%|########2 | 164/200 [02:35<00:32,  1.09it/s]\n",
      "Attention Walk (Loss=1.7958):  82%|########2 | 164/200 [02:36<00:32,  1.09it/s]\n",
      "Attention Walk (Loss=1.7958):  82%|########2 | 165/200 [02:36<00:31,  1.11it/s]\n",
      "Attention Walk (Loss=1.7946):  82%|########2 | 165/200 [02:37<00:31,  1.11it/s]\n",
      "Attention Walk (Loss=1.7946):  83%|########2 | 166/200 [02:37<00:30,  1.10it/s]\n",
      "Attention Walk (Loss=1.7935):  83%|########2 | 166/200 [02:37<00:30,  1.10it/s]\n",
      "Attention Walk (Loss=1.7935):  84%|########3 | 167/200 [02:37<00:29,  1.11it/s]\n",
      "Attention Walk (Loss=1.7923):  84%|########3 | 167/200 [02:38<00:29,  1.11it/s]\n",
      "Attention Walk (Loss=1.7923):  84%|########4 | 168/200 [02:38<00:28,  1.13it/s]\n",
      "Attention Walk (Loss=1.7912):  84%|########4 | 168/200 [02:39<00:28,  1.13it/s]\n",
      "Attention Walk (Loss=1.7912):  84%|########4 | 169/200 [02:39<00:26,  1.15it/s]\n",
      "Attention Walk (Loss=1.7901):  84%|########4 | 169/200 [02:40<00:26,  1.15it/s]\n",
      "Attention Walk (Loss=1.7901):  85%|########5 | 170/200 [02:40<00:25,  1.16it/s]\n",
      "Attention Walk (Loss=1.789):  85%|########5 | 170/200 [02:41<00:25,  1.16it/s] \n",
      "Attention Walk (Loss=1.789):  86%|########5 | 171/200 [02:41<00:25,  1.15it/s]\n",
      "Attention Walk (Loss=1.788):  86%|########5 | 171/200 [02:42<00:25,  1.15it/s]\n",
      "Attention Walk (Loss=1.788):  86%|########6 | 172/200 [02:42<00:24,  1.16it/s]\n",
      "Attention Walk (Loss=1.7869):  86%|########6 | 172/200 [02:42<00:24,  1.16it/s]\n",
      "Attention Walk (Loss=1.7869):  86%|########6 | 173/200 [02:42<00:22,  1.19it/s]\n",
      "Attention Walk (Loss=1.7859):  86%|########6 | 173/200 [02:43<00:22,  1.19it/s]\n",
      "Attention Walk (Loss=1.7859):  87%|########7 | 174/200 [02:43<00:22,  1.18it/s]\n",
      "Attention Walk (Loss=1.7848):  87%|########7 | 174/200 [02:44<00:22,  1.18it/s]\n",
      "Attention Walk (Loss=1.7848):  88%|########7 | 175/200 [02:44<00:20,  1.19it/s]\n",
      "Attention Walk (Loss=1.7848):  88%|########7 | 175/200 [02:45<00:23,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/facebook/facebook_combined.csv --embedding-path ../../result/embeddings_facebook_AW_128.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c81e3",
   "metadata": {
    "id": "de1c81e3"
   },
   "source": [
    "## Test with the citation network\n",
    "Infeasible!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1dc864",
   "metadata": {
    "id": "ab1dc864",
    "outputId": "a9337ff5-b81b-407b-80a2-072176a06703"
   },
   "outputs": [],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/cit-HepTh_edges.csv --embedding-path ../../result/cit-HepTh_embeddings_attention.csv --attention-path ../../result/cit-HepTh_attention.csv --epochs 176"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P2KVPWY9ovVm",
   "metadata": {
    "id": "P2KVPWY9ovVm"
   },
   "source": [
    "## Test with the biological network\n",
    "Infeasible!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-URjbGoLozuS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-URjbGoLozuS",
    "outputId": "17f2f3d5-e0ad-49dd-ffb1-760414c60dec"
   },
   "outputs": [],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/bio-CE-CX_edges.csv --embedding-path ../../result/bio-CE-CX_embeddings_attention.csv --attention-path ....//result/bio-CE-CX_attention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc8b89",
   "metadata": {},
   "source": [
    "## Test proteins network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee1c589",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------------------------------+\n",
      "| Attention path |     ./output/chameleon_AW_attention.csv     |\n",
      "+================+=============================================+\n",
      "| Beta           | 0.500                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Dimensions     | 128                                         |\n",
      "+----------------+---------------------------------------------+\n",
      "| Edge path      | ../../data/proteins/PROTEINS-full.csv       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Embedding path | ../../result/embeddings_PROTEINS_AW_128.csv |\n",
      "+----------------+---------------------------------------------+\n",
      "| Epochs         | 200                                         |\n",
      "+----------------+---------------------------------------------+\n",
      "| Gamma          | 0.500                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Learning rate  | 0.010                                       |\n",
      "+----------------+---------------------------------------------+\n",
      "| Num of walks   | 80                                          |\n",
      "+----------------+---------------------------------------------+\n",
      "| Window size    | 5                                           |\n",
      "+----------------+---------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_data.py:133: RuntimeWarning: divide by zero encountered in power\n",
      "  return self._with_data(data ** n)\n",
      "\n",
      "Adjacency matrix powers:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:  25%|██▌       | 1/4 [00:00<00:01,  1.75it/s]\n",
      "Adjacency matrix powers:  50%|█████     | 2/4 [00:01<00:01,  1.07it/s]\n",
      "Adjacency matrix powers:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\main.py\", line 19, in <module>\n",
      "    main()\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\main.py\", line 14, in main\n",
      "    model = AttentionWalkTrainer(args)\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\attentionwalk.py\", line 78, in __init__\n",
      "    self.initialize_model_and_features()\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\attentionwalk.py\", line 84, in initialize_model_and_features\n",
      "    self.target_tensor = feature_calculator(self.args, self.graph)\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\utils.py\", line 56, in feature_calculator\n",
      "    to_add = powered_A.todense()\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 959, in todense\n",
      "    return self._ascontainer(self.toarray(order=order, out=out))\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py\", line 1106, in toarray\n",
      "    out = self._process_toarray_args(order, out)\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 1327, in _process_toarray_args\n",
      "    return np.zeros(self.shape, dtype=self.dtype, order=order)\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 14.1 GiB for an array with shape (43472, 43472) and data type float64\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/proteins/PROTEINS-full.csv --embedding-path ../../result/embeddings_PROTEINS_AW_128.csv --dimensions 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f6d07",
   "metadata": {},
   "source": [
    "## Test with CL-100K-1d8-L9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674d6f6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------------------------------------+\n",
      "| Attention path |      ../../result/CL-100K-1d8-L9_attention.csv       |\n",
      "+================+======================================================+\n",
      "| Beta           | 0.500                                                |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Dimensions     | 128                                                  |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Edge path      | ../../data/CL-100K-1d8-L9.csv                        |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Embedding path | ../../result/CL-100K-1d8-L9_embeddings_attention.csv |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Epochs         | 200                                                  |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Gamma          | 0.500                                                |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Learning rate  | 0.010                                                |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Num of walks   | 80                                                   |\n",
      "+----------------+------------------------------------------------------+\n",
      "| Window size    | 5                                                    |\n",
      "+----------------+------------------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adjacency matrix powers:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:  40%|████      | 2/5 [00:00<00:00,  4.25it/s]\n",
      "Adjacency matrix powers:  60%|██████    | 3/5 [00:24<00:20, 10.05s/it]\n",
      "Adjacency matrix powers:  60%|██████    | 3/5 [01:46<01:11, 35.56s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\main.py\", line 19, in <module>\n",
      "    main()\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\main.py\", line 14, in main\n",
      "    model = AttentionWalkTrainer(args)\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\attentionwalk.py\", line 71, in __init__\n",
      "    self._initialize_model_and_data()\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\attentionwalk.py\", line 75, in _initialize_model_and_data\n",
      "    sparse_target_tensor = feature_calculator(self.args, self.graph)\n",
      "  File \"c:\\Users\\pietr\\OneDrive\\Desktop\\lfn_project\\src\\AttentionWalk\\src\\utils.py\", line 56, in feature_calculator\n",
      "    powered_A = powered_A @ normalized_adjacency_matrix\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 695, in __matmul__\n",
      "    return self._matmul_dispatch(other)\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 606, in _matmul_dispatch\n",
      "    return self._matmul_sparse(other)\n",
      "  File \"c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py\", line 532, in _matmul_sparse\n",
      "    data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype))\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 20.9 GiB for an array with shape (5621787628,) and data type float32\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/CL-100K-1d8-L9.csv --embedding-path ../../result/CL-100K-1d8-L9_embeddings_attention.csv --attention-path ../../result/CL-100K-1d8-L9_attention.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125957f4",
   "metadata": {},
   "source": [
    "## Test spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b5a6a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------------------------------+\n",
      "| Attention path |      ./output/chameleon_AW_attention.csv      |\n",
      "+================+===============================================+\n",
      "| Beta           | 0.500                                         |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Dimensions     | 128                                           |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Edge path      | ../../data/spam/spam.csv                      |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Embedding path | ../../result/embeddings_spam_AW_128_40_25.csv |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Epochs         | 200                                           |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Gamma          | 0.500                                         |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Learning rate  | 0.010                                         |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Num of walks   | 40                                            |\n",
      "+----------------+-----------------------------------------------+\n",
      "| Window size    | 16                                            |\n",
      "+----------------+-----------------------------------------------+\n",
      "\n",
      "Target matrix creation started.\n",
      "\n",
      "\n",
      "Training the model.\n",
      "\n",
      "\n",
      "Saving the model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_data.py:133: RuntimeWarning: divide by zero encountered in power\n",
      "  return self._with_data(data ** n)\n",
      "\n",
      "Adjacency matrix powers:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Adjacency matrix powers:   7%|▋         | 1/15 [00:01<00:27,  1.95s/it]\n",
      "Adjacency matrix powers:  13%|█▎        | 2/15 [00:22<02:44, 12.68s/it]\n",
      "Adjacency matrix powers:  20%|██        | 3/15 [00:45<03:31, 17.66s/it]\n",
      "Adjacency matrix powers:  27%|██▋       | 4/15 [01:09<03:40, 20.01s/it]\n",
      "Adjacency matrix powers:  33%|███▎      | 5/15 [01:33<03:35, 21.58s/it]\n",
      "Adjacency matrix powers:  40%|████      | 6/15 [02:01<03:34, 23.79s/it]\n",
      "Adjacency matrix powers:  47%|████▋     | 7/15 [02:26<03:12, 24.04s/it]\n",
      "Adjacency matrix powers:  53%|█████▎    | 8/15 [02:50<02:48, 24.10s/it]\n",
      "Adjacency matrix powers:  60%|██████    | 9/15 [03:16<02:27, 24.60s/it]\n",
      "Adjacency matrix powers:  67%|██████▋   | 10/15 [03:42<02:05, 25.14s/it]\n",
      "Adjacency matrix powers:  73%|███████▎  | 11/15 [04:11<01:44, 26.20s/it]\n",
      "Adjacency matrix powers:  80%|████████  | 12/15 [04:39<01:20, 26.80s/it]\n",
      "Adjacency matrix powers:  87%|████████▋ | 13/15 [05:09<00:55, 27.81s/it]\n",
      "Adjacency matrix powers:  93%|█████████▎| 14/15 [05:38<00:28, 28.02s/it]\n",
      "Adjacency matrix powers: 100%|██████████| 15/15 [06:05<00:00, 27.71s/it]\n",
      "Adjacency matrix powers: 100%|██████████| 15/15 [06:05<00:00, 24.33s/it]\n",
      "\n",
      "Loss:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Attention Walk (Loss=17.7343):   0%|          | 0/200 [00:12<?, ?it/s]\n",
      "Attention Walk (Loss=17.7343):   0%|          | 1/200 [00:12<40:31, 12.22s/it]\n",
      "Attention Walk (Loss=17.7319):   0%|          | 1/200 [00:22<40:31, 12.22s/it]\n",
      "Attention Walk (Loss=17.7319):   1%|          | 2/200 [00:22<37:08, 11.25s/it]\n",
      "Attention Walk (Loss=17.7272):   1%|          | 2/200 [00:30<37:08, 11.25s/it]\n",
      "Attention Walk (Loss=17.7272):   2%|▏         | 3/200 [00:30<31:34,  9.62s/it]\n",
      "Attention Walk (Loss=17.7008):   2%|▏         | 3/200 [00:37<31:34,  9.62s/it]\n",
      "Attention Walk (Loss=17.7008):   2%|▏         | 4/200 [00:37<28:35,  8.75s/it]\n",
      "Attention Walk (Loss=17.6262):   2%|▏         | 4/200 [00:49<28:35,  8.75s/it]\n",
      "Attention Walk (Loss=17.6262):   2%|▎         | 5/200 [00:49<31:17,  9.63s/it]\n",
      "Attention Walk (Loss=17.4804):   2%|▎         | 5/200 [00:56<31:17,  9.63s/it]\n",
      "Attention Walk (Loss=17.4804):   3%|▎         | 6/200 [00:56<28:26,  8.79s/it]\n",
      "Attention Walk (Loss=17.2444):   3%|▎         | 6/200 [01:02<28:26,  8.79s/it]\n",
      "Attention Walk (Loss=17.2444):   4%|▎         | 7/200 [01:02<25:38,  7.97s/it]\n",
      "Attention Walk (Loss=16.9062):   4%|▎         | 7/200 [01:13<25:38,  7.97s/it]\n",
      "Attention Walk (Loss=16.9062):   4%|▍         | 8/200 [01:13<28:46,  8.99s/it]\n",
      "Attention Walk (Loss=16.4582):   4%|▍         | 8/200 [01:18<28:46,  8.99s/it]\n",
      "Attention Walk (Loss=16.4582):   4%|▍         | 9/200 [01:18<24:32,  7.71s/it]\n",
      "Attention Walk (Loss=15.8969):   4%|▍         | 9/200 [01:23<24:32,  7.71s/it]\n",
      "Attention Walk (Loss=15.8969):   5%|▌         | 10/200 [01:23<21:28,  6.78s/it]\n",
      "Attention Walk (Loss=15.2236):   5%|▌         | 10/200 [01:27<21:28,  6.78s/it]\n",
      "Attention Walk (Loss=15.2236):   6%|▌         | 11/200 [01:27<19:05,  6.06s/it]\n",
      "Attention Walk (Loss=14.4442):   6%|▌         | 11/200 [01:31<19:05,  6.06s/it]\n",
      "Attention Walk (Loss=14.4442):   6%|▌         | 12/200 [01:31<17:00,  5.43s/it]\n",
      "Attention Walk (Loss=13.5703):   6%|▌         | 12/200 [01:35<17:00,  5.43s/it]\n",
      "Attention Walk (Loss=13.5703):   6%|▋         | 13/200 [01:35<15:34,  5.00s/it]\n",
      "Attention Walk (Loss=12.6184):   6%|▋         | 13/200 [01:42<15:34,  5.00s/it]\n",
      "Attention Walk (Loss=12.6184):   7%|▋         | 14/200 [01:42<16:49,  5.43s/it]\n",
      "Attention Walk (Loss=11.6109):   7%|▋         | 14/200 [01:47<16:49,  5.43s/it]\n",
      "Attention Walk (Loss=11.6109):   8%|▊         | 15/200 [01:47<16:43,  5.42s/it]\n",
      "Attention Walk (Loss=10.5748):   8%|▊         | 15/200 [01:51<16:43,  5.42s/it]\n",
      "Attention Walk (Loss=10.5748):   8%|▊         | 16/200 [01:51<15:37,  5.09s/it]\n",
      "Attention Walk (Loss=9.5413):   8%|▊         | 16/200 [01:56<15:37,  5.09s/it] \n",
      "Attention Walk (Loss=9.5413):   8%|▊         | 17/200 [01:56<15:11,  4.98s/it]\n",
      "Attention Walk (Loss=8.5433):   8%|▊         | 17/200 [02:04<15:11,  4.98s/it]\n",
      "Attention Walk (Loss=8.5433):   9%|▉         | 18/200 [02:04<17:51,  5.88s/it]\n",
      "Attention Walk (Loss=7.613):   9%|▉         | 18/200 [02:11<17:51,  5.88s/it] \n",
      "Attention Walk (Loss=7.613):  10%|▉         | 19/200 [02:11<19:07,  6.34s/it]\n",
      "Attention Walk (Loss=6.7785):  10%|▉         | 19/200 [02:23<19:07,  6.34s/it]\n",
      "Attention Walk (Loss=6.7785):  10%|█         | 20/200 [02:23<23:34,  7.86s/it]\n",
      "Attention Walk (Loss=6.0609):  10%|█         | 20/200 [02:32<23:34,  7.86s/it]\n",
      "Attention Walk (Loss=6.0609):  10%|█         | 21/200 [02:32<24:54,  8.35s/it]\n",
      "Attention Walk (Loss=5.4711):  10%|█         | 21/200 [02:42<24:54,  8.35s/it]\n",
      "Attention Walk (Loss=5.4711):  11%|█         | 22/200 [02:42<25:54,  8.73s/it]\n",
      "Attention Walk (Loss=5.0099):  11%|█         | 22/200 [02:50<25:54,  8.73s/it]\n",
      "Attention Walk (Loss=5.0099):  12%|█▏        | 23/200 [02:50<25:00,  8.47s/it]\n",
      "Attention Walk (Loss=4.6681):  12%|█▏        | 23/200 [03:01<25:00,  8.47s/it]\n",
      "Attention Walk (Loss=4.6681):  12%|█▏        | 24/200 [03:01<26:59,  9.20s/it]\n",
      "Attention Walk (Loss=4.4299):  12%|█▏        | 24/200 [03:08<26:59,  9.20s/it]\n",
      "Attention Walk (Loss=4.4299):  12%|█▎        | 25/200 [03:08<24:42,  8.47s/it]\n",
      "Attention Walk (Loss=4.2758):  12%|█▎        | 25/200 [03:18<24:42,  8.47s/it]\n",
      "Attention Walk (Loss=4.2758):  13%|█▎        | 26/200 [03:18<26:19,  9.08s/it]\n",
      "Attention Walk (Loss=4.1857):  13%|█▎        | 26/200 [03:26<26:19,  9.08s/it]\n",
      "Attention Walk (Loss=4.1857):  14%|█▎        | 27/200 [03:26<24:52,  8.63s/it]\n",
      "Attention Walk (Loss=4.141):  14%|█▎        | 27/200 [03:33<24:52,  8.63s/it] \n",
      "Attention Walk (Loss=4.141):  14%|█▍        | 28/200 [03:33<23:41,  8.26s/it]\n",
      "Attention Walk (Loss=4.1263):  14%|█▍        | 28/200 [03:40<23:41,  8.26s/it]\n",
      "Attention Walk (Loss=4.1263):  14%|█▍        | 29/200 [03:40<22:32,  7.91s/it]\n",
      "Attention Walk (Loss=4.1292):  14%|█▍        | 29/200 [03:47<22:32,  7.91s/it]\n",
      "Attention Walk (Loss=4.1292):  15%|█▌        | 30/200 [03:47<21:57,  7.75s/it]\n",
      "Attention Walk (Loss=4.1409):  15%|█▌        | 30/200 [03:55<21:57,  7.75s/it]\n",
      "Attention Walk (Loss=4.1409):  16%|█▌        | 31/200 [03:55<21:51,  7.76s/it]\n",
      "Attention Walk (Loss=4.1549):  16%|█▌        | 31/200 [04:03<21:51,  7.76s/it]\n",
      "Attention Walk (Loss=4.1549):  16%|█▌        | 32/200 [04:03<21:37,  7.72s/it]\n",
      "Attention Walk (Loss=4.1675):  16%|█▌        | 32/200 [04:09<21:37,  7.72s/it]\n",
      "Attention Walk (Loss=4.1675):  16%|█▋        | 33/200 [04:09<20:01,  7.20s/it]\n",
      "Attention Walk (Loss=4.1762):  16%|█▋        | 33/200 [04:15<20:01,  7.20s/it]\n",
      "Attention Walk (Loss=4.1762):  17%|█▋        | 34/200 [04:15<19:23,  7.01s/it]\n",
      "Attention Walk (Loss=4.18):  17%|█▋        | 34/200 [04:19<19:23,  7.01s/it]  \n",
      "Attention Walk (Loss=4.18):  18%|█▊        | 35/200 [04:19<16:47,  6.11s/it]\n",
      "Attention Walk (Loss=4.1785):  18%|█▊        | 35/200 [04:25<16:47,  6.11s/it]\n",
      "Attention Walk (Loss=4.1785):  18%|█▊        | 36/200 [04:25<15:58,  5.84s/it]\n",
      "Attention Walk (Loss=4.172):  18%|█▊        | 36/200 [04:30<15:58,  5.84s/it] \n",
      "Attention Walk (Loss=4.172):  18%|█▊        | 37/200 [04:30<15:05,  5.56s/it]\n",
      "Attention Walk (Loss=4.1609):  18%|█▊        | 37/200 [04:36<15:05,  5.56s/it]\n",
      "Attention Walk (Loss=4.1609):  19%|█▉        | 38/200 [04:36<15:28,  5.73s/it]\n",
      "Attention Walk (Loss=4.146):  19%|█▉        | 38/200 [04:42<15:28,  5.73s/it] \n",
      "Attention Walk (Loss=4.146):  20%|█▉        | 39/200 [04:42<15:33,  5.80s/it]\n",
      "Attention Walk (Loss=4.1282):  20%|█▉        | 39/200 [04:48<15:33,  5.80s/it]\n",
      "Attention Walk (Loss=4.1282):  20%|██        | 40/200 [04:48<15:31,  5.82s/it]\n",
      "Attention Walk (Loss=4.1082):  20%|██        | 40/200 [04:56<15:31,  5.82s/it]\n",
      "Attention Walk (Loss=4.1082):  20%|██        | 41/200 [04:56<17:26,  6.58s/it]\n",
      "Attention Walk (Loss=4.0871):  20%|██        | 41/200 [05:03<17:26,  6.58s/it]\n",
      "Attention Walk (Loss=4.0871):  21%|██        | 42/200 [05:03<17:50,  6.77s/it]\n",
      "Attention Walk (Loss=4.0654):  21%|██        | 42/200 [05:12<17:50,  6.77s/it]\n",
      "Attention Walk (Loss=4.0654):  22%|██▏       | 43/200 [05:12<19:40,  7.52s/it]\n",
      "Attention Walk (Loss=4.0439):  22%|██▏       | 43/200 [05:20<19:40,  7.52s/it]\n",
      "Attention Walk (Loss=4.0439):  22%|██▏       | 44/200 [05:20<19:20,  7.44s/it]\n",
      "Attention Walk (Loss=4.023):  22%|██▏       | 44/200 [05:28<19:20,  7.44s/it] \n",
      "Attention Walk (Loss=4.023):  22%|██▎       | 45/200 [05:28<19:57,  7.73s/it]\n",
      "Attention Walk (Loss=4.003):  22%|██▎       | 45/200 [05:35<19:57,  7.73s/it]\n",
      "Attention Walk (Loss=4.003):  23%|██▎       | 46/200 [05:35<19:33,  7.62s/it]\n",
      "Attention Walk (Loss=3.9844):  23%|██▎       | 46/200 [05:43<19:33,  7.62s/it]\n",
      "Attention Walk (Loss=3.9844):  24%|██▎       | 47/200 [05:43<19:24,  7.61s/it]\n",
      "Attention Walk (Loss=3.9672):  24%|██▎       | 47/200 [05:50<19:24,  7.61s/it]\n",
      "Attention Walk (Loss=3.9672):  24%|██▍       | 48/200 [05:50<18:46,  7.41s/it]\n",
      "Attention Walk (Loss=3.9516):  24%|██▍       | 48/200 [05:55<18:46,  7.41s/it]\n",
      "Attention Walk (Loss=3.9516):  24%|██▍       | 49/200 [05:55<16:38,  6.61s/it]\n",
      "Attention Walk (Loss=3.9377):  24%|██▍       | 49/200 [06:01<16:38,  6.61s/it]\n",
      "Attention Walk (Loss=3.9377):  25%|██▌       | 50/200 [06:01<16:11,  6.48s/it]\n",
      "Attention Walk (Loss=3.9252):  25%|██▌       | 50/200 [06:06<16:11,  6.48s/it]\n",
      "Attention Walk (Loss=3.9252):  26%|██▌       | 51/200 [06:06<15:27,  6.22s/it]\n",
      "Attention Walk (Loss=3.9143):  26%|██▌       | 51/200 [06:12<15:27,  6.22s/it]\n",
      "Attention Walk (Loss=3.9143):  26%|██▌       | 52/200 [06:12<15:03,  6.11s/it]\n",
      "Attention Walk (Loss=3.9046):  26%|██▌       | 52/200 [06:16<15:03,  6.11s/it]\n",
      "Attention Walk (Loss=3.9046):  26%|██▋       | 53/200 [06:16<13:26,  5.49s/it]\n",
      "Attention Walk (Loss=3.8962):  26%|██▋       | 53/200 [06:20<13:26,  5.49s/it]\n",
      "Attention Walk (Loss=3.8962):  27%|██▋       | 54/200 [06:20<12:21,  5.08s/it]\n",
      "Attention Walk (Loss=3.8889):  27%|██▋       | 54/200 [06:25<12:21,  5.08s/it]\n",
      "Attention Walk (Loss=3.8889):  28%|██▊       | 55/200 [06:25<11:49,  4.89s/it]\n",
      "Attention Walk (Loss=3.8824):  28%|██▊       | 55/200 [06:30<11:49,  4.89s/it]\n",
      "Attention Walk (Loss=3.8824):  28%|██▊       | 56/200 [06:30<11:57,  4.98s/it]\n",
      "Attention Walk (Loss=3.8766):  28%|██▊       | 56/200 [06:38<11:57,  4.98s/it]\n",
      "Attention Walk (Loss=3.8766):  28%|██▊       | 57/200 [06:38<13:38,  5.72s/it]\n",
      "Attention Walk (Loss=3.8715):  28%|██▊       | 57/200 [06:42<13:38,  5.72s/it]\n",
      "Attention Walk (Loss=3.8715):  29%|██▉       | 58/200 [06:42<12:34,  5.31s/it]\n",
      "Attention Walk (Loss=3.8669):  29%|██▉       | 58/200 [06:46<12:34,  5.31s/it]\n",
      "Attention Walk (Loss=3.8669):  30%|██▉       | 59/200 [06:46<11:31,  4.90s/it]\n",
      "Attention Walk (Loss=3.8627):  30%|██▉       | 59/200 [06:50<11:31,  4.90s/it]\n",
      "Attention Walk (Loss=3.8627):  30%|███       | 60/200 [06:50<10:46,  4.62s/it]\n",
      "Attention Walk (Loss=3.8588):  30%|███       | 60/200 [06:54<10:46,  4.62s/it]\n",
      "Attention Walk (Loss=3.8588):  30%|███       | 61/200 [06:54<10:15,  4.43s/it]\n",
      "Attention Walk (Loss=3.8552):  30%|███       | 61/200 [06:58<10:15,  4.43s/it]\n",
      "Attention Walk (Loss=3.8552):  31%|███       | 62/200 [06:58<09:57,  4.33s/it]\n",
      "Attention Walk (Loss=3.8518):  31%|███       | 62/200 [07:03<09:57,  4.33s/it]\n",
      "Attention Walk (Loss=3.8518):  32%|███▏      | 63/200 [07:03<10:17,  4.51s/it]\n",
      "Attention Walk (Loss=3.8486):  32%|███▏      | 63/200 [07:09<10:17,  4.51s/it]\n",
      "Attention Walk (Loss=3.8486):  32%|███▏      | 64/200 [07:09<11:14,  4.96s/it]\n",
      "Attention Walk (Loss=3.8456):  32%|███▏      | 64/200 [07:14<11:14,  4.96s/it]\n",
      "Attention Walk (Loss=3.8456):  32%|███▎      | 65/200 [07:14<11:35,  5.15s/it]\n",
      "Attention Walk (Loss=3.8427):  32%|███▎      | 65/200 [07:19<11:35,  5.15s/it]\n",
      "Attention Walk (Loss=3.8427):  33%|███▎      | 66/200 [07:19<10:46,  4.83s/it]\n",
      "Attention Walk (Loss=3.8401):  33%|███▎      | 66/200 [07:22<10:46,  4.83s/it]\n",
      "Attention Walk (Loss=3.8401):  34%|███▎      | 67/200 [07:22<10:08,  4.57s/it]\n",
      "Attention Walk (Loss=3.8376):  34%|███▎      | 67/200 [07:27<10:08,  4.57s/it]\n",
      "Attention Walk (Loss=3.8376):  34%|███▍      | 68/200 [07:27<09:44,  4.43s/it]\n",
      "Attention Walk (Loss=3.8354):  34%|███▍      | 68/200 [07:32<09:44,  4.43s/it]\n",
      "Attention Walk (Loss=3.8354):  34%|███▍      | 69/200 [07:32<10:24,  4.77s/it]\n",
      "Attention Walk (Loss=3.8333):  34%|███▍      | 69/200 [07:38<10:24,  4.77s/it]\n",
      "Attention Walk (Loss=3.8333):  35%|███▌      | 70/200 [07:38<11:11,  5.16s/it]\n",
      "Attention Walk (Loss=3.8314):  35%|███▌      | 70/200 [07:44<11:11,  5.16s/it]\n",
      "Attention Walk (Loss=3.8314):  36%|███▌      | 71/200 [07:44<11:48,  5.49s/it]\n",
      "Attention Walk (Loss=3.8297):  36%|███▌      | 71/200 [07:51<11:48,  5.49s/it]\n",
      "Attention Walk (Loss=3.8297):  36%|███▌      | 72/200 [07:51<12:32,  5.88s/it]\n",
      "Attention Walk (Loss=3.8282):  36%|███▌      | 72/200 [07:56<12:32,  5.88s/it]\n",
      "Attention Walk (Loss=3.8282):  36%|███▋      | 73/200 [07:56<11:42,  5.53s/it]\n",
      "Attention Walk (Loss=3.8268):  36%|███▋      | 73/200 [08:02<11:42,  5.53s/it]\n",
      "Attention Walk (Loss=3.8268):  37%|███▋      | 74/200 [08:02<11:56,  5.69s/it]\n",
      "Attention Walk (Loss=3.8256):  37%|███▋      | 74/200 [08:06<11:56,  5.69s/it]\n",
      "Attention Walk (Loss=3.8256):  38%|███▊      | 75/200 [08:06<10:53,  5.22s/it]\n",
      "Attention Walk (Loss=3.8245):  38%|███▊      | 75/200 [08:15<10:53,  5.22s/it]\n",
      "Attention Walk (Loss=3.8245):  38%|███▊      | 76/200 [08:15<12:44,  6.16s/it]\n",
      "Attention Walk (Loss=3.8236):  38%|███▊      | 76/200 [08:22<12:44,  6.16s/it]\n",
      "Attention Walk (Loss=3.8236):  38%|███▊      | 77/200 [08:22<13:17,  6.48s/it]\n",
      "Attention Walk (Loss=3.8227):  38%|███▊      | 77/200 [08:26<13:17,  6.48s/it]\n",
      "Attention Walk (Loss=3.8227):  39%|███▉      | 78/200 [08:26<11:44,  5.77s/it]\n",
      "Attention Walk (Loss=3.8218):  39%|███▉      | 78/200 [08:33<11:44,  5.77s/it]\n",
      "Attention Walk (Loss=3.8218):  40%|███▉      | 79/200 [08:33<12:25,  6.16s/it]\n",
      "Attention Walk (Loss=3.8211):  40%|███▉      | 79/200 [08:37<12:25,  6.16s/it]\n",
      "Attention Walk (Loss=3.8211):  40%|████      | 80/200 [08:37<11:04,  5.54s/it]\n",
      "Attention Walk (Loss=3.8204):  40%|████      | 80/200 [08:43<11:04,  5.54s/it]\n",
      "Attention Walk (Loss=3.8204):  40%|████      | 81/200 [08:43<11:21,  5.73s/it]\n",
      "Attention Walk (Loss=3.8197):  40%|████      | 81/200 [08:50<11:21,  5.73s/it]\n",
      "Attention Walk (Loss=3.8197):  41%|████      | 82/200 [08:50<11:48,  6.00s/it]\n",
      "Attention Walk (Loss=3.8191):  41%|████      | 82/200 [08:57<11:48,  6.00s/it]\n",
      "Attention Walk (Loss=3.8191):  42%|████▏     | 83/200 [08:57<12:28,  6.39s/it]\n",
      "Attention Walk (Loss=3.8185):  42%|████▏     | 83/200 [09:05<12:28,  6.39s/it]\n",
      "Attention Walk (Loss=3.8185):  42%|████▏     | 84/200 [09:05<13:03,  6.76s/it]\n",
      "Attention Walk (Loss=3.818):  42%|████▏     | 84/200 [09:12<13:03,  6.76s/it] \n",
      "Attention Walk (Loss=3.818):  42%|████▎     | 85/200 [09:12<13:07,  6.85s/it]\n",
      "Attention Walk (Loss=3.8175):  42%|████▎     | 85/200 [09:17<13:07,  6.85s/it]\n",
      "Attention Walk (Loss=3.8175):  43%|████▎     | 86/200 [09:17<11:48,  6.22s/it]\n",
      "Attention Walk (Loss=3.817):  43%|████▎     | 86/200 [09:25<11:48,  6.22s/it] \n",
      "Attention Walk (Loss=3.817):  44%|████▎     | 87/200 [09:25<12:56,  6.87s/it]\n",
      "Attention Walk (Loss=3.8165):  44%|████▎     | 87/200 [09:32<12:56,  6.87s/it]\n",
      "Attention Walk (Loss=3.8165):  44%|████▍     | 88/200 [09:32<12:42,  6.81s/it]\n",
      "Attention Walk (Loss=3.8161):  44%|████▍     | 88/200 [09:36<12:42,  6.81s/it]\n",
      "Attention Walk (Loss=3.8161):  44%|████▍     | 89/200 [09:36<11:27,  6.19s/it]\n",
      "Attention Walk (Loss=3.8157):  44%|████▍     | 89/200 [09:44<11:27,  6.19s/it]\n",
      "Attention Walk (Loss=3.8157):  45%|████▌     | 90/200 [09:44<12:13,  6.67s/it]\n",
      "Attention Walk (Loss=3.8153):  45%|████▌     | 90/200 [09:50<12:13,  6.67s/it]\n",
      "Attention Walk (Loss=3.8153):  46%|████▌     | 91/200 [09:50<11:55,  6.57s/it]\n",
      "Attention Walk (Loss=3.8149):  46%|████▌     | 91/200 [09:58<11:55,  6.57s/it]\n",
      "Attention Walk (Loss=3.8149):  46%|████▌     | 92/200 [09:58<12:28,  6.93s/it]\n",
      "Attention Walk (Loss=3.8146):  46%|████▌     | 92/200 [10:05<12:28,  6.93s/it]\n",
      "Attention Walk (Loss=3.8146):  46%|████▋     | 93/200 [10:05<12:03,  6.76s/it]\n",
      "Attention Walk (Loss=3.8142):  46%|████▋     | 93/200 [10:09<12:03,  6.76s/it]\n",
      "Attention Walk (Loss=3.8142):  47%|████▋     | 94/200 [10:09<10:33,  5.98s/it]\n",
      "Attention Walk (Loss=3.8139):  47%|████▋     | 94/200 [10:13<10:33,  5.98s/it]\n",
      "Attention Walk (Loss=3.8139):  48%|████▊     | 95/200 [10:13<09:26,  5.39s/it]\n",
      "Attention Walk (Loss=3.8136):  48%|████▊     | 95/200 [10:18<09:26,  5.39s/it]\n",
      "Attention Walk (Loss=3.8136):  48%|████▊     | 96/200 [10:18<09:29,  5.47s/it]\n",
      "Attention Walk (Loss=3.8133):  48%|████▊     | 96/200 [10:24<09:29,  5.47s/it]\n",
      "Attention Walk (Loss=3.8133):  48%|████▊     | 97/200 [10:24<09:36,  5.59s/it]\n",
      "Attention Walk (Loss=3.813):  48%|████▊     | 97/200 [10:31<09:36,  5.59s/it] \n",
      "Attention Walk (Loss=3.813):  49%|████▉     | 98/200 [10:31<10:08,  5.96s/it]\n",
      "Attention Walk (Loss=3.8127):  49%|████▉     | 98/200 [10:35<10:08,  5.96s/it]\n",
      "Attention Walk (Loss=3.8127):  50%|████▉     | 99/200 [10:35<09:00,  5.35s/it]\n",
      "Attention Walk (Loss=3.8124):  50%|████▉     | 99/200 [10:41<09:00,  5.35s/it]\n",
      "Attention Walk (Loss=3.8124):  50%|█████     | 100/200 [10:41<09:16,  5.56s/it]\n",
      "Attention Walk (Loss=3.8121):  50%|█████     | 100/200 [10:48<09:16,  5.56s/it]\n",
      "Attention Walk (Loss=3.8121):  50%|█████     | 101/200 [10:48<09:39,  5.86s/it]\n",
      "Attention Walk (Loss=3.8119):  50%|█████     | 101/200 [10:53<09:39,  5.86s/it]\n",
      "Attention Walk (Loss=3.8119):  51%|█████     | 102/200 [10:53<09:21,  5.73s/it]\n",
      "Attention Walk (Loss=3.8116):  51%|█████     | 102/200 [10:57<09:21,  5.73s/it]\n",
      "Attention Walk (Loss=3.8116):  52%|█████▏    | 103/200 [10:57<08:18,  5.14s/it]\n",
      "Attention Walk (Loss=3.8114):  52%|█████▏    | 103/200 [11:03<08:18,  5.14s/it]\n",
      "Attention Walk (Loss=3.8114):  52%|█████▏    | 104/200 [11:03<08:36,  5.38s/it]\n",
      "Attention Walk (Loss=3.8111):  52%|█████▏    | 104/200 [11:09<08:36,  5.38s/it]\n",
      "Attention Walk (Loss=3.8111):  52%|█████▎    | 105/200 [11:09<08:43,  5.51s/it]\n",
      "Attention Walk (Loss=3.8109):  52%|█████▎    | 105/200 [11:13<08:43,  5.51s/it]\n",
      "Attention Walk (Loss=3.8109):  53%|█████▎    | 106/200 [11:13<07:57,  5.08s/it]\n",
      "Attention Walk (Loss=3.8107):  53%|█████▎    | 106/200 [11:18<07:57,  5.08s/it]\n",
      "Attention Walk (Loss=3.8107):  54%|█████▎    | 107/200 [11:18<08:07,  5.25s/it]\n",
      "Attention Walk (Loss=3.8104):  54%|█████▎    | 107/200 [11:24<08:07,  5.25s/it]\n",
      "Attention Walk (Loss=3.8104):  54%|█████▍    | 108/200 [11:24<08:17,  5.40s/it]\n",
      "Attention Walk (Loss=3.8102):  54%|█████▍    | 108/200 [11:28<08:17,  5.40s/it]\n",
      "Attention Walk (Loss=3.8102):  55%|█████▍    | 109/200 [11:28<07:27,  4.92s/it]\n",
      "Attention Walk (Loss=3.81):  55%|█████▍    | 109/200 [11:34<07:27,  4.92s/it]  \n",
      "Attention Walk (Loss=3.81):  55%|█████▌    | 110/200 [11:34<07:50,  5.23s/it]\n",
      "Attention Walk (Loss=3.8097):  55%|█████▌    | 110/200 [11:39<07:50,  5.23s/it]\n",
      "Attention Walk (Loss=3.8097):  56%|█████▌    | 111/200 [11:39<07:55,  5.34s/it]\n",
      "Attention Walk (Loss=3.8095):  56%|█████▌    | 111/200 [11:45<07:55,  5.34s/it]\n",
      "Attention Walk (Loss=3.8095):  56%|█████▌    | 112/200 [11:45<08:01,  5.47s/it]\n",
      "Attention Walk (Loss=3.8093):  56%|█████▌    | 112/200 [11:51<08:01,  5.47s/it]\n",
      "Attention Walk (Loss=3.8093):  56%|█████▋    | 113/200 [11:51<08:12,  5.66s/it]\n",
      "Attention Walk (Loss=3.8091):  56%|█████▋    | 113/200 [11:57<08:12,  5.66s/it]\n",
      "Attention Walk (Loss=3.8091):  57%|█████▋    | 114/200 [11:57<08:06,  5.66s/it]\n",
      "Attention Walk (Loss=3.8088):  57%|█████▋    | 114/200 [12:01<08:06,  5.66s/it]\n",
      "Attention Walk (Loss=3.8088):  57%|█████▊    | 115/200 [12:01<07:20,  5.18s/it]\n",
      "Attention Walk (Loss=3.8086):  57%|█████▊    | 115/200 [12:06<07:20,  5.18s/it]\n",
      "Attention Walk (Loss=3.8086):  58%|█████▊    | 116/200 [12:06<06:56,  4.96s/it]\n",
      "Attention Walk (Loss=3.8084):  58%|█████▊    | 116/200 [12:10<06:56,  4.96s/it]\n",
      "Attention Walk (Loss=3.8084):  58%|█████▊    | 117/200 [12:10<06:35,  4.76s/it]\n",
      "Attention Walk (Loss=3.8082):  58%|█████▊    | 117/200 [12:14<06:35,  4.76s/it]\n",
      "Attention Walk (Loss=3.8082):  59%|█████▉    | 118/200 [12:14<06:06,  4.46s/it]\n",
      "Attention Walk (Loss=3.808):  59%|█████▉    | 118/200 [12:17<06:06,  4.46s/it] \n",
      "Attention Walk (Loss=3.808):  60%|█████▉    | 119/200 [12:17<05:45,  4.27s/it]\n",
      "Attention Walk (Loss=3.8077):  60%|█████▉    | 119/200 [12:22<05:45,  4.27s/it]\n",
      "Attention Walk (Loss=3.8077):  60%|██████    | 120/200 [12:22<05:37,  4.22s/it]\n",
      "Attention Walk (Loss=3.8075):  60%|██████    | 120/200 [12:26<05:37,  4.22s/it]\n",
      "Attention Walk (Loss=3.8075):  60%|██████    | 121/200 [12:26<05:29,  4.17s/it]\n",
      "Attention Walk (Loss=3.8073):  60%|██████    | 121/200 [12:31<05:29,  4.17s/it]\n",
      "Attention Walk (Loss=3.8073):  61%|██████    | 122/200 [12:31<06:06,  4.69s/it]\n",
      "Attention Walk (Loss=3.8071):  61%|██████    | 122/200 [12:35<06:06,  4.69s/it]\n",
      "Attention Walk (Loss=3.8071):  62%|██████▏   | 123/200 [12:35<05:39,  4.41s/it]\n",
      "Attention Walk (Loss=3.8068):  62%|██████▏   | 123/200 [12:40<05:39,  4.41s/it]\n",
      "Attention Walk (Loss=3.8068):  62%|██████▏   | 124/200 [12:40<05:51,  4.63s/it]\n",
      "Attention Walk (Loss=3.8066):  62%|██████▏   | 124/200 [12:46<05:51,  4.63s/it]\n",
      "Attention Walk (Loss=3.8066):  62%|██████▎   | 125/200 [12:46<06:04,  4.86s/it]\n",
      "Attention Walk (Loss=3.8064):  62%|██████▎   | 125/200 [12:52<06:04,  4.86s/it]\n",
      "Attention Walk (Loss=3.8064):  63%|██████▎   | 126/200 [12:52<06:29,  5.26s/it]\n",
      "Attention Walk (Loss=3.8062):  63%|██████▎   | 126/200 [12:58<06:29,  5.26s/it]\n",
      "Attention Walk (Loss=3.8062):  64%|██████▎   | 127/200 [12:58<06:40,  5.49s/it]\n",
      "Attention Walk (Loss=3.8059):  64%|██████▎   | 127/200 [13:05<06:40,  5.49s/it]\n",
      "Attention Walk (Loss=3.8059):  64%|██████▍   | 128/200 [13:05<07:05,  5.91s/it]\n",
      "Attention Walk (Loss=3.8057):  64%|██████▍   | 128/200 [13:11<07:05,  5.91s/it]\n",
      "Attention Walk (Loss=3.8057):  64%|██████▍   | 129/200 [13:11<07:02,  5.95s/it]\n",
      "Attention Walk (Loss=3.8055):  64%|██████▍   | 129/200 [13:17<07:02,  5.95s/it]\n",
      "Attention Walk (Loss=3.8055):  65%|██████▌   | 130/200 [13:17<06:59,  5.99s/it]\n",
      "Attention Walk (Loss=3.8053):  65%|██████▌   | 130/200 [13:23<06:59,  5.99s/it]\n",
      "Attention Walk (Loss=3.8053):  66%|██████▌   | 131/200 [13:23<06:59,  6.08s/it]\n",
      "Attention Walk (Loss=3.805):  66%|██████▌   | 131/200 [13:28<06:59,  6.08s/it] \n",
      "Attention Walk (Loss=3.805):  66%|██████▌   | 132/200 [13:28<06:28,  5.71s/it]\n",
      "Attention Walk (Loss=3.8048):  66%|██████▌   | 132/200 [13:34<06:28,  5.71s/it]\n",
      "Attention Walk (Loss=3.8048):  66%|██████▋   | 133/200 [13:34<06:18,  5.65s/it]\n",
      "Attention Walk (Loss=3.8046):  66%|██████▋   | 133/200 [13:37<06:18,  5.65s/it]\n",
      "Attention Walk (Loss=3.8046):  67%|██████▋   | 134/200 [13:37<05:37,  5.12s/it]\n",
      "Attention Walk (Loss=3.8043):  67%|██████▋   | 134/200 [13:41<05:37,  5.12s/it]\n",
      "Attention Walk (Loss=3.8043):  68%|██████▊   | 135/200 [13:41<05:10,  4.78s/it]\n",
      "Attention Walk (Loss=3.8041):  68%|██████▊   | 135/200 [13:48<05:10,  4.78s/it]\n",
      "Attention Walk (Loss=3.8041):  68%|██████▊   | 136/200 [13:48<05:29,  5.15s/it]\n",
      "Attention Walk (Loss=3.8039):  68%|██████▊   | 136/200 [13:52<05:29,  5.15s/it]\n",
      "Attention Walk (Loss=3.8039):  68%|██████▊   | 137/200 [13:52<05:14,  4.99s/it]\n",
      "Attention Walk (Loss=3.8036):  68%|██████▊   | 137/200 [13:59<05:14,  4.99s/it]\n",
      "Attention Walk (Loss=3.8036):  69%|██████▉   | 138/200 [13:59<05:40,  5.50s/it]\n",
      "Attention Walk (Loss=3.8034):  69%|██████▉   | 138/200 [14:07<05:40,  5.50s/it]\n",
      "Attention Walk (Loss=3.8034):  70%|██████▉   | 139/200 [14:07<06:16,  6.18s/it]\n",
      "Attention Walk (Loss=3.8032):  70%|██████▉   | 139/200 [14:11<06:16,  6.18s/it]\n",
      "Attention Walk (Loss=3.8032):  70%|███████   | 140/200 [14:11<05:35,  5.60s/it]\n",
      "Attention Walk (Loss=3.8029):  70%|███████   | 140/200 [14:15<05:35,  5.60s/it]\n",
      "Attention Walk (Loss=3.8029):  70%|███████   | 141/200 [14:15<05:00,  5.09s/it]\n",
      "Attention Walk (Loss=3.8027):  70%|███████   | 141/200 [14:19<05:00,  5.09s/it]\n",
      "Attention Walk (Loss=3.8027):  71%|███████   | 142/200 [14:19<04:34,  4.73s/it]\n",
      "Attention Walk (Loss=3.8024):  71%|███████   | 142/200 [14:24<04:34,  4.73s/it]\n",
      "Attention Walk (Loss=3.8024):  72%|███████▏  | 143/200 [14:24<04:48,  5.05s/it]\n",
      "Attention Walk (Loss=3.8022):  72%|███████▏  | 143/200 [14:28<04:48,  5.05s/it]\n",
      "Attention Walk (Loss=3.8022):  72%|███████▏  | 144/200 [14:28<04:23,  4.71s/it]\n",
      "Attention Walk (Loss=3.8019):  72%|███████▏  | 144/200 [14:33<04:23,  4.71s/it]\n",
      "Attention Walk (Loss=3.8019):  72%|███████▎  | 145/200 [14:33<04:11,  4.57s/it]\n",
      "Attention Walk (Loss=3.8017):  72%|███████▎  | 145/200 [14:38<04:11,  4.57s/it]\n",
      "Attention Walk (Loss=3.8017):  73%|███████▎  | 146/200 [14:38<04:15,  4.72s/it]\n",
      "Attention Walk (Loss=3.8014):  73%|███████▎  | 146/200 [14:42<04:15,  4.72s/it]\n",
      "Attention Walk (Loss=3.8014):  74%|███████▎  | 147/200 [14:42<04:11,  4.74s/it]\n",
      "Attention Walk (Loss=3.8012):  74%|███████▎  | 147/200 [14:46<04:11,  4.74s/it]\n",
      "Attention Walk (Loss=3.8012):  74%|███████▍  | 148/200 [14:46<03:53,  4.48s/it]\n",
      "Attention Walk (Loss=3.8009):  74%|███████▍  | 148/200 [14:52<03:53,  4.48s/it]\n",
      "Attention Walk (Loss=3.8009):  74%|███████▍  | 149/200 [14:52<04:08,  4.88s/it]\n",
      "Attention Walk (Loss=3.8007):  74%|███████▍  | 149/200 [14:58<04:08,  4.88s/it]\n",
      "Attention Walk (Loss=3.8007):  75%|███████▌  | 150/200 [14:58<04:26,  5.33s/it]\n",
      "Attention Walk (Loss=3.8004):  75%|███████▌  | 150/200 [15:03<04:26,  5.33s/it]\n",
      "Attention Walk (Loss=3.8004):  76%|███████▌  | 151/200 [15:03<04:05,  5.01s/it]\n",
      "Attention Walk (Loss=3.8002):  76%|███████▌  | 151/200 [15:07<04:05,  5.01s/it]\n",
      "Attention Walk (Loss=3.8002):  76%|███████▌  | 152/200 [15:07<03:47,  4.73s/it]\n",
      "Attention Walk (Loss=3.7999):  76%|███████▌  | 152/200 [15:11<03:47,  4.73s/it]\n",
      "Attention Walk (Loss=3.7999):  76%|███████▋  | 153/200 [15:11<03:31,  4.50s/it]\n",
      "Attention Walk (Loss=3.7997):  76%|███████▋  | 153/200 [15:17<03:31,  4.50s/it]\n",
      "Attention Walk (Loss=3.7997):  77%|███████▋  | 154/200 [15:17<03:50,  5.02s/it]\n",
      "Attention Walk (Loss=3.7994):  77%|███████▋  | 154/200 [15:21<03:50,  5.02s/it]\n",
      "Attention Walk (Loss=3.7994):  78%|███████▊  | 155/200 [15:21<03:32,  4.73s/it]\n",
      "Attention Walk (Loss=3.7991):  78%|███████▊  | 155/200 [15:25<03:32,  4.73s/it]\n",
      "Attention Walk (Loss=3.7991):  78%|███████▊  | 156/200 [15:25<03:19,  4.54s/it]\n",
      "Attention Walk (Loss=3.7989):  78%|███████▊  | 156/200 [15:30<03:19,  4.54s/it]\n",
      "Attention Walk (Loss=3.7989):  78%|███████▊  | 157/200 [15:30<03:16,  4.57s/it]\n",
      "Attention Walk (Loss=3.7986):  78%|███████▊  | 157/200 [15:34<03:16,  4.57s/it]\n",
      "Attention Walk (Loss=3.7986):  79%|███████▉  | 158/200 [15:34<03:07,  4.46s/it]\n",
      "Attention Walk (Loss=3.7983):  79%|███████▉  | 158/200 [15:40<03:07,  4.46s/it]\n",
      "Attention Walk (Loss=3.7983):  80%|███████▉  | 159/200 [15:40<03:22,  4.93s/it]\n",
      "Attention Walk (Loss=3.7981):  80%|███████▉  | 159/200 [15:47<03:22,  4.93s/it]\n",
      "Attention Walk (Loss=3.7981):  80%|████████  | 160/200 [15:47<03:44,  5.60s/it]\n",
      "Attention Walk (Loss=3.7978):  80%|████████  | 160/200 [15:51<03:44,  5.60s/it]\n",
      "Attention Walk (Loss=3.7978):  80%|████████  | 161/200 [15:51<03:23,  5.21s/it]\n",
      "Attention Walk (Loss=3.7975):  80%|████████  | 161/200 [15:56<03:23,  5.21s/it]\n",
      "Attention Walk (Loss=3.7975):  81%|████████  | 162/200 [15:56<03:05,  4.87s/it]\n",
      "Attention Walk (Loss=3.7973):  81%|████████  | 162/200 [16:00<03:05,  4.87s/it]\n",
      "Attention Walk (Loss=3.7973):  82%|████████▏ | 163/200 [16:00<02:51,  4.63s/it]\n",
      "Attention Walk (Loss=3.797):  82%|████████▏ | 163/200 [16:06<02:51,  4.63s/it] \n",
      "Attention Walk (Loss=3.797):  82%|████████▏ | 164/200 [16:06<03:02,  5.07s/it]\n",
      "Attention Walk (Loss=3.7967):  82%|████████▏ | 164/200 [16:10<03:02,  5.07s/it]\n",
      "Attention Walk (Loss=3.7967):  82%|████████▎ | 165/200 [16:10<02:46,  4.75s/it]\n",
      "Attention Walk (Loss=3.7964):  82%|████████▎ | 165/200 [16:14<02:46,  4.75s/it]\n",
      "Attention Walk (Loss=3.7964):  83%|████████▎ | 166/200 [16:14<02:31,  4.47s/it]\n",
      "Attention Walk (Loss=3.7961):  83%|████████▎ | 166/200 [16:18<02:31,  4.47s/it]\n",
      "Attention Walk (Loss=3.7961):  84%|████████▎ | 167/200 [16:18<02:22,  4.32s/it]\n",
      "Attention Walk (Loss=3.7959):  84%|████████▎ | 167/200 [16:22<02:22,  4.32s/it]\n",
      "Attention Walk (Loss=3.7959):  84%|████████▍ | 168/200 [16:22<02:15,  4.24s/it]\n",
      "Attention Walk (Loss=3.7956):  84%|████████▍ | 168/200 [16:26<02:15,  4.24s/it]\n",
      "Attention Walk (Loss=3.7956):  84%|████████▍ | 169/200 [16:26<02:14,  4.34s/it]\n",
      "Attention Walk (Loss=3.7953):  84%|████████▍ | 169/200 [16:34<02:14,  4.34s/it]\n",
      "Attention Walk (Loss=3.7953):  85%|████████▌ | 170/200 [16:34<02:41,  5.38s/it]\n",
      "Attention Walk (Loss=3.795):  85%|████████▌ | 170/200 [16:40<02:41,  5.38s/it] \n",
      "Attention Walk (Loss=3.795):  86%|████████▌ | 171/200 [16:40<02:44,  5.67s/it]\n",
      "Attention Walk (Loss=3.7947):  86%|████████▌ | 171/200 [16:47<02:44,  5.67s/it]\n",
      "Attention Walk (Loss=3.7947):  86%|████████▌ | 172/200 [16:47<02:45,  5.90s/it]\n",
      "Attention Walk (Loss=3.7944):  86%|████████▌ | 172/200 [16:51<02:45,  5.90s/it]\n",
      "Attention Walk (Loss=3.7944):  86%|████████▋ | 173/200 [16:51<02:23,  5.31s/it]\n",
      "Attention Walk (Loss=3.7942):  86%|████████▋ | 173/200 [16:55<02:23,  5.31s/it]\n",
      "Attention Walk (Loss=3.7942):  87%|████████▋ | 174/200 [16:55<02:08,  4.93s/it]\n",
      "Attention Walk (Loss=3.7939):  87%|████████▋ | 174/200 [17:02<02:08,  4.93s/it]\n",
      "Attention Walk (Loss=3.7939):  88%|████████▊ | 175/200 [17:02<02:19,  5.58s/it]\n",
      "Attention Walk (Loss=3.7936):  88%|████████▊ | 175/200 [17:06<02:19,  5.58s/it]\n",
      "Attention Walk (Loss=3.7936):  88%|████████▊ | 176/200 [17:06<02:03,  5.13s/it]\n",
      "Attention Walk (Loss=3.7933):  88%|████████▊ | 176/200 [17:10<02:03,  5.13s/it]\n",
      "Attention Walk (Loss=3.7933):  88%|████████▊ | 177/200 [17:10<01:52,  4.88s/it]\n",
      "Attention Walk (Loss=3.793):  88%|████████▊ | 177/200 [17:14<01:52,  4.88s/it] \n",
      "Attention Walk (Loss=3.793):  89%|████████▉ | 178/200 [17:14<01:40,  4.55s/it]\n",
      "Attention Walk (Loss=3.7927):  89%|████████▉ | 178/200 [17:18<01:40,  4.55s/it]\n",
      "Attention Walk (Loss=3.7927):  90%|████████▉ | 179/200 [17:18<01:30,  4.33s/it]\n",
      "Attention Walk (Loss=3.7924):  90%|████████▉ | 179/200 [17:22<01:30,  4.33s/it]\n",
      "Attention Walk (Loss=3.7924):  90%|█████████ | 180/200 [17:22<01:22,  4.15s/it]\n",
      "Attention Walk (Loss=3.7921):  90%|█████████ | 180/200 [17:25<01:22,  4.15s/it]\n",
      "Attention Walk (Loss=3.7921):  90%|█████████ | 181/200 [17:25<01:17,  4.07s/it]\n",
      "Attention Walk (Loss=3.7919):  90%|█████████ | 181/200 [17:31<01:17,  4.07s/it]\n",
      "Attention Walk (Loss=3.7919):  91%|█████████ | 182/200 [17:31<01:23,  4.63s/it]\n",
      "Attention Walk (Loss=3.7916):  91%|█████████ | 182/200 [17:35<01:23,  4.63s/it]\n",
      "Attention Walk (Loss=3.7916):  92%|█████████▏| 183/200 [17:35<01:14,  4.40s/it]\n",
      "Attention Walk (Loss=3.7913):  92%|█████████▏| 183/200 [17:39<01:14,  4.40s/it]\n",
      "Attention Walk (Loss=3.7913):  92%|█████████▏| 184/200 [17:39<01:07,  4.21s/it]\n",
      "Attention Walk (Loss=3.791):  92%|█████████▏| 184/200 [17:45<01:07,  4.21s/it] \n",
      "Attention Walk (Loss=3.791):  92%|█████████▎| 185/200 [17:45<01:11,  4.76s/it]\n",
      "Attention Walk (Loss=3.7907):  92%|█████████▎| 185/200 [17:49<01:11,  4.76s/it]\n",
      "Attention Walk (Loss=3.7907):  93%|█████████▎| 186/200 [17:49<01:02,  4.49s/it]\n",
      "Attention Walk (Loss=3.7904):  93%|█████████▎| 186/200 [17:53<01:02,  4.49s/it]\n",
      "Attention Walk (Loss=3.7904):  94%|█████████▎| 187/200 [17:53<00:58,  4.47s/it]\n",
      "Attention Walk (Loss=3.7901):  94%|█████████▎| 187/200 [18:00<00:58,  4.47s/it]\n",
      "Attention Walk (Loss=3.7901):  94%|█████████▍| 188/200 [18:00<01:03,  5.25s/it]\n",
      "Attention Walk (Loss=3.7898):  94%|█████████▍| 188/200 [18:08<01:03,  5.25s/it]\n",
      "Attention Walk (Loss=3.7898):  94%|█████████▍| 189/200 [18:08<01:04,  5.87s/it]\n",
      "Attention Walk (Loss=3.7896):  94%|█████████▍| 189/200 [18:16<01:04,  5.87s/it]\n",
      "Attention Walk (Loss=3.7896):  95%|█████████▌| 190/200 [18:16<01:04,  6.46s/it]\n",
      "Attention Walk (Loss=3.7893):  95%|█████████▌| 190/200 [18:21<01:04,  6.46s/it]\n",
      "Attention Walk (Loss=3.7893):  96%|█████████▌| 191/200 [18:21<00:56,  6.26s/it]\n",
      "Attention Walk (Loss=3.789):  96%|█████████▌| 191/200 [18:27<00:56,  6.26s/it] \n",
      "Attention Walk (Loss=3.789):  96%|█████████▌| 192/200 [18:27<00:49,  6.14s/it]\n",
      "Attention Walk (Loss=3.7887):  96%|█████████▌| 192/200 [18:31<00:49,  6.14s/it]\n",
      "Attention Walk (Loss=3.7887):  96%|█████████▋| 193/200 [18:31<00:38,  5.51s/it]\n",
      "Attention Walk (Loss=3.7884):  96%|█████████▋| 193/200 [18:35<00:38,  5.51s/it]\n",
      "Attention Walk (Loss=3.7884):  97%|█████████▋| 194/200 [18:35<00:29,  4.98s/it]\n",
      "Attention Walk (Loss=3.7882):  97%|█████████▋| 194/200 [18:39<00:29,  4.98s/it]\n",
      "Attention Walk (Loss=3.7882):  98%|█████████▊| 195/200 [18:39<00:23,  4.63s/it]\n",
      "Attention Walk (Loss=3.7879):  98%|█████████▊| 195/200 [18:43<00:23,  4.63s/it]\n",
      "Attention Walk (Loss=3.7879):  98%|█████████▊| 196/200 [18:43<00:17,  4.43s/it]\n",
      "Attention Walk (Loss=3.7876):  98%|█████████▊| 196/200 [18:50<00:17,  4.43s/it]\n",
      "Attention Walk (Loss=3.7876):  98%|█████████▊| 197/200 [18:50<00:16,  5.35s/it]\n",
      "Attention Walk (Loss=3.7873):  98%|█████████▊| 197/200 [18:55<00:16,  5.35s/it]\n",
      "Attention Walk (Loss=3.7873):  99%|█████████▉| 198/200 [18:55<00:10,  5.09s/it]\n",
      "Attention Walk (Loss=3.787):  99%|█████████▉| 198/200 [18:59<00:10,  5.09s/it] \n",
      "Attention Walk (Loss=3.787): 100%|█████████▉| 199/200 [18:59<00:04,  4.72s/it]\n",
      "Attention Walk (Loss=3.7868): 100%|█████████▉| 199/200 [19:02<00:04,  4.72s/it]\n",
      "Attention Walk (Loss=3.7868): 100%|██████████| 200/200 [19:02<00:00,  4.46s/it]\n",
      "Attention Walk (Loss=3.7868): 100%|██████████| 200/200 [19:02<00:00,  5.71s/it]\n"
     ]
    }
   ],
   "source": [
    "!cd AttentionWalk && python src/main.py --edge-path ../../data/spam/spam.csv --embedding-path ../../result/embeddings_spam_AW_128_40_25.csv --window-size 16 --num-of-walks 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a558d7",
   "metadata": {},
   "source": [
    "# GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd gae && python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95228f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd gae/gae && python train.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
