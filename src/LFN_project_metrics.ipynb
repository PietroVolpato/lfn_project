{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda4e133-8221-48cb-9c4f-bce54f81dc71",
   "metadata": {},
   "source": [
    "# Learning from networks project: Evaluation of different Node Embedding algorithms\n",
    "Members:<br>\n",
    "- D'Emilio Filippo, id : 2120931\n",
    "- Volpato Pietro, id : 2120825\n",
    "\n",
    "## Embedding evaluation notebook\n",
    "Using this notebook you can evaluate the quality of the embeddings produced using the following metrics:\n",
    "- Reconstruction error: measures the ability of reconstructing the original graph from the embedding vectors of the nodes.\n",
    "- Node classification: train classifier to predict which class a node belongs to. The classifier uses as features the embedding vectors of the nodes (or quantities computed from them). Transductive scenario\n",
    "- Link prediction: train a classifier that, given a pair of node embeddings, predicts whether exist an edge between such nodes in the graph.  Transductive scenario\n",
    "- Neighborhood preservation: quantifies the corrispondence between the neighborhood of a node in the embedding space and the actual neighborhood of a node in the graph.\n",
    "- Clustering (meaningful on facebook graph): we want to partition the nodes by clustering the embedding vectors space. We can then visualize the clustering on the nodes by coloring the nodes in the same cluster with the same color, using networkX to plot the graph. Since we are choosing as goal of the clustering community detection, the facebook graph is very suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a643ad2a-470f-4eca-8cb2-1fc209541c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import gzip\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d9ed8-3910-48db-a759-ee09a3c3f673",
   "metadata": {},
   "source": [
    "# configuration\n",
    "Here you can properly configure the names of the graphs and the names of the embedding strategies.<br>\n",
    "Set emb_dim = 128 to load 128-dimensional embeddings, set emb_dim = 256 to load 256-dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77bf3ab-83c4-46d5-b007-3982b64bc9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_keys = [\"facebook\",\"citation\",\"biological\",\"proteins\", \"spam\"]\n",
    "embedding_keys = [\"LINE\", \"node2vec\", \"AW\"]\n",
    "emb_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e598ebc-0beb-4076-abc7-b185f437fb7e",
   "metadata": {},
   "source": [
    "# Loading the embeddings\n",
    "The embeddings should be saved in ../result folder as a .npy file.<br>\n",
    "Embeddings are stored in a dictionary of dictionaries.<br>\n",
    "The first index refer to the graph (e.g. embeddings[\"facebook\"] contains the embeddings of the facebook graph for every embedding technique).<br>\n",
    "The second index refer to the embedding technique (e.g. embeddings[\"facebook\"][\"LINE\"] cointans the embedding of facebook graph computed using LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab0adc4-e065-4bc3-8296-39ca7a42eca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../result/embeddings_facebook_LINE_128.npy successfully\n",
      "Loaded ../result/embeddings_facebook_node2vec_128.npy successfully\n",
      "Loaded ../result/embeddings_facebook_AW_128.npy successfully\n",
      "Loaded ../result/embeddings_citation_LINE_128.npy successfully\n",
      "Warning: File '../result/embeddings_citation_node2vec_128.npy' not found.\n",
      "Loaded ../result/embeddings_citation_AW_128.npy successfully\n",
      "Loaded ../result/embeddings_biological_LINE_128.npy successfully\n",
      "Loaded ../result/embeddings_biological_node2vec_128.npy successfully\n",
      "Loaded ../result/embeddings_biological_AW_128.npy successfully\n",
      "Loaded ../result/embeddings_proteins_LINE_128.npy successfully\n",
      "Loaded ../result/embeddings_proteins_node2vec_128.npy successfully\n",
      "Loaded ../result/embeddings_proteins_AW_128.npy successfully\n",
      "Loaded ../result/embeddings_spam_LINE_128.npy successfully\n",
      "Loaded ../result/embeddings_spam_node2vec_128.npy successfully\n",
      "Loaded ../result/embeddings_spam_AW_128.npy successfully\n"
     ]
    }
   ],
   "source": [
    "def load(name):\n",
    "    \"\"\"\n",
    "    Loads a NumPy array from a file. If the file is not found, \n",
    "    displays a warning and returns None.\n",
    "\n",
    "    name (str): The name of the file (without extension) to load from the 'embeddings' directory.\n",
    "    \n",
    "    return: np.ndarray or None: The loaded NumPy array, or None if the file is not found.\n",
    "    \"\"\"\n",
    "    file_name = f\"../result/{name}.npy\"\n",
    "    if not os.path.exists(file_name):\n",
    "        print(f\"Warning: File '{file_name}' not found.\")\n",
    "        return None\n",
    "\n",
    "    emb = np.load(file_name)\n",
    "    print(f\"Loaded {file_name} successfully\")\n",
    "    return emb\n",
    "\n",
    "embeddings = {}\n",
    "for k in graph_keys:\n",
    "    embeddings[k] = {}\n",
    "\n",
    "for graph_key in graph_keys:\n",
    "    for emb_key in embedding_keys:\n",
    "        s = f\"embeddings_{graph_key}_{emb_key}_{emb_dim}\"\n",
    "        embeddings[graph_key][emb_key] = load(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0573ed-f2fa-4226-88f4-ebbcf1c470e4",
   "metadata": {},
   "source": [
    "# Loading the graphs\n",
    "Selected graphs:\n",
    "\n",
    "- Facebook_combined    https://snap.stanford.edu/data/ego-Facebook.html          \n",
    "- cit-Helpth           https://networkrepository.com/cit-HepTh.php             \n",
    "- bio-CE-CX            https://networkrepository.com/bio-CE-CX.php             \n",
    "- proteins-full        https://networkrepository.com/PROTEINS-full.php ---- the graph has node labels\n",
    "- spam                 https://networkrepository.com/web-spam-detection.php  ---- the graph has node labels\n",
    "\n",
    "To run this notebook, adjust the paths to match where the files are saved in your PC.<br>\n",
    "To keep paths as they are, download the repository.<br>\n",
    "Graphs are stored as a dictionary: the key is the graph name, the value is the corresponding netowrkx graph.<br>\n",
    "\n",
    "When it is created a networkX graph from a text file the node are renamed as integers form 0 to |V|-1, so that we can store the embeddings\n",
    "on a matrix, and each row index corresponds to the embedding vector of the corrisponding node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "067626d5-62a8-42a3-bf77-6952d5af8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_path = '../data/facebook/facebook_combined.txt.gz'\n",
    "citation_path = '../data/citation/cit-HepTh.edges'\n",
    "biological_path = '../data/biological/bio-CE-CX.edges'\n",
    "proteins_path = \"../data/proteins/PROTEINS-full.edges\"\n",
    "spam_path = \"../data/spam/web-spam-detection.edges\"\n",
    "\n",
    "proteins_labels_path = \"../data/proteins/PROTEINS-full.node_labels\"\n",
    "spam_labels_path = \"../data/spam/web-spam-detection.node_labels\"\n",
    "features_path = \"../data/proteins/PROTEINS-full.node_attrs\"  # node features, only proteins graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b13a0738-853b-40ab-a2fc-2f7d9c0a24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_edges(path):\n",
    "    \"\"\"\n",
    "    For files with extension .edges\n",
    "    nodes are renamed as integers, starting from 0\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('%'):  # Skip comment lines\n",
    "                continue\n",
    "            # Split the line based on spaces or commas\n",
    "            data = re.split(r'[,\\s]+', line.strip())\n",
    "            if len(data) < 2:  # Skip lines that don't have at least two columns\n",
    "                continue\n",
    "            # Extract the first two columns (nodes)\n",
    "            node1, node2 = int(data[0]), int(data[1])\n",
    "            G.add_edge(node1, node2)\n",
    "        \n",
    "    return relabel_get_mapping(G)\n",
    "\n",
    "def load_graph_with_gz(path):\n",
    "    \"\"\"\n",
    "    For files with extension .txt.gz\n",
    "    nodes are renamed as integers, starting from 0\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            node1, node2 = map(int, line.strip().split())\n",
    "            G.add_edge(node1, node2)\n",
    "            \n",
    "    return relabel_get_mapping(G)\n",
    "\n",
    "def print_graphs_info(graphs):\n",
    "    for k in graph_keys:\n",
    "        G = graphs[k]\n",
    "        print(f\"{k}: |V|={len(G.nodes)}, |E|={len(G.edges)}\")\n",
    "\n",
    "def relabel_get_mapping(G):\n",
    "    \"\"\"\n",
    "    Given a graph G, this function returns a graph where the nodes are relabeled as integers, form 0 to |V|-1.\n",
    "    It is also returned the mapping from original name to relabeled name.\n",
    "    \"\"\"\n",
    "    inverse_mapping = {i : node for i,node in enumerate(G.nodes)} # mappoing new_name : original_name\n",
    "    direct_mapping = {node : i for i,node in enumerate(G.nodes)} # mapping original_name : new_name\n",
    "    G = nx.relabel_nodes(G, direct_mapping)\n",
    "    return G, direct_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b659245-9c28-4968-8261-b5a05ea83ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook: |V|=4039, |E|=88234\n",
      "citation: |V|=22908, |E|=2444798\n",
      "biological: |V|=15229, |E|=245952\n",
      "proteins: |V|=43471, |E|=81049\n",
      "spam: |V|=9072, |E|=473854\n"
     ]
    }
   ],
   "source": [
    "graphs = {}  # dictionary containg the graphs\n",
    "mappings = {} # dictionary to contain the mappings. Original name : relabeled name\n",
    "for k in graph_keys:\n",
    "    mappings[k] = {}\n",
    "    \n",
    "# facebook graph is the only one .tar.gz        \n",
    "graphs[graph_keys[0]], mappings[graph_keys[0]] = load_graph_with_gz(facebook_path) \n",
    "graphs[graph_keys[1]], mappings[graph_keys[1]] = load_graph_edges(citation_path)\n",
    "graphs[graph_keys[2]], mappings[graph_keys[2]] = load_graph_edges(biological_path)\n",
    "graphs[graph_keys[3]], mappings[graph_keys[3]] = load_graph_edges(proteins_path)  # node labeled\n",
    "graphs[graph_keys[4]], mappings[graph_keys[4]] = load_graph_edges(spam_path)  # node labeled\n",
    "\n",
    "print_graphs_info(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0f1f9-4e40-4f9a-a7b1-83d9791f3662",
   "metadata": {},
   "source": [
    "## Extract the labels\n",
    "The graphs \"proteins\" and \"spam\" have node labels. <br>\n",
    "In the text file containing the labels each line has only one number (corresponding class), and such element refers implicitly to the line number node, according to the original node names definition.<br>\n",
    "Since when we load a graph we rename node as integers starting from 0, the mapping is applied to match each label with the correct node according\n",
    "to the modified node names.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c02b2a2-b8e9-491a-8f00-280af4c90900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_node_labels(file_path, graph_key):\n",
    "    \"\"\"\n",
    "    Reads a file containing node labels and returns a dictionary mapping nodes to labels.\n",
    "    The labels are assumed to be listed in sequential order: first label is relative to first node (according to original name), and so on...\n",
    "\n",
    "    Parameters: file_path (str): Path to the node label file.\n",
    "                graph_key : key of the graph, needed for the mappings between original and renamed nodes.\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "        node_labels (dict): A dictionary where keys are oroginal node names, values are their corresponding labels.\n",
    "    \"\"\"\n",
    "    node_labels = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        i = 1 # keep track of incedes of original names because bad CL fata format\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "\n",
    "            # i is the node original name, but labels skip nodes that are isolated\n",
    "            map = mappings[graph_key]\n",
    "            while i not in map.keys():  # skip isolated nodes\n",
    "                i += 1\n",
    "            \n",
    "            real_name = map[i]\n",
    "            label = int(line)  # Parse the labels\n",
    "            node_labels[real_name] = label  # Map the line number (node ID) to the label\n",
    "            i += 1\n",
    "\n",
    "    return node_labels\n",
    "\n",
    "labels = {}\n",
    "labels[\"proteins\"] = load_node_labels(proteins_labels_path, \"proteins\")\n",
    "labels[\"spam\"] = load_node_labels(spam_labels_path, \"spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e094e049-0d43-45fa-b2ac-26df13a76c1f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## 2D visualization of the embeddings\n",
    "Using PCA or tsne is possible to visualize the embedding vectors in a 2D space.<br>\n",
    "This might be useful to have an idea of the embedding vectors distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa515bea-41df-4b58-9aa9-cd38aee642cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings, labels=None, method=\"tsne\", random_state=69):\n",
    "    \"\"\"\n",
    "    Visualize embeddings in 2D space using t-SNE or PCA.\n",
    "    \n",
    "    Parameters:\n",
    "        embeddings (np.ndarray): Embedding matrix of shape (n_nodes, dimensions).\n",
    "        labels (list or np.ndarray): Optional, class labels for nodes. Used for coloring.\n",
    "        method (str): Dimensionality reduction method (\"tsne\" or \"pca\").\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        \n",
    "    \"\"\"\n",
    "    if method == \"tsne\":\n",
    "        reducer = TSNE(n_components=2, random_state=random_state, init='random', perplexity=30)\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    elif method == \"pca\":\n",
    "        reducer = PCA(n_components=2, random_state=random_state)\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'tsne' or 'pca'\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    if labels is not None:\n",
    "        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap='tab10', s=10, alpha=0.8)\n",
    "        plt.legend(*scatter.legend_elements(), title=\"Classes\", loc=\"upper right\", fontsize=\"small\", markerscale=2)\n",
    "    else:\n",
    "        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=10, alpha=0.8)\n",
    "    \n",
    "    plt.title(f\"2D Visualization of Embeddings using {method.upper()}\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f472b18-4249-4166-ac24-1f1a1aecee76",
   "metadata": {},
   "source": [
    "## Choose the embeddings you want to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eea42f-daf6-44da-8399-cbcc5efa7cf8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CHOOSE\n",
    "graph_key = \"facebook\"\n",
    "embedding_key = \"AW\"\n",
    "mathod = \"tsne\"\n",
    "\n",
    "lab = None\n",
    "if graph_key in [\"proteins\",\"spam\"]: \n",
    "    # sort the labels in a list to visualize node classes colors\n",
    "    lab = []\n",
    "    for i in range(len(labels[graph_key])):\n",
    "        lab.append(labels[graph_key][i])\n",
    "        \n",
    "visualize_embeddings(embeddings[graph_key][embedding_key], lab, method = mathod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbf757-a870-4245-a68c-acfc79b39e06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Reconstruction error\n",
    "The reconstruction error measures the ability of reconstructing the original graph using the node embedding vectors.<br>\n",
    "Reconstruct the connections between nodes given the embeddings is not trivial, since different embedding techniques may produce the embedding vectors in very different ways, hence the best way to reconstruct the graph vary with the embedding strategy considered.<br>\n",
    "Doing an exact computation of the reconstruction error implies to scan all possible pair of nodes, which is computationally infeasible for large graphs (10^5 nodes ----> 10^10 computation of similarities between pair of 128-dimensional vectors).<br>\n",
    "To solve the computational complexity problem, we developed a simple approximation algorithm. <br>\n",
    "First of all, we need to define a similarity function, that given the embeddings of two nodes, returns the similarity of those w.r.t. a define measure (e.g. cosine similarity, Euclidean distance). Note that a similarity measures might be optimal for an embedding strategy, and meaningless for another embedding strategy.<br>\n",
    "The approximate_reconstruction_error() is a very simple algorithm based on sampling, and can be summaryzed in two steps:\n",
    "1) Draw positive and negative samples to define a threshold, which will be the criterion to \"guess\" if, given pair of embedding vectors, there is an edge in the graph.\n",
    "2) Draw new positive and negative samples, use the similarity function and apply the threshold to such value to make the predictions. Then the reconstruction error is defined as the number of errors divided by the number of samples. Note that an error can be a FP: guess a non-existing edge, or a FN: missed to guess an existing edge.\n",
    "\n",
    "INPUT:\n",
    "- G=(V,E) undirected graph\n",
    "- embeddings: matrix of embeddings, embeddings[i] is the embedding vector of node i\n",
    "- sim_function: a function returning the similarity of two vectors. Assumption: higher (with sign) value represents higher similarity.\n",
    "\n",
    "OUTPUT: approximated RE of G with given embeddings\n",
    "<pre>\n",
    "approximate_reconstruction_error(G, embeddings, sim_function)\n",
    "\n",
    "N <--- int(ln(|E|)^3.5)\n",
    "{draw samples to define the threshold}\n",
    "positive_edges <---- draw uniformly at random N edges\n",
    "negative_edges <---- draw uniformly at random N pair of nodes for which does not exist an edge in the graph\n",
    "sim_pos <---- average similarity of all pair of nodes in positive_edges\n",
    "sim_neg <---- average similarity of all pair of nodes in negative_edges\n",
    "threshold <---- (sim_pos+sim_neg)/2\n",
    "{draw samples to reconstruct the graph}\n",
    "positive_edges <---- draw uniformly at random N edges\n",
    "negative_edges <---- draw uniformly at random N pair of nodes for which does not exist an edge in the graph\n",
    "samples <---- [positive_edges,negative_edges]\n",
    "errors <---- 0\n",
    "for each (u,v) in samples:\n",
    "    prediction <--- True if sim_function(embeddings[u],embeddings[v])>= threshold, False otherwise\n",
    "    compare the prediction with the actual presence of edge (u,v) in the graph, and update error counter\n",
    "return errors/|samples|\n",
    "</pre>\n",
    "\n",
    "The reconstruction error is in the interval [0,1]. RE = 1 means that (approximately) we are able to perfectly reconstruct the original graph given the embedding vectors. <br>\n",
    "NOTES:\n",
    "- We are free to choose the similarity function that is more meaningful w.r.t. the embedding strategy, for example the cosine similarity works perfectly with LINE embeddings, but very poorly for node2vec or Attention Walk. This is because of how the embeddings are generated: LINE embeddings are optimized to explicitly preserve proximity between nodes, making cosine similarity a natural measure of similarity. For other methods like Node2Vec and Attention Walk, the embeddings might encode other structural properties that cosine similarity fails to capture.\n",
    "- The computation of the reconstruction error might seem similar to a link prediction task, but there is a big key difference: in link prediction we are training a model providing examples, constituted by the features and the ground truth (1 = edge, 0 = no edge). During the computation of the RE instead we are only using the embeddings trying to guess the presence of edges, without ever having any information of the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0180ec5d-ce56-48ce-93af-06814b1b4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_function(x,y, f = \"euclidean distance\"):\n",
    "    if f == \"cosine similarity\":\n",
    "        return cosine_similarity(x.reshape(1, -1), y.reshape(1, -1))[0, 0]\n",
    "    if f == \"euclidean distance\":\n",
    "        return -np.linalg.norm(x - y)  # since the returned value is interpreted as a similarity, - sign\n",
    "    \n",
    "def approximate_reconstruction_error(\n",
    "    G,\n",
    "    embeddings,\n",
    "    sim_function = \"euclidean distance\",\n",
    "    threshold_function = lambda a,b: (a + b)/2\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes an approximate reconstruction error for a graph embedding.\n",
    "    \n",
    "    Parameters:\n",
    "        G (nx.Graph): Input graph.\n",
    "        embeddings (np.ndarray): Embedding matrix (n_nodes x dimensions).\n",
    "        sim_function: defines what metric is used to establish the similarity between two embedding vectors\n",
    "        threshold_function defines how is computed the threshold from sim_neg = a, sim_pos = b\n",
    "    \n",
    "    Returns:\n",
    "        dict: Reconstruction error and additional statistics (e.g., FP, FN).\n",
    "    \"\"\"\n",
    "    if sim_function not in [\"cosine similarity\", \"euclidean distance\"]:\n",
    "        print(\"Possible values of sim_function: cosine similarity, euclidean distance\")\n",
    "        return None\n",
    "        \n",
    "    nodes = list(G.nodes())\n",
    "    n_samples = int(math.log(len(G.edges))**3.5)\n",
    "\n",
    "    # pick the samples to compute average cosine similarity of positive and negative edges\n",
    "    positive_edges = random.sample(list(G.edges()), n_samples)\n",
    "    negative_edges = []\n",
    "    while len(negative_edges) < len(positive_edges):\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not G.has_edge(u, v) and (u, v) not in negative_edges and (v, u) not in negative_edges:\n",
    "            negative_edges.append((u, v))\n",
    "            \n",
    "    # average cos sim for existing edges\n",
    "    sim_pos = np.mean([similarity_function(embeddings[u], embeddings[v], sim_function) for u,v in positive_edges])\n",
    "    sim_neg = np.mean([similarity_function(embeddings[u], embeddings[v], sim_function) for u,v in negative_edges])\n",
    "\n",
    "    # define threshold for reconstruction of the graph\n",
    "    threshold = threshold_function(sim_neg, sim_pos)\n",
    "    \n",
    "    # pick new fresh samples to approximate the reconstructon ability of the graph\n",
    "    positive_edges = random.sample(list(G.edges()), n_samples)\n",
    "    negative_edges = []\n",
    "    while len(negative_edges) < len(positive_edges):\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not G.has_edge(u, v) and (u, v) not in negative_edges and (v, u) not in negative_edges:\n",
    "            negative_edges.append((u, v))\n",
    "\n",
    "    errors = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    samples = positive_edges + negative_edges  # concatenate the samples\n",
    "    for u,v in samples:\n",
    "        sim = similarity_function(embeddings[u], embeddings[v], sim_function)\n",
    "        prediction = True if sim >= threshold else False\n",
    "        if prediction == True and not G.has_edge(u, v):  # false positive\n",
    "            errors+=1\n",
    "            fp+=1\n",
    "        if prediction == False and  G.has_edge(u, v):  # false positive\n",
    "            errors+=1\n",
    "            fn+=1\n",
    "    re = errors/len(samples)\n",
    "    fp_rate = fp/len(samples)\n",
    "    fn_rate = fn/len(samples)\n",
    "    return {\n",
    "        \"re\": re,\n",
    "        \"fp_rate\": fp_rate,\n",
    "        \"fn_rate\": fn_rate,\n",
    "        \"sim_pos\": sim_pos,\n",
    "        \"sim_neg\": sim_neg\n",
    "    }\n",
    "    \n",
    "def print_reconstruction_error(dict, graph_name , embedding_technique, sim_function, show_all):\n",
    "    re = dict[\"re\"]\n",
    "    fp_rate = dict[\"fp_rate\"]\n",
    "    fn_rate = dict[\"fn_rate\"]\n",
    "    sim_pos = dict[\"sim_pos\"]\n",
    "    sim_neg = dict[\"sim_neg\"]\n",
    "    if show_all:\n",
    "        s = f\"RE ({sim_function}) of {graph_name} using {embedding_technique} = {re:.4f}. FP rate = {fp_rate:.4f}. FN rate = {fn_rate:.4f}. \"\n",
    "        s += f\"avg_pos = {sim_pos:.4f}, avg_neg = {sim_neg:.4f}\"\n",
    "        print(s)\n",
    "    else:\n",
    "        print(f\"RE of {graph_name} using {embedding_technique} = {re:.4f}\")\n",
    "        \n",
    "def compute_all_reconstruction_errors(graph_keys, embedding_keys, f_dict, thresholds_dict, show_all = True):\n",
    "    for emb_key in embedding_keys:\n",
    "        for graph_key in graph_keys:\n",
    "            G = graphs[graph_key]\n",
    "            E = embeddings[graph_key][emb_key]\n",
    "            sim_function = f_dict[emb_key]  # associate the similarity function\n",
    "            threshold_function = thresholds_dict[emb_key]  # associate the threshold function\n",
    "            if  E is not None and E.size != 0:\n",
    "                dict = approximate_reconstruction_error(\n",
    "                    G, E, sim_function = sim_function, threshold_function = threshold_function\n",
    "                )\n",
    "                print_reconstruction_error(dict, graph_key, emb_key, sim_function, show_all = show_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680dc3f3-8fa9-4563-b910-bf10da750a29",
   "metadata": {},
   "source": [
    "## Compute the RE\n",
    "Here you can compute the reconstruction error.<br>\n",
    "- Set graph_keys_RE with the keys of the graphs you are interested. graph_keys_RE = graph_keys for all graphs.<br>\n",
    "- set embedding_keys_RE with the keys of the embedding strategies you are interested. graph_keys_RE = embedding_keys for all embedding strategies.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "507c10ab-a648-4469-82b7-2ad908c1c3f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RE (euclidean distance) of facebook using node2vec = 0.0341. FP rate = 0.0306. FN rate = 0.0035. avg_pos = -1.6526, avg_neg = -4.0482\n",
      "RE (euclidean distance) of biological using node2vec = 0.0421. FP rate = 0.0416. FN rate = 0.0005. avg_pos = -2.1538, avg_neg = -4.3660\n",
      "RE (euclidean distance) of proteins using node2vec = 0.0003. FP rate = 0.0003. FN rate = 0.0000. avg_pos = -0.4781, avg_neg = -5.7073\n",
      "RE (euclidean distance) of spam using node2vec = 0.0925. FP rate = 0.0750. FN rate = 0.0174. avg_pos = -2.0148, avg_neg = -3.5319\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m embedding_keys_RE \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode2vec\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLINE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     18\u001b[0m show_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mcompute_all_reconstruction_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_keys_RE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_keys_RE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholds_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_all\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshow_all\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 100\u001b[0m, in \u001b[0;36mcompute_all_reconstruction_errors\u001b[1;34m(graph_keys, embedding_keys, f_dict, thresholds_dict, show_all)\u001b[0m\n\u001b[0;32m     98\u001b[0m threshold_function \u001b[38;5;241m=\u001b[39m thresholds_dict[emb_key]  \u001b[38;5;66;03m# associate the threshold function\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m  E \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m E\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mapproximate_reconstruction_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msim_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold_function\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     print_reconstruction_error(\u001b[38;5;28mdict\u001b[39m, graph_key, emb_key, sim_function, show_all \u001b[38;5;241m=\u001b[39m show_all)\n",
      "Cell \u001b[1;32mIn[8], line 60\u001b[0m, in \u001b[0;36mapproximate_reconstruction_error\u001b[1;34m(G, embeddings, sim_function, threshold_function)\u001b[0m\n\u001b[0;32m     58\u001b[0m samples \u001b[38;5;241m=\u001b[39m positive_edges \u001b[38;5;241m+\u001b[39m negative_edges  \u001b[38;5;66;03m# concatenate the samples\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u,v \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[1;32m---> 60\u001b[0m     sim \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m sim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m G\u001b[38;5;241m.\u001b[39mhas_edge(u, v):  \u001b[38;5;66;03m# false positive\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m, in \u001b[0;36msimilarity_function\u001b[1;34m(x, y, f)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_function\u001b[39m(x,y, f \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean distance\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean distance\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x \u001b[38;5;241m-\u001b[39m y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1584\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1582\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m X_normalized\n\u001b[0;32m   1583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1584\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1586\u001b[0m K \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39mdense_output)\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:187\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1841\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# axis == 1:\u001b[39;00m\n\u001b[0;32m   1839\u001b[0m     sparse_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1841\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1843\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthe normalize function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1849\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:986\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[1;32m--> 986\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;66;03m# always make a copy for non-numpy arrays\u001b[39;00m\n\u001b[0;32m    991\u001b[0m     array \u001b[38;5;241m=\u001b[39m _asarray_with_order(\n\u001b[0;32m    992\u001b[0m         array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\_array_api.py:378\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# chose the similarity function for each embedding strategy. this choice has a big impact on the results.\n",
    "f_dict = {\n",
    "    \"LINE\" : \"cosine similarity\",\n",
    "    \"AW\" : \"euclidean distance\",\n",
    "    \"node2vec\" : \"euclidean distance\"\n",
    "}\n",
    "\n",
    "thresholds_dict = {\n",
    "    # a will be sim_neg, b will be sim_pos. Assume sim_neg < sim_pos\n",
    "    \"LINE\" : lambda a,b: (a + b)/2,   # good threshold for line is a simple average \n",
    "    \"AW\" : lambda a,b: (a + b)/2, # threshold for AW is more than the average (otherwise too many FP) a + (b - a)*1/4,\n",
    "    \"node2vec\" : lambda a,b: (a + b)/2\n",
    "}\n",
    "\n",
    "# CHOOSE\n",
    "graph_keys_RE = [\"facebook\", \"biological\", \"citation\", \"proteins\", \"spam\"]\n",
    "embedding_keys_RE = [\"node2vec\", \"LINE\", \"AW\"]\n",
    "show_all = True\n",
    "\n",
    "compute_all_reconstruction_errors(graph_keys_RE, embedding_keys_RE, f_dict, thresholds_dict, show_all = show_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce44b9d-c39d-4c41-8d31-fbca41859cb6",
   "metadata": {},
   "source": [
    "# NODE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c3296b-c090-4ac0-8121-1f5e2b1871c7",
   "metadata": {},
   "source": [
    "## Analysis of the datasets\n",
    "It is always a good idea to have a look at the datasets we are dealing with.<br>\n",
    "- features are the embeddings of the nodes, which is an D-dimensional vector, where D (e.g. 128) is the dimension of the specific embeddings\n",
    "we are using to training.\n",
    "- the labels represent the classes of the nodes. We can analyze the labels set to see how many different classes there are and understand how balanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87b8e962-b3fe-452a-9174-38adb7b05953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO DATASET PROTEINS GRAPH\n",
      "Number of samples: 43471\n",
      "Number of classes: 3\n",
      "Samples of class 0: 21151\n",
      "Samples of class 1: 20931\n",
      "Samples of class 2: 1389\n",
      "INFO DATASET SPAM GRAPH\n",
      "Number of samples: 9072\n",
      "Number of classes: 3\n",
      "Samples of class 2: 6365\n",
      "Samples of class 1: 594\n",
      "Samples of class 3: 2113\n"
     ]
    }
   ],
   "source": [
    "def analyze_labels(labels):\n",
    "    labels_count = {}\n",
    "    for label in labels.values():\n",
    "        if not label in labels_count.keys():\n",
    "            labels_count[label] = 0\n",
    "        labels_count[label] += 1\n",
    "    print(f\"Number of samples: {len(labels)}\")\n",
    "    print(f\"Number of classes: {len(labels_count)}\")\n",
    "    for label in labels_count.keys():\n",
    "        print(f\"Samples of class {label}: {labels_count[label]}\")\n",
    "\n",
    "print(\"INFO DATASET PROTEINS GRAPH\")\n",
    "analyze_labels(labels[\"proteins\"])\n",
    "\n",
    "print(\"INFO DATASET SPAM GRAPH\")\n",
    "analyze_labels(labels[\"spam\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b989ec4-f073-47ce-a580-41c7faa33c22",
   "metadata": {},
   "source": [
    "## Train and evaluate the model\n",
    "A support vector machine is trained from the dataset provided in input.<br>\n",
    "Class weights are used to handle the case of an imbalance dataset (some classes have considerably less examples than others).<br>\n",
    "The model is trained on 80% of the dataset (training set), while 20% of the dataset (test set) is left to make predictions and evaluate the performances of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "982501d4-c3d3-4f9c-bdc6-0e15d6e56438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_balanced_classifier(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Train and evaluate a balanced classifier for multi-class node classification.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings (np.ndarray): Numpy array where each row is a node's embedding.\n",
    "        labels (dict): Dictionary mapping node indices to their labels.\n",
    "\n",
    "    \"\"\"\n",
    "    # Ensure X (features) and y (labels) are aligned\n",
    "    X = np.array(embeddings)  # Node embeddings\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)  # Normalize embeddings\n",
    "    y = np.array([labels[i] for i in range(len(labels))])  # Ensure correct ordering of labels\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "    # Initialize the classifier\n",
    "    clf = SVC(kernel='rbf', class_weight = 'balanced', decision_function_shape='ovo')    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate on training set\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "    train_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "    # Predict and evaluate on test set\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "    test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Training Results:\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Training Macro F1-Score: {train_f1:.4f}\")\n",
    "    #print(\"Training Classification Report:\\n\", train_report)\n",
    "\n",
    "    print(\"Test Results:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Macro F1-Score: {test_f1:.4f}\")\n",
    "    print(\"Test Classification Report:\\n\", test_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f07958-6c62-422a-83a6-885c2df44f53",
   "metadata": {},
   "source": [
    "## Choose the graph and the embedding strategy to train the model\n",
    "The graphs with node labels are \"proteins\" and \"spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32ca218-3478-478e-8365-dba1092d8c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Training Accuracy: 0.3291\n",
      "Training Macro F1-Score: 0.3287\n",
      "Test Results:\n",
      "Test Accuracy: 0.3157\n",
      "Test Macro F1-Score: 0.3250\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.92      0.39       134\n",
      "           2       0.72      0.08      0.15      1258\n",
      "           3       0.29      0.81      0.43       423\n",
      "\n",
      "    accuracy                           0.32      1815\n",
      "   macro avg       0.42      0.61      0.33      1815\n",
      "weighted avg       0.58      0.32      0.23      1815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CHOOSE\n",
    "embedding_key = \"AW\"\n",
    "graph_key = \"spam\"  # proteins or spam\n",
    "\n",
    "E = embeddings[graph_key][embedding_key]\n",
    "y = labels[graph_key]\n",
    "\n",
    "x = train_balanced_classifier(E, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc300f-1cdd-4976-a58d-34502e08bf1f",
   "metadata": {},
   "source": [
    "## Train a model using real node attributes as features\n",
    "This can be done only for \"proteins\" graph, which is the only dataset with node attributes.<br>\n",
    "We train a model using as features the real node attributes (knowledge of quantities in considered domain).<br>\n",
    "We expect the following model to perform better than a model training using only the embedding vectors as features, since:\n",
    "- The embeddings are produced using only information about the network. (connections between nodes)\n",
    "- The node attributes represents real-world measured quantites relative to a node. (domain knowledge)\n",
    "\n",
    "It is reasonable that the attributes of a node are a better representation than the embedding vector, since we are dealing with a node classification task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce3ce9a-350d-49bb-8837-ce8ca335d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attributes(path, map):\n",
    "    with open(path, 'r') as file:\n",
    "        i = 1\n",
    "        features_dict = {}\n",
    "        for line in file:\n",
    "            # Split the line based on spaces or commas\n",
    "            node_features = np.array(re.split(r'[,\\s]+', line.strip()))\n",
    "            # mapping needed to correctly match features with the renaming of the nodes\n",
    "            real_name = map[i]\n",
    "            features_dict[real_name] = node_features \n",
    "            i += 1\n",
    "    features = np.array([features_dict[i] for i in range(len(features_dict))]) # node-sorted numpy array\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517931c1-a01a-4068-88f8-5c16657ae7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = extract_attributes(features_path, mappings[\"proteins\"])\n",
    "y = labels[\"proteins\"]\n",
    "\n",
    "train_balanced_classifier(X, y)  # we train same classifier, but using node attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a83584-a598-41b7-8644-293d7f6f5106",
   "metadata": {},
   "source": [
    "## Train a model combining attributes and embedding vectors as features\n",
    "Even if node attributes are more powerful than embedding vectors for a node classification task, it might be that using as features both the node attributes and the embedding vectors we are able to produce a classifier that performs better than the two separate cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe0099ec-ddd6-4e5e-8972-b44acc687dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Training Accuracy: 0.9129\n",
      "Training Macro F1-Score: 0.8638\n",
      "Test Results:\n",
      "Test Accuracy: 0.8703\n",
      "Test Macro F1-Score: 0.8193\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      4209\n",
      "           1       0.88      0.87      0.87      4226\n",
      "           2       0.57      0.92      0.70       260\n",
      "\n",
      "    accuracy                           0.87      8695\n",
      "   macro avg       0.78      0.89      0.82      8695\n",
      "weighted avg       0.88      0.87      0.87      8695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb_key = \"AW\"  # chose the embedding strategy\n",
    "\n",
    "E = embeddings[\"proteins\"][emb_key]\n",
    "X = extract_attributes(features_path, mappings[\"proteins\"])\n",
    "y = labels[\"proteins\"]\n",
    "\n",
    "combined_features = np.zeros((X.shape[0], X.shape[1]+E.shape[1])) # concatenation of node attributes and embedding vectors\n",
    "\n",
    "for i in range(combined_features.shape[0]):  # for each row\n",
    "    combined_features[i] = np.concatenate((X[i], E[i]))  # concatenate node attributes and node embeddings\n",
    "    \n",
    "train_balanced_classifier(combined_features, y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634f62e-ad49-480a-a4df-7045dcc42d8a",
   "metadata": {},
   "source": [
    "# LINK PREDICTION\n",
    "The link prediction task consists on training a model, logistic regression in this case, that given as input the features of a pair of nodes predicts whether the edge between them exists in the graph or not.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c7493-a241-40e7-ba76-76663489b710",
   "metadata": {},
   "source": [
    "## PRODUCING THE DATASET\n",
    "We decided to produce a perfectly balanced dataset in this way: all the positive examples are all the pair of nodes (actually the features we decided to use to represent them) for which exists an edge in the graph.<br>\n",
    "The negative examples instead are created by chosing at random a pair of nodes, and if there is no edge between them such pair is a negative example. We repeat this procedure until the number of negative examples matches the number of positive examples<br>\n",
    "\n",
    "We defined 3 possible choices for the features representing pair of nodes:\n",
    "- The concatenation of the embeddings of the two nodes\n",
    "- The concatenation of the embeddings + the similarity between those (a scalar)\n",
    "- Just the scalar similarity between the two embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404fdd9-3796-448e-8caa-e9958d04d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_link_prediction_dataset(G, embeddings, sim_function = None, only_sim_function = False):\n",
    "    \"\"\"\n",
    "    Creates a dataset for link prediction. \n",
    "\n",
    "    Parameters:\n",
    "        G (nx.Graph): The input graph.\n",
    "        embeddings (np.ndarray): Node embeddings as a numpy array (indexed by node ID).\n",
    "        sim_function: lambda function computing the similarity scalar between two embeddings.\n",
    "                      if sim_function is not none, the similarity is added as feature (+1 features dimension)\n",
    "        only_sim_function: if True the features are just the scalar representing the similarity between the two embedding vectors.\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Features.\n",
    "        y (np.ndarray): Labels (1 for existing edges, 0 for non-existing edges).\n",
    "    \"\"\"\n",
    "    if only_sim_function and sim_function is None:\n",
    "        print(\"You set only_sim_function = True, but no similarity function provided\")\n",
    "        return None\n",
    "        \n",
    "    positive_edges = list(G.edges())\n",
    "    num_positive = len(positive_edges)\n",
    "\n",
    "    # Generate negative edges\n",
    "    nodes = list(G.nodes())\n",
    "    negative_edges = set()\n",
    "    while len(negative_edges) < num_positive:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not G.has_edge(u, v) and (u, v) not in negative_edges and (v, u) not in negative_edges:\n",
    "            negative_edges.add((u, v))\n",
    "\n",
    "    # Combine positive and negative edges\n",
    "    all_edges = positive_edges + list(negative_edges)\n",
    "    labels = [1] * len(positive_edges) + [0] * len(negative_edges)\n",
    "\n",
    "    # Create features from embeddings\n",
    "    X = []\n",
    "    for u, v in all_edges:       \n",
    "        u_emb = embeddings[u].reshape(1, -1)\n",
    "        v_emb = embeddings[v].reshape(1, -1)\n",
    "        \n",
    "        if only_sim_function:  # features are only cosine similarities (scalar)\n",
    "            sim = sim_function(u_emb, v_emb)\n",
    "            X.append(sim)\n",
    "            \n",
    "        else:  # features are concatenation of embeddings and eventually also the cosine similarity            \n",
    "            if sim_function is not None:         \n",
    "                sim = sim_function(u_emb, v_emb)\n",
    "                feature = np.concatenate([embeddings[u], embeddings[v], [sim]]) # Concatenate embeddings and cosine similarity\n",
    "            else:\n",
    "                feature = np.concatenate([embeddings[u], embeddings[v]]) # concatenate the embeddings\n",
    "            X.append(feature)\n",
    "            \n",
    "    X = np.array(X).reshape(-1, 1) if only_sim_function else np.array(X)        \n",
    "    y = np.array(labels)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba97a2-b3e4-4f42-ac7f-40230f913e5d",
   "metadata": {},
   "source": [
    "## TRAIN LOGISTIC REGRESSION CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2475125-54a2-426e-928d-06c883a5d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_link_prediction(X,y):\n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier for link prediction\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)  # normalization\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "    # Train logistic regression model\n",
    "    clf = LogisticRegression(max_iter = 2000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"ROC AUC Score:\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322c1d5-2ffb-40ca-a377-4079d41d5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_key = \"AW\"\n",
    "graph_key = \"facebook\"\n",
    "\n",
    "# dictionary mapping embedding keys to proper similarity functions\n",
    "sim_f_dict = {\n",
    "    \"LNE\" : lambda a,b: cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0, 0],\n",
    "    \"AW\" : lambda a,b: -np.linalg.norm(a - b),\n",
    "    \"node2vec\" : lambda a,b: -np.linalg.norm(a - b)\n",
    "}\n",
    "\n",
    "f = sim_f_dict[embedding_key] if embedding_key in sim_f_dict.keys() else None  # avoid key errors\n",
    "only_sim_function = False\n",
    "\n",
    "G = graphs[graph_key]\n",
    "E = embeddings[graph_key][embedding_key]\n",
    "X,y = create_link_prediction_dataset(G, E, sim_function = f, only_sim_function = only_sim_function)\n",
    "\n",
    "LR_link_prediction(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965b90c-61d8-4b4b-afc8-2e0f6211fbda",
   "metadata": {},
   "source": [
    "# Neighborhood preservation\n",
    "To compute the top-k nearest neighbors (k-NN) we use FAISS (Facebook AI Similarity Search), which is an approximation algorithm to compute the k-NN for each node in the graph. Since we are evaluating neighborhood preservation of the embeddings, we excluded from the top-k list the node itself (FAISS include by default the node itself as its nearest neighbor).<br>\n",
    "The neighborhood preservation score is defined as follow:<br>\n",
    "\n",
    "INPUT:\n",
    "- G = (V,E): undirected graph\n",
    "- embeddings: numpy array, where embeddings[i] is the embedding vector of node i in G\n",
    "- k: for each node are extracted the top-k neighbors in the embedding space\n",
    "- order: can be either 1 or 2. The neighborhood of u G is defined as the set of nodes reachable from u with a path of lenght \"order\" (1 or 2)\n",
    "\n",
    "OUTPUT: neighborhood preservation score\n",
    "\n",
    "<pre>\n",
    "neighborhood_preservation_score(G, embeddings, k, order)\n",
    "    \n",
    "extract top-k neighbors in embedding space using FAISS for each node in V\n",
    "for each node v in V:\n",
    "    let i be the order of neighborhood considered\n",
    "    let Nv be the set of i-th order neighborhood of v\n",
    "    let k-NNv be the set of top-k nearest neighbors of v in the embedding space (euclidean distance)\n",
    "    overlap <---- |Nv INTERSECTION k-NNv|\n",
    "    normalization <---- MIN{|Nv|, k}   (k = |k-NNv|)\n",
    "    score(v) <---- overlap / normalization\n",
    "n_p_score <---- (sum of score(v) for all v in V) / |V|\n",
    "return n_p_score\n",
    "</pre>\n",
    "\n",
    "The returned score is always in the interval [0,1]. The goal of the neighborhood preservation score is to have a an ojbective quantity\n",
    "that tells if the neighbors of the nodes in the embeddings space match with the actual neighbors of the nodes in the graph. \n",
    "<br>\n",
    "\n",
    "Consider a node v.\n",
    "- if |Nv| <= k ( MIN = |Nv|), then the score represents how many nodes in Nv are also in k-NNv. (1 = all |Nv| nodes, 0 = none)\n",
    "- if |Nv| > k ( MIN = k), then the score represents how many nodes in k-NNv are also in Nv. (1 = all k nodes, 0 = none)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50d040-7c01-4ad4-8789-5627b210d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def get_k_nearest_neighbors(embeddings, k = 5, index_type = \"L2\"):\n",
    "    if index_type not in [\"L2\", \"cosine\"]:\n",
    "        raise ValueError(\"index_type can be 'L2' or 'cosine'\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    if index_type == \"cosine\": \n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        # Use IndexFlatIP for cosine similarity\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "    if index_type == \"L2\":\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    index.add(embeddings)\n",
    "    # Query for top-k nearest neighbors\n",
    "    distances, indices = index.search(embeddings, k + 1)  # Query for k+1 neighbors\n",
    "    top_k_neighbors = [ind[1:] for ind in indices]  # Exclude self (first neighbor)\n",
    "    return top_k_neighbors\n",
    "    \n",
    "def neighborhood_preservation_score(G, E, k = 5, order = 1, index_type = \"L2\"):\n",
    "    if order not in [1,2]:\n",
    "        raise ValueError(\"Order must be either 1 (first order neighborhood) or 2 (second order neighborhood)\")\n",
    "    if index_type not in [\"L2\", \"cosine\"]:\n",
    "        raise ValueError(\"index_type can be 'L2' or 'cosine'\")\n",
    "\n",
    "    top_k_neighbors = get_k_nearest_neighbors(E, k=k, index_type = index_type)\n",
    "    scores = []\n",
    "    for v in G.nodes():\n",
    "        Nv = set(G.neighbors(v))\n",
    "        if order == 2:\n",
    "            for u in G.neighbors(v):\n",
    "                Nv = Nv.union(set(G.neighbors(u)))\n",
    "        k_NN = set(top_k_neighbors[v])\n",
    "        overlap = len(Nv & k_NN)\n",
    "        normalization = min(len(Nv), k)\n",
    "        scores.append(overlap/normalization)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c1ebf-6e5d-46d6-9651-a04a0bbde73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_keys = [\"facebook\", \"citation\", \"biological\", \"proteins\", \"spam\"]\n",
    "e_keys = [\"node2vec\", \"LINE\", \"AW\"]\n",
    "order = 1\n",
    "k = 5\n",
    "\n",
    "indeces_dict = {\n",
    "    \"LINE\" : \"cosine\",\n",
    "    \"AW\" : \"L2\",\n",
    "    \"node2vec\" : \"L2\"\n",
    "}\n",
    "\n",
    "for ek in e_keys:  \n",
    "    index_type = indeces_dict[ek]\n",
    "    for gk in g_keys:\n",
    "        G = graphs[gk]\n",
    "        E = embeddings[gk][ek]\n",
    "        if  E is not None and E.size != 0:        \n",
    "            score = neighborhood_preservation_score(G, E, k = k, order = order, index_type = index_type)\n",
    "            print(f\"Order {order} neighborhood preservation score of {gk} with {ek}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff13b12-7aff-4551-b0e2-8c96762d98e7",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Using k-means, we cluster the embedding space. We raccomend to use \"facebook\" graph, since it is the dataset where clustering is more meaningful.<br>\n",
    "We can then evaluate the clustering:\n",
    "- It is possible to see a plot of the graph, using networkX, where all the nodes of the same cluster are colored the same.\n",
    "- We compute the modularity of the clustering for an ojbective evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7675c8e-34a3-481e-b3ba-6bb585694f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from networkx.algorithms.community import modularity\n",
    "\n",
    "def cluster_embeddings(embeddings, k=5):\n",
    " \n",
    "    # Normalize embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings = scaler.fit_transform(embeddings) \n",
    "    clusterer = KMeans(n_clusters=k, random_state=21)\n",
    "    # Fit and predict\n",
    "    cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "    # Evaluate clustering with modularity\n",
    "    clustering = []\n",
    "    for i in range(k):\n",
    "        clustering.append(set())  # clustering is a list of sets\n",
    "\n",
    "    for i, cluster in enumerate(cluster_labels):\n",
    "        clustering[cluster].add(i) # add to the set of the cluster the node (which is the index of cluster_labels)\n",
    "\n",
    "    modularity_score = modularity(G, clustering)\n",
    "    return cluster_labels, modularity_score\n",
    "\n",
    "def visualize_clustering(G, embeddings, cluster_labels):\n",
    "    \n",
    "    # Create a color map for clusters\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "    color_map = {label: color for label, color in zip(unique_labels, colors)}\n",
    "\n",
    "    # Assign colors to nodes based on their cluster labels\n",
    "    node_colors = [color_map[cluster_labels[node]] for node in G.nodes()]\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.spring_layout(G, seed=42)  # Use spring layout for visualization\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=20, alpha=0.8)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "    plt.title(\"Graph Clustering Visualization\", fontsize=16)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a3648-a963-4367-b0c7-128c57ce9b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gk = \"facebook\"\n",
    "ek = \"AW\"\n",
    "\n",
    "G = graphs[gk]\n",
    "E = embeddings[gk][ek]\n",
    "cluster_labels, mod = cluster_embeddings(E, k = 8)\n",
    "print(f\"Modularity of clustering: {mod}\")\n",
    "#visualize_embeddings(E, labels=results[\"cluster_labels\"], method=\"tsne\")\n",
    "visualize_clustering(G, E, cluster_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
