{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda4e133-8221-48cb-9c4f-bce54f81dc71",
   "metadata": {},
   "source": [
    "# Learning from networks project: Evaluation of different Node Embedding algorithms\n",
    "Members:<br>\n",
    "- D'Emilio Filippo, id : 2120931\n",
    "- Volpato Pietro, id : 2120825\n",
    "\n",
    "## Embedding evaluation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a643ad2a-470f-4eca-8cb2-1fc209541c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import gzip\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE  # pip install imblearn\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d9ed8-3910-48db-a759-ee09a3c3f673",
   "metadata": {},
   "source": [
    "# configuration\n",
    "Here you can properly configure the names of the graphs and the names of the embedding strategies. Use meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e77bf3ab-83c4-46d5-b007-3982b64bc9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_keys = [\"facebook\",\"citation\",\"biological\",\"proteins\", \"CL\"]\n",
    "embedding_keys = [\"LINE\", \"node2vec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f0b75-4676-40ba-b40b-cf8a0a4d7d49",
   "metadata": {},
   "source": [
    "### Functions temporary container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f9f1f0-5afb-40db-84da-951f2c5dbc92",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_PCA(embeddings, graph_name = \"G\"):\n",
    "    # Reduce dimensions to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.scatter(pca_embeddings[:, 0], pca_embeddings[:, 1], s=10)\n",
    "    plt.title(f\"Visualization in 2D of the embeddings of {graph_name} graph.\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.figsize(5)\n",
    "    plt.show()\n",
    "\n",
    "#plot_PCA(embeddings_facebook[\"LINE\"], \"facebook\")\n",
    "#plot_PCA(embeddings_CL[\"LINE\"], \"CL\")\n",
    "#plot_PCA(embeddings_biological[\"LINE\"], \"biological\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e598ebc-0beb-4076-abc7-b185f437fb7e",
   "metadata": {},
   "source": [
    "# Loading the embeddings\n",
    "Now we load the embeddings, which should be stored as a file in the /embeddings folder as a .npy file.<br>\n",
    "*NOTE*: the file names must respect the format: \"embeddings_{graph_key}_{embedding_key}.npy\".<br>\n",
    "Embeddings are stored in a dictionary of dictionaries.<br>\n",
    "The first index refer to the graph (e.g. embeddings[\"facebook\"] contains the embeddings of the facebook graph for every embedding technique).<br>\n",
    "The second index refer to the embedding technique (e.g. embeddings[\"facebook\"][\"LINE\"] cointans the embedding of facebook graph computed using LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2ab0adc4-e065-4bc3-8296-39ca7a42eca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File '../result/embeddings_citation_node2vec.npy' not found.\n",
      "Warning: File '../result/embeddings_CL_node2vec.npy' not found.\n"
     ]
    }
   ],
   "source": [
    "def load(name):\n",
    "    \"\"\"\n",
    "    Loads a NumPy array from a file. If the file is not found, \n",
    "    displays a warning and returns None.\n",
    "\n",
    "    name (str): The name of the file (without extension) to load from the 'embeddings' directory.\n",
    "    \n",
    "    return: np.ndarray or None: The loaded NumPy array, or None if the file is not found.\n",
    "    \"\"\"\n",
    "    file_name = f\"../result/{name}.npy\"\n",
    "    if not os.path.exists(file_name):\n",
    "        print(f\"Warning: File '{file_name}' not found.\")\n",
    "        return None\n",
    "\n",
    "    emb = np.load(file_name)\n",
    "    return emb\n",
    "\n",
    "embeddings = {}\n",
    "for k in graph_keys:\n",
    "    embeddings[k] = {}\n",
    "\n",
    "for graph_key in graph_keys:\n",
    "    for emb_key in embedding_keys:\n",
    "        emb_key.lower()\n",
    "        graph_key.lower()\n",
    "        s = f\"embeddings_{graph_key}_{emb_key}\"\n",
    "        embeddings[graph_key][emb_key] = load(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0573ed-f2fa-4226-88f4-ebbcf1c470e4",
   "metadata": {},
   "source": [
    "# Loading the graphs\n",
    "Selected graphs:\n",
    "- Facebook_combined    https://snap.stanford.edu/data/ego-Facebook.html          \n",
    "- cit-Helpth           https://networkrepository.com/cit-HepTh.php             \n",
    "- bio-CE-CX            https://networkrepository.com/bio-CE-CX.php             \n",
    "- proteins-full        https://networkrepository.com/PROTEINS-full.php ---- the graph has node labels\n",
    "- COX2-MD              https://networkrepository.com/COX2-MD.php  ---- the graph has node labels\n",
    "\n",
    "To run this notebook, adjust the paths to match where the files are saved in your PC.<br>\n",
    "To keep paths as they are, download the repository.<br>\n",
    "Graphs are stored as a dictionary: the key is the graph name, the value is the corresponding netowrkx graph.<br>\n",
    "\n",
    "When it is created a networkX graph from a text file the node are renamed as integers form 0 to |V|-1, so that we can store the embeddings\n",
    "on a matrix, and each row index corresponds to the embedding vector of the corrisponding node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "067626d5-62a8-42a3-bf77-6952d5af8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_path = '../data/facebook/facebook_combined.txt.gz'\n",
    "citation_path = '../data/citation/cit-HepTh.edges'\n",
    "biological_path = '../data/biological/bio-CE-CX.edges'\n",
    "proteins_path = \"../data/proteins/PROTEINS-full.edges\"\n",
    "CL_path = \"../data/CL-100K-1d8-L9.edges\"\n",
    "\n",
    "proteins_labels_path = \"../data/proteins/PROTEINS-full.node_labels\"\n",
    "CL_labels_path = \"../data/CL-100K-1d8-L9.node_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b13a0738-853b-40ab-a2fc-2f7d9c0a24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_edges(path):\n",
    "    \"\"\"\n",
    "    For files with extension .edges\n",
    "    nodes are renamed as integers, starting from 0\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('%'):  # Skip comment lines\n",
    "                continue\n",
    "            # Split the line based on spaces or commas\n",
    "            data = re.split(r'[,\\s]+', line.strip())\n",
    "            if len(data) < 2:  # Skip lines that don't have at least two columns\n",
    "                continue\n",
    "            # Extract the first two columns (nodes)\n",
    "            node1, node2 = int(data[0]), int(data[1])\n",
    "            G.add_edge(node1, node2)\n",
    "        \n",
    "    return relabel_get_mapping(G)\n",
    "\n",
    "def load_graph_with_gz(path):\n",
    "    \"\"\"\n",
    "    For files with extension .txt.gz\n",
    "    nodes are renamed as integers, starting from 0\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            node1, node2 = map(int, line.strip().split())\n",
    "            G.add_edge(node1, node2)\n",
    "            \n",
    "    return relabel_get_mapping(G)\n",
    "\n",
    "def print_graphs_info(graphs):\n",
    "    for k in graph_keys:\n",
    "        G = graphs[k]\n",
    "        print(f\"{k}: |V|={len(G.nodes)}, |E|={len(G.edges)}\")\n",
    "\n",
    "def relabel_get_mapping(G):\n",
    "    \"\"\"\n",
    "    Given a graph G, this function returns a graph where the nodes are relabeled as integers, form 0 to |V|-1.\n",
    "    It is also returned the mapping from original name to relabeled name.\n",
    "    \"\"\"\n",
    "    inverse_mapping = {i : node for i,node in enumerate(G.nodes)} # mappoing new_name : original_name\n",
    "    direct_mapping = {node : i for i,node in enumerate(G.nodes)} # mapping original_name : new_name\n",
    "    G = nx.relabel_nodes(G, direct_mapping)\n",
    "    return G, direct_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1b659245-9c28-4968-8261-b5a05ea83ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook: |V|=4039, |E|=88234\n",
      "citation: |V|=22908, |E|=2444798\n",
      "biological: |V|=15229, |E|=245952\n",
      "proteins: |V|=43471, |E|=81049\n",
      "CL: |V|=92482, |E|=436611\n"
     ]
    }
   ],
   "source": [
    "graphs = {}  # dictionary containg the graphs\n",
    "mappings = {} # dictionary to contain the mappings. Original name : relabeled name\n",
    "for k in graph_keys:\n",
    "    mappings[k] = {}\n",
    "    \n",
    "# facebook graph is the only one .tar.gz        \n",
    "graphs[graph_keys[0]], mappings[graph_keys[0]] = load_graph_with_gz(facebook_path)  # relabeling nodes to integer\n",
    "graphs[graph_keys[1]], mappings[graph_keys[1]] = load_graph_edges(citation_path)\n",
    "graphs[graph_keys[2]], mappings[graph_keys[2]] = load_graph_edges(biological_path)\n",
    "graphs[graph_keys[3]], mappings[graph_keys[3]] = load_graph_edges(proteins_path)  # node labeled\n",
    "graphs[graph_keys[4]], mappings[graph_keys[4]] = load_graph_edges(CL_path)  # node labeled\n",
    "\n",
    "print_graphs_info(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbf757-a870-4245-a68c-acfc79b39e06",
   "metadata": {},
   "source": [
    "# Reconstruction error\n",
    "In order to avoid memory issues because of the allocation of very large adjacency matrices, it is computed the reconstruction error of the graph by comparing cosine similarity only for existing edges in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0180ec5d-ce56-48ce-93af-06814b1b4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reconstruction error data structure is built as the embeddings data structure.\n",
    "RE = {}\n",
    "for k in graph_keys:\n",
    "    RE[k] = {}\n",
    "\n",
    "def reconstruction_error(G, embeddings):\n",
    "    \"\"\"\n",
    "    Computes the reconstruction error of the graph by comparing cosine similarity\n",
    "    only for existing edges in the graph, avoiding dense adjacency matrix computations.\n",
    "    The reason is that for large graphs an exact computation causes memory issues, due to very large adjacency matrices.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph): The input graph.\n",
    "        embeddings (NumPy array): numpy array containing the embeddings, each row is a node embedding\n",
    "\n",
    "    Returns:\n",
    "        float: The reconstruction error as the average squared difference for existing edges.\n",
    "    \"\"\"\n",
    "    # Convert embeddings to matrix\n",
    "    embedding_vectors = np.array([embeddings[node] for node in G.nodes])\n",
    "\n",
    "    # Compute similarities only for existing edges\n",
    "    error = 0\n",
    "    for u, v in G.edges():\n",
    "        u_vec = embedding_vectors[u].reshape(1, -1)\n",
    "        v_vec = embedding_vectors[v].reshape(1, -1)\n",
    "        sim = cosine_similarity(u_vec, v_vec)[0, 0]\n",
    "        error += (1 - sim) ** 2\n",
    "\n",
    "    return error / G.number_of_edges()\n",
    "\n",
    "def print_reconstruction_error(err, graph_name , embedding_technique):\n",
    "    print(f\"RE of {graph_name} graph using {embedding_technique}: {err}\")\n",
    "\n",
    "def compute_all_reconstruction_errors(graph_keys, embedding_keys, show_results = True):\n",
    "    for graph_key in graph_keys:\n",
    "        if show_results:\n",
    "            print(f\"\\nReconstruction errors for {graph_key} graph:\\n\")\n",
    "        for emb_key in embedding_keys:     \n",
    "            RE[graph_key][emb_key]= reconstruction_error(graphs[graph_key], embeddings[graph_key][emb_key])\n",
    "            if show_results:\n",
    "                print_reconstruction_error(RE[graph_key][emb_key], graph_key, emb_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680dc3f3-8fa9-4563-b910-bf10da750a29",
   "metadata": {},
   "source": [
    "## Compute the RE\n",
    "Here you can compute the reconstruction error.<br>\n",
    "- Set graph_keys_RE with the keys of the graphs you are interested. graph_keys_RE = graph_keys for all graphs.<br>\n",
    "- set embedding_keys_RE with the keys of the embedding strategies you are interested. graph_keys_RE = embedding_keys for all embedding strategies.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "507c10ab-a648-4469-82b7-2ad908c1c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reconstruction errors for proteins graph:\n",
      "\n",
      "RE of proteins graph using LINE: 0.08742006770010227\n",
      "RE of proteins graph using node2vec: 0.419693647047861\n"
     ]
    }
   ],
   "source": [
    "graph_keys_RE = [\"proteins\"]\n",
    "embedding_keys_RE = [\"LINE\", \"node2vec\"]\n",
    "compute_all_reconstruction_errors(graph_keys_RE, embedding_keys_RE, show_results = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce44b9d-c39d-4c41-8d31-fbca41859cb6",
   "metadata": {},
   "source": [
    "# NODE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e7d4b-21ca-4493-8e33-5f00dcdad0ab",
   "metadata": {},
   "source": [
    "## EXTRACT THE DATASET\n",
    "The embeddings (features) are already well formed, and were loaded form the .npy files in the \"result\" folder.<br>\n",
    "To extract the labels some considerations are needed. In the text files containing the labels each line has only one number (class label), and such element refers implicitly to the line number node, according to the original node names definition.<br>\n",
    "Since when we load a graph we rename node as integers starting from 0, the mapping is applied to match each label with the correct node according\n",
    "to the modified node names.<br>\n",
    "\n",
    "NOTE: since isolated nodes are not present in the graph (leaded from .edges file), they are skippend, hence they label is not considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7c02b2a2-b8e9-491a-8f00-280af4c90900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_node_labels(file_path, graph_key):\n",
    "    \"\"\"\n",
    "    Reads a file containing node labels and returns a dictionary mapping nodes to labels.\n",
    "    The labels are assumed to be listed in sequential order: first label is relative to first node (according to original name), and so on...\n",
    "\n",
    "    Parameters: file_path (str): Path to the node label file.\n",
    "                graph_key : key of the graph, needed for the mappings between original and renamed nodes.\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "        node_labels (dict): A dictionary where keys are oroginal node names, values are their corresponding labels.\n",
    "    \"\"\"\n",
    "    node_labels = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        i = 1 # keep track of incedes of original names because bad CL fata format\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "\n",
    "            # i is the node original name, but labels skip nodes that are isolated\n",
    "            map = mappings[graph_key]\n",
    "            while i not in map.keys():  # skip isolated nodes: they don't have label in CL data\n",
    "                i += 1\n",
    "            \n",
    "            real_name = map[i]\n",
    "            label = int(line)  # Parse the labels\n",
    "            node_labels[real_name] = label  # Map the line number (node ID) to the label\n",
    "            i += 1\n",
    "\n",
    "    return node_labels\n",
    "\n",
    "labels = {}\n",
    "labels[\"proteins\"] = load_node_labels(proteins_labels_path, \"proteins\")\n",
    "labels[\"CL\"] = load_node_labels(CL_labels_path, \"CL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c3296b-c090-4ac0-8121-1f5e2b1871c7",
   "metadata": {},
   "source": [
    "## Analysis of the datasets\n",
    "It is always a good idea to have a look at the datasets we are dealing with.<br>\n",
    "- features are the embeddings of the nodes, which is an D-dimensional vector, where D is the dimension of the specific embeddings\n",
    "we are using to training.\n",
    "- the labels represent the classes of the nodes. We can analyze the labels set to see how many different classes there are and understand how balanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "87b8e962-b3fe-452a-9174-38adb7b05953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO DATASET PROTEINS GRAPH\n",
      "Number of samples: 43471\n",
      "Number of classes: 3\n",
      "Samples of class 0: 21151\n",
      "Samples of class 1: 20931\n",
      "Samples of class 2: 1389\n",
      "INFO DATASET CL GRAPH\n",
      "Number of samples: 92482\n",
      "Number of classes: 9\n",
      "Samples of class 8: 10276\n",
      "Samples of class 9: 10274\n",
      "Samples of class 2: 10276\n",
      "Samples of class 6: 10276\n",
      "Samples of class 1: 10276\n",
      "Samples of class 3: 10276\n",
      "Samples of class 5: 10276\n",
      "Samples of class 4: 10276\n",
      "Samples of class 7: 10276\n"
     ]
    }
   ],
   "source": [
    "def analyze_labels(labels):\n",
    "    labels_count = {}\n",
    "    for label in labels.values():\n",
    "        if not label in labels_count.keys():\n",
    "            labels_count[label] = 0\n",
    "        labels_count[label] += 1\n",
    "    print(f\"Number of samples: {len(labels)}\")\n",
    "    print(f\"Number of classes: {len(labels_count)}\")\n",
    "    for label in labels_count.keys():\n",
    "        print(f\"Samples of class {label}: {labels_count[label]}\")\n",
    "\n",
    "print(\"INFO DATASET PROTEINS GRAPH\")\n",
    "analyze_labels(labels[\"proteins\"])\n",
    "\n",
    "print(\"INFO DATASET CL GRAPH\")\n",
    "analyze_labels(labels[\"CL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b989ec4-f073-47ce-a580-41c7faa33c22",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "A support vector machine is trained from the dataset provided in input.<br>\n",
    "Class weights are used to handle the case of an imbalance dataset (some classes have considerably less examples than others).<br>\n",
    "The model is trained on 80% of the dataset (training set), while 20% of the dataset (test set) is left to make predictions and evaluate the performances of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "982501d4-c3d3-4f9c-bdc6-0e15d6e56438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SVM(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Train and evaluate an SVM classifier for multi-class node classification.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings (np.ndarray): Numpy array where each row is a node's embedding.\n",
    "        labels (dict): Dictionary mapping node indices to their labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with accuracy, F1 score, and a detailed classification report.\n",
    "    \"\"\"\n",
    "    # Ensure X (features) and y (labels) are aligned\n",
    "    X = np.array(embeddings)  # Node embeddings\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)  # normalization\n",
    "    y = np.array([labels[i] for i in range(len(labels))])  # Ensure correct ordering of labels\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "    #smote = SMOTE(random_state=42) # (Synthetic Minority Oversampling Technique) \n",
    "    #X_train, y_train = smote.fit_resample(X_train, y_train)  #resampling to handle imbalance dataset\n",
    "\n",
    "    clf = SVC(kernel='rbf', class_weight = 'balanced', decision_function_shape='ovo')  # 'ovo' = one-vs-one for multi-class\n",
    "    #clf = BalancedRandomForestClassifier(n_estimators=100, random_state=42, sampling_strategy = 'all', bootstrap = False, replacement = True)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(\"SVM Classifier Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1-Score: {f1:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\"accuracy\": accuracy, \"macro_f1\": f1, \"report\": report}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f07958-6c62-422a-83a6-885c2df44f53",
   "metadata": {},
   "source": [
    "## Chose the graph and the embedding strategy to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d32ca218-3478-478e-8365-dba1092d8c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Results:\n",
      "Accuracy: 0.1123\n",
      "Macro F1-Score: 0.1123\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.10      0.11      0.11      2006\n",
      "           2       0.12      0.12      0.12      2099\n",
      "           3       0.11      0.11      0.11      1985\n",
      "           4       0.10      0.11      0.10      2041\n",
      "           5       0.12      0.13      0.13      2023\n",
      "           6       0.11      0.10      0.11      2070\n",
      "           7       0.12      0.11      0.11      2063\n",
      "           8       0.12      0.11      0.11      2150\n",
      "           9       0.11      0.10      0.11      2060\n",
      "\n",
      "    accuracy                           0.11     18497\n",
      "   macro avg       0.11      0.11      0.11     18497\n",
      "weighted avg       0.11      0.11      0.11     18497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_key = \"LINE\"\n",
    "graph_key = \"CL\"\n",
    "x = train_SVM(embeddings[graph_key][embedding_key], labels[graph_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634f62e-ad49-480a-a4df-7045dcc42d8a",
   "metadata": {},
   "source": [
    "# LINK PREDICTION\n",
    "The link prediction task consists on training a model, logistic regression in this case, that given as input the features of a pair of nodes predicts whether the edge between them exists in the graph or not.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c7493-a241-40e7-ba76-76663489b710",
   "metadata": {},
   "source": [
    "## PRODUCING THE DATASET\n",
    "We decided to produce a perfectly balanced dataset in this way: all the positive examples are all the pair of nodes (actually the features we decided to use to represent them) for which exists an edge in the graph.<br>\n",
    "The negative examples instead are created by chosing at random a pair of nodes, and if there is no edge between them such pair is a negative example. We repeat this procedure until the number of negative examples matches the number of positive examples<br>\n",
    "\n",
    "We defined 3 possible choices for the features representing pair of nodes:\n",
    "- The concatenation of the embeddings of the two nodes\n",
    "- The concatenation of the embeddings + the cosine similarity between those (a scalar)\n",
    "- Just the scalar cosine similarity between the two embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3404fdd9-3796-448e-8caa-e9958d04d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_link_prediction_dataset(G, embeddings, add_cosine_sim = True):\n",
    "    \"\"\"\n",
    "    Creates a dataset for link prediction. if add_cosine_sim = False, the features are the concatenation\n",
    "    between the embedding of pair on nodes, if add_cosine_sim = True it is concatenated in addition the\n",
    "    cosine similarity between the two embeddings (1 additional feature)\n",
    "\n",
    "    Parameters:\n",
    "        G (nx.Graph): The input graph.\n",
    "        embeddings (np.ndarray): Node embeddings as a numpy array (indexed by node ID).\n",
    "        add_cosine_sim: if True the cosine similarity is concatenated to the embeddings to produce the features\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Feature pairs with cosine similarity as an additional feature.\n",
    "        y (np.ndarray): Labels (1 for existing edges, 0 for non-existing edges).\n",
    "    \"\"\"\n",
    "    positive_edges = list(G.edges())\n",
    "    num_positive = len(positive_edges)\n",
    "\n",
    "    # Generate negative edges\n",
    "    nodes = list(G.nodes())\n",
    "    negative_edges = set()\n",
    "    while len(negative_edges) < num_positive:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not G.has_edge(u, v) and (u, v) not in negative_edges and (v, u) not in negative_edges:\n",
    "            negative_edges.add((u, v))\n",
    "\n",
    "    # Combine positive and negative edges\n",
    "    all_edges = positive_edges + list(negative_edges)\n",
    "    labels = [1] * len(positive_edges) + [0] * len(negative_edges)\n",
    "\n",
    "    # Create features from embeddings\n",
    "    X = []\n",
    "    for u, v in all_edges:\n",
    "        # Compute cosine similarity\n",
    "        u_emb = embeddings[u].reshape(1, -1)\n",
    "        v_emb = embeddings[v].reshape(1, -1)\n",
    "        if add_cosine_sim:         # Concatenate embeddings and cosine similarity\n",
    "            cosine_sim = cosine_similarity(u_emb, v_emb)[0, 0]\n",
    "            feature = np.concatenate([embeddings[u], embeddings[v], [cosine_sim]])\n",
    "        else:\n",
    "            feature = np.concatenate([embeddings[u], embeddings[v]])\n",
    "        X.append(feature)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "288c487a-2dd1-4561-b97d-f60cce7f36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_link_prediction_dataset_only_cosine_similarity(G, embeddings):\n",
    "    \"\"\"\n",
    "    Creates a dataset for link prediction, where each sample has a single feature (scalar), which is the cosine similarity\n",
    "    between the two nodes adjacent to the edge.\n",
    "    \"\"\"\n",
    "    positive_edges = list(G.edges())\n",
    "    num_positive = len(positive_edges)\n",
    "\n",
    "    # Generate negative edges\n",
    "    nodes = list(G.nodes())\n",
    "    negative_edges = set()\n",
    "    while len(negative_edges) < num_positive:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not G.has_edge(u, v) and (u, v) not in negative_edges and (v, u) not in negative_edges:\n",
    "            negative_edges.add((u, v))\n",
    "\n",
    "    # Combine positive and negative edges\n",
    "    all_edges = positive_edges + list(negative_edges)\n",
    "    labels = [1] * len(positive_edges) + [0] * len(negative_edges)\n",
    "\n",
    "    # Create features from embeddings\n",
    "    cosine_similarities = []\n",
    "    for u, v in all_edges:\n",
    "        # Compute cosine similarity\n",
    "        u_emb = embeddings[u].reshape(1, -1)\n",
    "        v_emb = embeddings[v].reshape(1, -1)\n",
    "        cosine_sim = cosine_similarity(u_emb, v_emb)[0, 0]\n",
    "        cosine_similarities.append(cosine_sim)\n",
    "\n",
    "    X = np.array(cosine_similarities).reshape(-1, 1)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba97a2-b3e4-4f42-ac7f-40230f913e5d",
   "metadata": {},
   "source": [
    "## TRAIN LOGISTIC REGRESSION CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f2475125-54a2-426e-928d-06c883a5d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_link_prediction(X,y):\n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier for link prediction\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)  # normalization\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "    # Train logistic regression model\n",
    "    clf = LogisticRegression(max_iter = 1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"ROC AUC Score:\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3322c1d5-2ffb-40ca-a377-4079d41d5ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5014188772362739\n",
      "ROC AUC Score: 0.5044029397798646\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.54      0.52     16178\n",
      "           1       0.50      0.46      0.48     16242\n",
      "\n",
      "    accuracy                           0.50     32420\n",
      "   macro avg       0.50      0.50      0.50     32420\n",
      "weighted avg       0.50      0.50      0.50     32420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_key = \"node2vec\"\n",
    "graph_key = \"proteins\"\n",
    "X,y = create_link_prediction_dataset_only_cosine_similarity(graphs[graph_key], embeddings[graph_key][embedding_key])\n",
    "LR_link_prediction(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cd52e6ca-5bd0-4972-80cc-a14ebf09c56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e67c1-baf0-4182-8ab2-7b0d6850bbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
