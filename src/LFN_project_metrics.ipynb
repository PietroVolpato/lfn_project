{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda4e133-8221-48cb-9c4f-bce54f81dc71",
   "metadata": {},
   "source": [
    "# Learning from networks project: Evaluation of different Node Embedding algorithms\n",
    "Members:<br>\n",
    "- D'Emilio Filippo, id : 2120931\n",
    "- Volpato Pietro, id : 2120825\n",
    "\n",
    "## Embedding evaluation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a643ad2a-470f-4eca-8cb2-1fc209541c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import gzip\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE  # pip install imblearn\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d9ed8-3910-48db-a759-ee09a3c3f673",
   "metadata": {},
   "source": [
    "# configuration\n",
    "Here you can properly configure the names of the graphs and the names of the embedding strategies. Use meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77bf3ab-83c4-46d5-b007-3982b64bc9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_keys = [\"facebook\",\"citation\",\"biological\",\"CL\",\"COX2\"]\n",
    "embedding_keys = [\"LINE\", \"node2vec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f0b75-4676-40ba-b40b-cf8a0a4d7d49",
   "metadata": {},
   "source": [
    "### Functions temporary container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f9f1f0-5afb-40db-84da-951f2c5dbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PCA(embeddings, graph_name = \"G\"):\n",
    "    # Reduce dimensions to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.scatter(pca_embeddings[:, 0], pca_embeddings[:, 1], s=10)\n",
    "    plt.title(f\"Visualization in 2D of the embeddings of {graph_name} graph.\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.figsize(5)\n",
    "    plt.show()\n",
    "\n",
    "#plot_PCA(embeddings_facebook[\"LINE\"], \"facebook\")\n",
    "#plot_PCA(embeddings_CL[\"LINE\"], \"CL\")\n",
    "#plot_PCA(embeddings_biological[\"LINE\"], \"biological\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e598ebc-0beb-4076-abc7-b185f437fb7e",
   "metadata": {},
   "source": [
    "# Loading the embeddings\n",
    "Now we load the embeddings, which should be stored as a file in the /embeddings folder as a .npy file.<br>\n",
    "*NOTE*: the file names must respect the format: \"embeddings_{graph_key}_{embedding_key}.npy\".<br>\n",
    "Embeddings are stored in a dictionary of dictionaries.<br>\n",
    "The first index refer to the graph (e.g. embeddings[\"facebook\"] contains the embeddings of the facebook graph for every embedding technique).<br>\n",
    "The second index refer to the embedding technique (e.g. embeddings[\"facebook\"][\"LINE\"] cointans the embedding of facebook graph computed using LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ab0adc4-e065-4bc3-8296-39ca7a42eca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File '../result/embeddings_citation_node2vec.npy' not found.\n",
      "Warning: File '../result/embeddings_CL_node2vec.npy' not found.\n"
     ]
    }
   ],
   "source": [
    "def load(name):\n",
    "    \"\"\"\n",
    "    Loads a NumPy array from a file. If the file is not found, \n",
    "    displays a warning and returns None.\n",
    "\n",
    "    name (str): The name of the file (without extension) to load from the 'embeddings' directory.\n",
    "    \n",
    "    return: np.ndarray or None: The loaded NumPy array, or None if the file is not found.\n",
    "    \"\"\"\n",
    "    file_name = f\"../result/{name}.npy\"\n",
    "    if not os.path.exists(file_name):\n",
    "        print(f\"Warning: File '{file_name}' not found.\")\n",
    "        return None\n",
    "\n",
    "    emb = np.load(file_name)\n",
    "    return emb\n",
    "\n",
    "embeddings = {}\n",
    "for k in graph_keys:\n",
    "    embeddings[k] = {}\n",
    "\n",
    "for graph_key in graph_keys:\n",
    "    for emb_key in embedding_keys:\n",
    "        emb_key.lower()\n",
    "        graph_key.lower()\n",
    "        s = f\"embeddings_{graph_key}_{emb_key}\"\n",
    "        embeddings[graph_key][emb_key] = load(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0573ed-f2fa-4226-88f4-ebbcf1c470e4",
   "metadata": {},
   "source": [
    "# Loading the graphs\n",
    "Selected graphs:\n",
    "- Facebook_combined    https://snap.stanford.edu/data/ego-Facebook.html          \n",
    "- cit-Helpth           https://networkrepository.com/cit-HepTh.php             \n",
    "- bio-CE-CX            https://networkrepository.com/bio-CE-CX.php             \n",
    "- CL-100K-1d8-L9       https://networkrepository.com/CL-100K-1d8-L9.php ---- the graph has node labels\n",
    "- COX2-MD              https://networkrepository.com/COX2-MD.php  ---- the graph has node labels\n",
    "\n",
    "To run this notebook, adjust the paths to match where the files are saved in your PC.<br>\n",
    "To keep paths as they are, create a \"data\" folder inside the directory of this notebook, and store the files there.<br><br>\n",
    "\n",
    "Graphs are stored as a dictionary: the key is the graph name, the value is the corresponding netowrkx graph.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "067626d5-62a8-42a3-bf77-6952d5af8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_path = '../data/facebook_combined.txt.gz'\n",
    "citation_path = '../data/cit-HepTh.edges'\n",
    "biological_path = '../data/bio-CE-CX.edges'\n",
    "CL_path = \"../data/CL-100K-1d8-L9.edges\"\n",
    "COX2_path = \"../data/COX2-MD.edges\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13a0738-853b-40ab-a2fc-2f7d9c0a24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(path):\n",
    "    \"\"\"\n",
    "    For files with extension .edges\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('%'):  # Skip comment lines\n",
    "                continue\n",
    "            # Split the line based on spaces or commas\n",
    "            data = re.split(r'[,\\s]+', line.strip())\n",
    "            if len(data) < 2:  # Skip lines that don't have at least two columns\n",
    "                continue\n",
    "            # Extract the first two columns (nodes)\n",
    "            node1, node2 = int(data[0]), int(data[1])\n",
    "            G.add_edge(node1, node2)\n",
    "        \n",
    "    return relabel_get_mapping(G)\n",
    "\n",
    "def load_graph_with_gz(path):\n",
    "    \"\"\"\n",
    "    For files with extension .txt.gz\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            node1, node2 = map(int, line.strip().split())\n",
    "            G.add_edge(node1, node2)\n",
    "            \n",
    "    return relabel_get_mapping(G)\n",
    "\n",
    "def print_graphs_info(graphs):\n",
    "    for k in graph_keys:\n",
    "        G = graphs[k]\n",
    "        print(f\"{k}: |V|={len(G.nodes)}, |E|={len(G.edges)}\")\n",
    "\n",
    "def relabel_get_mapping(G):\n",
    "    \"\"\"\n",
    "    Given a graph G, this function returns a graph where the nodes are relabeled as integers, form 0 to |V|-1.\n",
    "    It is also returned the mapping from relabeled name to original name.\n",
    "    \"\"\"\n",
    "    mapping = {node : i for i,node in enumerate(G.nodes)} # mappoing original : relabeled\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    return G, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b659245-9c28-4968-8261-b5a05ea83ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook: |V|=4039, |E|=88234\n",
      "citation: |V|=22908, |E|=2444798\n",
      "biological: |V|=15229, |E|=245952\n",
      "CL: |V|=92482, |E|=436611\n",
      "COX2: |V|=7962, |E|=101542\n"
     ]
    }
   ],
   "source": [
    "graphs = {}  # dictionary containg the graphs\n",
    "mappings = {} # dictionary to contain the mappings. Original name : relabeled name\n",
    "for k in graph_keys:\n",
    "    mappings[k] = {}\n",
    "    \n",
    "# facebook graph is the only one .tar.gz        \n",
    "graphs[graph_keys[0]], mappings[graph_keys[0]] = load_graph_with_gz(facebook_path)  # relabeling nodes to integer\n",
    "graphs[graph_keys[1]], mappings[graph_keys[1]] = load_graph(citation_path)\n",
    "graphs[graph_keys[2]], mappings[graph_keys[2]] = load_graph(biological_path)\n",
    "graphs[graph_keys[3]], mappings[graph_keys[3]] = load_graph(CL_path)  # node labeled\n",
    "graphs[graph_keys[4]], mappings[graph_keys[4]] = load_graph(COX2_path)  # node labeled\n",
    "\n",
    "print_graphs_info(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbf757-a870-4245-a68c-acfc79b39e06",
   "metadata": {},
   "source": [
    "# Reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0180ec5d-ce56-48ce-93af-06814b1b4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reconstruction error data structure is built as the embeddings data structure.\n",
    "RE = {}\n",
    "for k in graph_keys:\n",
    "    RE[k] = {}\n",
    "\n",
    "def reconstruction_error(G, embeddings):\n",
    "    \"\"\"\n",
    "    Computes the reconstruction error of the graph by comparing cosine similarity\n",
    "    only for existing edges in the graph, avoiding dense adjacency matrix computations.\n",
    "    The reason is that for large graphs an exact computation causes memory issues, due to very large adjacency matrices.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph): The input graph.\n",
    "        embeddings (NumPy array): numpy array containing the embeddings, each row is a node embedding\n",
    "\n",
    "    Returns:\n",
    "        float: The reconstruction error as the average squared difference for existing edges.\n",
    "    \"\"\"\n",
    "    # Convert embeddings to matrix\n",
    "    embedding_vectors = np.array([embeddings[node] for node in G.nodes])\n",
    "\n",
    "    # Compute similarities only for existing edges\n",
    "    error = 0\n",
    "    for u, v in G.edges():\n",
    "        u_vec = embedding_vectors[u].reshape(1, -1)\n",
    "        v_vec = embedding_vectors[v].reshape(1, -1)\n",
    "        sim = cosine_similarity(u_vec, v_vec)[0, 0]\n",
    "        error += (1 - sim) ** 2\n",
    "\n",
    "    return error / G.number_of_edges()\n",
    "\n",
    "def print_reconstruction_error(err, graph_name , embedding_technique):\n",
    "    print(f\"RE of {graph_name} graph using {embedding_technique}: {err}\")\n",
    "\n",
    "def compute_all_reconstruction_errors(graph_keys, embedding_keys, show_results = True):\n",
    "    for graph_key in graph_keys:\n",
    "        if show_results:\n",
    "            print(f\"\\nReconstruction errors for {graph_key} graph:\\n\")\n",
    "        for emb_key in embedding_keys:     \n",
    "            RE[graph_key][emb_key]= reconstruction_error(graphs[graph_key], embeddings[graph_key][emb_key])\n",
    "            if show_results:\n",
    "                print_reconstruction_error(RE[graph_key][emb_key], graph_key, emb_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680dc3f3-8fa9-4563-b910-bf10da750a29",
   "metadata": {},
   "source": [
    "## Compute the RE\n",
    "Here you can compute the reconstruction error.<br>\n",
    "- Set graph_keys_RE with the keys of the graphs you are interested. graph_keys_RE = graph_keys for all graphs.<br>\n",
    "- set embedding_keys_RE with the keys of the embedding strategies you are interested. graph_keys_RE = embedding_keys for all embedding strategies.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c10ab-a648-4469-82b7-2ad908c1c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_keys_RE = [\"biological\"]\n",
    "embedding_keys_RE = [\"LINE\"]\n",
    "compute_all_reconstruction_errors(graph_keys_RE, embedding_keys_RE, show_results = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce44b9d-c39d-4c41-8d31-fbca41859cb6",
   "metadata": {},
   "source": [
    "# NODE CLASSIFICATION\n",
    "There are two graphs with node labels: CL graph and COX2 graph.<br>\n",
    "\n",
    "To do node classification we train a SVM. The dataset is composed of the embeddings of one of those two graphs, obtained with one of the embeddings algorithms considered, and of the node labels (multi-class classification task).\n",
    "\n",
    "The node classification consider the TRANSDUCTIVE CASE: some embeddings-labels are kept for the test set, but to compute the embeddings we considered\n",
    "the whole graph.\n",
    "\n",
    "IMPORTANT: the lables of the \"CL graph\" are organized in a very weird way: the nodes of the graph are not sequential numbers (e.g. some numbers for some reason are missing), but the labels are, and they don't respect the names of the original nodes. <br>\n",
    "For example, in the label file there is the entry 16,2 (node 16 has label 2), but in the original .edges file node 16 does not even exist. I assume that it means \"the 16th node has label 2\". <br>\n",
    "The function to extract node labels then assume that the labels are listed in sequential order (e.g. entry 1 is the label of first node, entry 2 of the second node, ecc..). This requirement is satisfied for both the labeled graphs we are considering. <br>\n",
    "We originally wanted to apply the mapping to the node-label pair, to correctly match the labels with the renamed nodes, but unfortunately because of this strange representation of the data for CL graph this is not possible, and we have to make this precise assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c02b2a2-b8e9-491a-8f00-280af4c90900",
   "metadata": {},
   "outputs": [],
   "source": [
    "COX2_labels_path = \"../data/COX2-MD.node_labels\"\n",
    "CL_labels_path = \"../data/CL-100K-1d8-L9.node_labels\"\n",
    "\n",
    "def load_node_labels(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file containing node labels and returns a dictionary mapping nodes to labels.\n",
    "    The file can have two formats:\n",
    "        1. Comma-separated: node,label\n",
    "        2. Space-separated: node label\n",
    "        \n",
    "    IMPORTANT: this function assumes that labels starts from node '1' and each line\n",
    "    represents the label of the nodes sequantially (e.g. line 1 label of first node,\n",
    "    line 2 label of second node, ecc...).\n",
    "    The motivation is given in the markdown cell above.\n",
    "    Parameters: file_path : Path to the node label file.\n",
    "    \n",
    "    Return: A dictionary where keys are node IDs and values are labels.\n",
    "    \"\"\"\n",
    "    node_labels = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "\n",
    "            if ',' in line:\n",
    "                node, label = line.split(',')  # Comma-separated\n",
    "            else:\n",
    "                node, label = line.split()  # Space-separated\n",
    "            \n",
    "            node_labels[int(node)] = int(label)\n",
    "\n",
    "    # labels has to start from index 0, like embeddings. In the considered files labels starts from 1 and proceed in order\n",
    "    node_labels = {int(node)-1 :label for node, label in node_labels.items()} \n",
    "    return node_labels\n",
    "\n",
    "def train_SVM(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Train and evaluate an SVM classifier for multi-class node classification.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings (np.ndarray): Numpy array where each row is a node's embedding.\n",
    "        labels (dict): Dictionary mapping node indices to their labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with accuracy, F1 score, and a detailed classification report.\n",
    "    \"\"\"\n",
    "    # Ensure X (features) and y (labels) are aligned\n",
    "    X = np.array(embeddings)  # Node embeddings\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)  # normalization\n",
    "    y = np.array([labels[i] for i in range(len(labels))])  # Ensure correct ordering of labels\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "    #smote = SMOTE(random_state=42) # (Synthetic Minority Oversampling Technique) \n",
    "    #X_train, y_train = smote.fit_resample(X_train, y_train)  #resampling to handle imbalance dataset\n",
    "\n",
    "    #clf = SVC(kernel='rbf', class_weight = 'balanced', decision_function_shape='ovo')  # 'ovo' = one-vs-one for multi-class\n",
    "    clf = BalancedRandomForestClassifier(n_estimators=100, random_state=42, sampling_strategy = 'all', bootstrap = False, replacement = True)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(\"SVM Classifier Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1-Score: {f1:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\"accuracy\": accuracy, \"macro_f1\": f1, \"report\": report}\n",
    "\n",
    "labels = {}\n",
    "labels[\"COX2\"] = load_node_labels(COX2_labels_path)\n",
    "labels[\"CL\"] = load_node_labels(CL_labels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146db805-4e7a-464b-8a5a-a46d8de3aff6",
   "metadata": {},
   "source": [
    "# Analysis of the datasets\n",
    "It is always a good idea to have a look at the datasets we are dealing with.<br>\n",
    "- features are the embeddings of the nodes, which is an D-dimensional vector, where D is the dimension of the specific embeddings\n",
    "we are using to training.\n",
    "- the labels represent the classes of the nodes. We can analyze the labels set to see how many different classes there are and understand how balanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87b8e962-b3fe-452a-9174-38adb7b05953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO DATASET COX2 GRAPH\n",
      "Number of samples: 7962\n",
      "Number of classes: 7\n",
      "Samples of class 1: 5575\n",
      "Samples of class 2: 599\n",
      "Samples of class 3: 571\n",
      "Samples of class 4: 354\n",
      "Samples of class 5: 767\n",
      "Samples of class 6: 89\n",
      "Samples of class 7: 7\n",
      "\n",
      "INFO DATASET CL GRAPH\n",
      "Number of samples: 92482\n",
      "Number of classes: 9\n",
      "Samples of class 8: 10276\n",
      "Samples of class 9: 10274\n",
      "Samples of class 2: 10276\n",
      "Samples of class 6: 10276\n",
      "Samples of class 1: 10276\n",
      "Samples of class 3: 10276\n",
      "Samples of class 5: 10276\n",
      "Samples of class 4: 10276\n",
      "Samples of class 7: 10276\n"
     ]
    }
   ],
   "source": [
    "def analyze_labels(labels):\n",
    "    labels_count = {}\n",
    "    for label in labels.values():\n",
    "        if not label in labels_count.keys():\n",
    "            labels_count[label] = 0\n",
    "        labels_count[label] += 1\n",
    "    print(f\"Number of samples: {len(labels)}\")\n",
    "    print(f\"Number of classes: {len(labels_count)}\")\n",
    "    for label in labels_count.keys():\n",
    "        print(f\"Samples of class {label}: {labels_count[label]}\")\n",
    "\n",
    "print(\"INFO DATASET COX2 GRAPH\")\n",
    "analyze_labels(labels[\"COX2\"])\n",
    "\n",
    "print(\"\\nINFO DATASET CL GRAPH\")\n",
    "analyze_labels(labels[\"CL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d32ca218-3478-478e-8365-dba1092d8c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Results:\n",
      "Accuracy: 0.1419\n",
      "Macro F1-Score: 0.0840\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.14      0.24      1111\n",
      "           2       0.06      0.12      0.08       115\n",
      "           3       0.06      0.17      0.09       104\n",
      "           4       0.03      0.09      0.04        75\n",
      "           5       0.09      0.16      0.12       167\n",
      "           6       0.01      0.17      0.02        18\n",
      "           7       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.14      1593\n",
      "   macro avg       0.14      0.12      0.08      1593\n",
      "weighted avg       0.52      0.14      0.19      1593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_key = \"node2vec\"\n",
    "graph_key = \"COX2\"\n",
    "x = train_SVM(embeddings[graph_key][embedding_key], labels[graph_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634f62e-ad49-480a-a4df-7045dcc42d8a",
   "metadata": {},
   "source": [
    "# LINK PREDICTION\n",
    "The link prediction task consists on training a model, logistic regression in this case, that given as input the features of a pair of nodes predicts whether the edge between them exists in the graph or not.<br>\n",
    "\n",
    "THE DATASET: we decided to produce a perfectly balanced dataset in this way: all the positive examples are all the pair of nodes (actually the features we decided to use to represent them) for which exists an edge in the graph.<br>\n",
    "The negative examples instead are created by chosing at random a pair of nodes, and if there is no edge between them such pair is a negative example. We repeat this procedure until the number of negative examples matches the number of positive examples<br>\n",
    "\n",
    "THE FEATURES: we defined 3 possible choices for the features representing pair of nodes:\n",
    "- The concatenation of the embeddings of the two nodes\n",
    "- The concatenation of the embeddings + the cosine similarity between those (a scalar)\n",
    "- Just the scalar cosine similarity between the two embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3404fdd9-3796-448e-8caa-e9958d04d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_link_prediction_dataset(G, embeddings, add_cosine_sim = True):\n",
    "    \"\"\"\n",
    "    Creates a dataset for link prediction. if add_cosine_sim = False, the features are the concatenation\n",
    "    between the embedding of pair on nodes, if add_cosine_sim = True it is concatenated in addition the\n",
    "    cosine similarity between the two embeddings (1 additional feature)\n",
    "\n",
    "    Parameters:\n",
    "        G (nx.Graph): The input graph.\n",
    "        embeddings (np.ndarray): Node embeddings as a numpy array (indexed by node ID).\n",
    "        add_cosine_sim: if True the cosine similarity is concatenated to the embeddings to produce the features\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Feature pairs with cosine similarity as an additional feature.\n",
    "        y (np.ndarray): Labels (1 for existing edges, 0 for non-existing edges).\n",
    "    \"\"\"\n",
    "    positive_edges = list(G.edges())\n",
    "    num_positive = len(positive_edges)\n",
    "\n",
    "    # Generate negative edges\n",
    "    nodes = list(G.nodes())\n",
    "    negative_edges = set()\n",
    "    while len(negative_edges) < num_positive:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not G.has_edge(u, v) and (u, v) not in negative_edges and (v, u) not in negative_edges:\n",
    "            negative_edges.add((u, v))\n",
    "\n",
    "    # Combine positive and negative edges\n",
    "    all_edges = positive_edges + list(negative_edges)\n",
    "    labels = [1] * len(positive_edges) + [0] * len(negative_edges)\n",
    "\n",
    "    # Create features from embeddings\n",
    "    X = []\n",
    "    for u, v in all_edges:\n",
    "        # Compute cosine similarity\n",
    "        u_emb = embeddings[u].reshape(1, -1)\n",
    "        v_emb = embeddings[v].reshape(1, -1)\n",
    "        if add_cosine_sim:         # Concatenate embeddings and cosine similarity\n",
    "            cosine_sim = cosine_similarity(u_emb, v_emb)[0, 0]\n",
    "            feature = np.concatenate([embeddings[u], embeddings[v], [cosine_sim]])\n",
    "        else:\n",
    "            feature = np.concatenate([embeddings[u], embeddings[v]])\n",
    "        X.append(feature)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "288c487a-2dd1-4561-b97d-f60cce7f36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_link_prediction_dataset_only_cosine_similarity(G, embeddings):\n",
    "    \"\"\"\n",
    "    Creates a dataset for link prediction, where each sample has a single feature (scalar), which is the cosine similarity\n",
    "    between the two nodes adjacent to the edge.\n",
    "    \"\"\"\n",
    "    positive_edges = list(G.edges())\n",
    "    num_positive = len(positive_edges)\n",
    "\n",
    "    # Generate negative edges\n",
    "    nodes = list(G.nodes())\n",
    "    negative_edges = set()\n",
    "    while len(negative_edges) < num_positive:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not G.has_edge(u, v) and (u, v) not in negative_edges and (v, u) not in negative_edges:\n",
    "            negative_edges.add((u, v))\n",
    "\n",
    "    # Combine positive and negative edges\n",
    "    all_edges = positive_edges + list(negative_edges)\n",
    "    labels = [1] * len(positive_edges) + [0] * len(negative_edges)\n",
    "\n",
    "    # Create features from embeddings\n",
    "    cosine_similarities = []\n",
    "    for u, v in all_edges:\n",
    "        # Compute cosine similarity\n",
    "        u_emb = embeddings[u].reshape(1, -1)\n",
    "        v_emb = embeddings[v].reshape(1, -1)\n",
    "        cosine_sim = cosine_similarity(u_emb, v_emb)[0, 0]\n",
    "        cosine_similarities.append(cosine_sim)\n",
    "\n",
    "    X = np.array(cosine_similarities).reshape(-1, 1)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f2475125-54a2-426e-928d-06c883a5d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_link_prediction(X,y):\n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier for link prediction\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)  # normalization\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)\n",
    "\n",
    "    # Train logistic regression model\n",
    "    clf = LogisticRegression(max_iter = 1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"ROC AUC Score:\", roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3322c1d5-2ffb-40ca-a377-4079d41d5ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777582591941973\n",
      "ROC AUC Score: 0.9944917035381748\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     17646\n",
      "           1       0.98      0.98      0.98     17648\n",
      "\n",
      "    accuracy                           0.98     35294\n",
      "   macro avg       0.98      0.98      0.98     35294\n",
      "weighted avg       0.98      0.98      0.98     35294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_key = \"LINE\"\n",
    "graph_key = \"facebook\"\n",
    "X,y = create_link_prediction_dataset_only_cosine_similarity(graphs[graph_key], embeddings[graph_key][embedding_key])\n",
    "LR_link_prediction(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a7b6e-8468-4513-9139-6481c2f2eb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
