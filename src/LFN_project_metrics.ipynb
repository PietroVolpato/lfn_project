{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda4e133-8221-48cb-9c4f-bce54f81dc71",
   "metadata": {},
   "source": [
    "# Learning from networks project: Evaluation of different Node Embedding algorithms\n",
    "Members:<br>\n",
    "- D'Emilio Filippo, id : 2120931\n",
    "- Volpato Pietro, id : 2120825\n",
    "\n",
    "## Embedding evaluation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a643ad2a-470f-4eca-8cb2-1fc209541c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import gzip\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d9ed8-3910-48db-a759-ee09a3c3f673",
   "metadata": {},
   "source": [
    "# configuration\n",
    "Here you can properly configure the names of the graphs and the names of the embedding strategies. Use meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77bf3ab-83c4-46d5-b007-3982b64bc9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_keys = [\"facebook\",\"citation\",\"biological\",\"CL\",\"COX2\"]\n",
    "embedding_keys = [\"LINE\", \"node2vec\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f0b75-4676-40ba-b40b-cf8a0a4d7d49",
   "metadata": {},
   "source": [
    "### Functions temporary container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f9f1f0-5afb-40db-84da-951f2c5dbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PCA(embeddings, graph_name = \"G\"):\n",
    "    # Reduce dimensions to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embeddings = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.scatter(pca_embeddings[:, 0], pca_embeddings[:, 1], s=10)\n",
    "    plt.title(f\"Visualization in 2D of the embeddings of {graph_name} graph.\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.figsize(5)\n",
    "    plt.show()\n",
    "\n",
    "#plot_PCA(embeddings_facebook[\"LINE\"], \"facebook\")\n",
    "#plot_PCA(embeddings_CL[\"LINE\"], \"CL\")\n",
    "#plot_PCA(embeddings_biological[\"LINE\"], \"biological\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e598ebc-0beb-4076-abc7-b185f437fb7e",
   "metadata": {},
   "source": [
    "# Loading the embeddings\n",
    "Now we load the embeddings, which should be stored as a file in the /embeddings folder as a .npy file.<br>\n",
    "*NOTE*: the file names must respect the format: \"embeddings_{graph_key}_{embedding_key}.npy\".<br>\n",
    "Embeddings are stored in a dictionary of dictionaries.<br>\n",
    "The first index refer to the graph (e.g. embeddings[\"facebook\"] contains the embeddings of the facebook graph for every embedding technique).<br>\n",
    "The second index refer to the embedding technique (e.g. embeddings[\"facebook\"][\"LINE\"] cointans the embedding of facebook graph computed using LINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab0adc4-e065-4bc3-8296-39ca7a42eca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File '../result/embeddings_citation_node2vec.npy' not found.\n",
      "Warning: File '../result/embeddings_CL_node2vec.npy' not found.\n"
     ]
    }
   ],
   "source": [
    "def load(name):\n",
    "    \"\"\"\n",
    "    Loads a NumPy array from a file. If the file is not found, \n",
    "    displays a warning and returns None.\n",
    "\n",
    "    name (str): The name of the file (without extension) to load from the 'embeddings' directory.\n",
    "    \n",
    "    return: np.ndarray or None: The loaded NumPy array, or None if the file is not found.\n",
    "    \"\"\"\n",
    "    file_name = f\"../result/{name}.npy\"\n",
    "    if not os.path.exists(file_name):\n",
    "        print(f\"Warning: File '{file_name}' not found.\")\n",
    "        return None\n",
    "\n",
    "    emb = np.load(file_name)\n",
    "    return emb\n",
    "\n",
    "embeddings = {}\n",
    "for k in graph_keys:\n",
    "    embeddings[k] = {}\n",
    "\n",
    "for graph_key in graph_keys:\n",
    "    for emb_key in embedding_keys:\n",
    "        emb_key.lower()\n",
    "        graph_key.lower()\n",
    "        s = f\"embeddings_{graph_key}_{emb_key}\"\n",
    "        embeddings[graph_key][emb_key] = load(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0573ed-f2fa-4226-88f4-ebbcf1c470e4",
   "metadata": {},
   "source": [
    "# Loading the graphs\n",
    "Selected graphs:\n",
    "- Facebook_combined    https://snap.stanford.edu/data/ego-Facebook.html          \n",
    "- cit-Helpth           https://networkrepository.com/cit-HepTh.php             \n",
    "- bio-CE-CX            https://networkrepository.com/bio-CE-CX.php             \n",
    "- CL-100K-1d8-L9       https://networkrepository.com/CL-100K-1d8-L9.php ---- the graph has node labels\n",
    "- COX2-MD              https://networkrepository.com/COX2-MD.php  ---- the graph has node labels\n",
    "\n",
    "To run this notebook, adjust the paths to match where the files are saved in your PC.<br>\n",
    "To keep paths as they are, create a \"data\" folder inside the directory of this notebook, and store the files there.<br><br>\n",
    "\n",
    "Graphs are stored as a dictionary: the key is the graph name, the value is the corresponding netowrkx graph.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "067626d5-62a8-42a3-bf77-6952d5af8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_path = '../data/facebook_combined.txt.gz'\n",
    "citation_path = '../data/cit-HepTh.edges'\n",
    "biological_path = '../data/bio-CE-CX.edges'\n",
    "CL_path = \"../data/CL-100K-1d8-L9.edges\"\n",
    "COX2_path = \"../data/COX2-MD.edges\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13a0738-853b-40ab-a2fc-2f7d9c0a24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(path):\n",
    "    \"\"\"\n",
    "    For files with extension .edges\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('%'):  # Skip comment lines\n",
    "                continue\n",
    "            # Split the line based on spaces or commas\n",
    "            data = re.split(r'[,\\s]+', line.strip())\n",
    "            if len(data) < 2:  # Skip lines that don't have at least two columns\n",
    "                continue\n",
    "            # Extract the first two columns (nodes)\n",
    "            node1, node2 = int(data[0]), int(data[1])\n",
    "            G.add_edge(node1, node2)\n",
    "        \n",
    "    return relabel_get_mapping(G)\n",
    "\n",
    "def load_graph_with_gz(path):\n",
    "    \"\"\"\n",
    "    For files with extension .txt.gz\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    with gzip.open(path, 'rt') as f:\n",
    "        for line in f:\n",
    "            node1, node2 = map(int, line.strip().split())\n",
    "            G.add_edge(node1, node2)\n",
    "            \n",
    "    return relabel_get_mapping(G)\n",
    "\n",
    "def print_graphs_info(graphs):\n",
    "    for k in graph_keys:\n",
    "        G = graphs[k]\n",
    "        print(f\"{k}: |V|={len(G.nodes)}, |E|={len(G.edges)}\")\n",
    "\n",
    "def relabel_get_mapping(G):\n",
    "    \"\"\"\n",
    "    Given a graph G, this function returns a graph where the nodes are relabeled as integers, form 0 to |V|-1.\n",
    "    It is also returned the mapping from relabeled name to original name.\n",
    "    \"\"\"\n",
    "    mapping = {node : i for i,node in enumerate(G.nodes)} # mappoing original : relabeled\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    return G, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b659245-9c28-4968-8261-b5a05ea83ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook: |V|=4039, |E|=88234\n",
      "citation: |V|=22908, |E|=2444798\n",
      "biological: |V|=15229, |E|=245952\n",
      "CL: |V|=92482, |E|=436611\n",
      "COX2: |V|=7962, |E|=101542\n"
     ]
    }
   ],
   "source": [
    "graphs = {}  # dictionary containg the graphs\n",
    "mappings = {} # dictionary to contain the mappings. Original name : relabeled name\n",
    "for k in graph_keys:\n",
    "    mappings[k] = {}\n",
    "    \n",
    "# facebook graph is the only one .tar.gz        \n",
    "graphs[graph_keys[0]], mappings[graph_keys[0]] = load_graph_with_gz(facebook_path)  # relabeling nodes to integer\n",
    "graphs[graph_keys[1]], mappings[graph_keys[1]] = load_graph(citation_path)\n",
    "graphs[graph_keys[2]], mappings[graph_keys[2]] = load_graph(biological_path)\n",
    "graphs[graph_keys[3]], mappings[graph_keys[3]] = load_graph(CL_path)  # node labeled\n",
    "graphs[graph_keys[4]], mappings[graph_keys[4]] = load_graph(COX2_path)  # node labeled\n",
    "\n",
    "print_graphs_info(graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbf757-a870-4245-a68c-acfc79b39e06",
   "metadata": {},
   "source": [
    "# Reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0180ec5d-ce56-48ce-93af-06814b1b4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reconstruction error data structure is built as the embeddings data structure.\n",
    "RE = {}\n",
    "for k in graph_keys:\n",
    "    RE[k] = {}\n",
    "\n",
    "def reconstruction_error(G, embeddings):\n",
    "    \"\"\"\n",
    "    Computes the reconstruction error of the graph by comparing cosine similarity\n",
    "    only for existing edges in the graph, avoiding dense adjacency matrix computations.\n",
    "    The reason is that for large graphs an exact computation causes memory issues, due to very large adjacency matrices.\n",
    "\n",
    "    Parameters:\n",
    "        G (networkx.Graph): The input graph.\n",
    "        embeddings (NumPy array): numpy array containing the embeddings, each row is a node embedding\n",
    "\n",
    "    Returns:\n",
    "        float: The reconstruction error as the average squared difference for existing edges.\n",
    "    \"\"\"\n",
    "    # Convert embeddings to matrix\n",
    "    embedding_vectors = np.array([embeddings[node] for node in G.nodes])\n",
    "\n",
    "    # Compute similarities only for existing edges\n",
    "    error = 0\n",
    "    for u, v in G.edges():\n",
    "        u_vec = embedding_vectors[u].reshape(1, -1)\n",
    "        v_vec = embedding_vectors[v].reshape(1, -1)\n",
    "        sim = cosine_similarity(u_vec, v_vec)[0, 0]\n",
    "        error += (1 - sim) ** 2\n",
    "\n",
    "    return error / G.number_of_edges()\n",
    "\n",
    "def print_reconstruction_error(err, graph_name , embedding_technique):\n",
    "    print(f\"RE of {graph_name} graph using {embedding_technique}: {err}\")\n",
    "\n",
    "def compute_all_reconstruction_errors(graph_keys, embedding_keys, show_results = True):\n",
    "    for graph_key in graph_keys:\n",
    "        if show_results:\n",
    "            print(f\"\\nReconstruction errors for {graph_key} graph:\\n\")\n",
    "        for emb_key in embedding_keys:     \n",
    "            RE[graph_key][emb_key]= reconstruction_error(graphs[graph_key], embeddings[graph_key][emb_key])\n",
    "            if show_results:\n",
    "                print_reconstruction_error(RE[graph_key][emb_key], graph_key, emb_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680dc3f3-8fa9-4563-b910-bf10da750a29",
   "metadata": {},
   "source": [
    "## Compute the RE\n",
    "Here you can compute the reconstruction error.<br>\n",
    "- Set graph_keys_RE with the keys of the graphs you are interested. graph_keys_RE = graph_keys for all graphs.<br>\n",
    "- set embedding_keys_RE with the keys of the embedding strategies you are interested. graph_keys_RE = embedding_keys for all embedding strategies.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c10ab-a648-4469-82b7-2ad908c1c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_keys_RE = [\"biological\"]\n",
    "embedding_keys_RE = [\"LINE\"]\n",
    "compute_all_reconstruction_errors(graph_keys_RE, embedding_keys_RE, show_results = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce44b9d-c39d-4c41-8d31-fbca41859cb6",
   "metadata": {},
   "source": [
    "# NODE CLASSIFICATION\n",
    "There are two graphs with node labels: CL graph and COX2 graph.<br>\n",
    "\n",
    "To do node classification we train a SVM. The dataset is composed of the embeddings of one of those two graphs, obtained with one of the embeddings algorithms considered, and of the node labels (multi-class classification task).\n",
    "\n",
    "The node classification consider the TRANSDUCTIVE CASE: some embeddings-labels are kept for the test set, but to compute the embeddings we considered\n",
    "the whole graph.\n",
    "\n",
    "IMPORTANT: the lables of the \"CL graph\" are organized in a very weird way: the nodes of the graph are not sequential numbers (e.g. some numbers for some reason are missing), but the labels are, and they don't respect the names of the original nodes. <br>\n",
    "For example, in the label file there is the entry 16,2 (node 16 has label 2), but in the original .edges file node 16 does not even exist. I assume that it means \"the 16th node has label 2\". <br>\n",
    "The function to extract node labels then assume that the labels are listed in sequential order (e.g. entry 1 is the label of first node, entry 2 of the second node, ecc..). This requirement is satisfied for both the labeled graphs we are considering. <br>\n",
    "We originally wanted to apply the mapping to the node-label pair, to correctly match the labels with the renamed nodes, but unfortunately because of this strange representation of the data for CL graph this is not possible, and we have to make this precise assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3afb662b-35b5-4e21-ab7b-e599e84dbc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c02b2a2-b8e9-491a-8f00-280af4c90900",
   "metadata": {},
   "outputs": [],
   "source": [
    "COX2_labels_path = \"../data/COX2-MD.node_labels\"\n",
    "CL_labels_path = \"../data/CL-100K-1d8-L9.node_labels\"\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_node_labels(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file containing node labels and returns a dictionary mapping nodes to labels.\n",
    "    The file can have two formats:\n",
    "        1. Comma-separated: node,label\n",
    "        2. Space-separated: node label\n",
    "        \n",
    "    IMPORTANT: this function assumes that labels starts from node '1' and each line\n",
    "    represents the label of the nodes sequantially (e.g. line 1 label of first node,\n",
    "    line 2 label of second node, ecc...).\n",
    "    The motivation is given in the markdown cell above.\n",
    "    Parameters: file_path : Path to the node label file.\n",
    "    \n",
    "    Return: A dictionary where keys are node IDs and values are labels.\n",
    "    \"\"\"\n",
    "    node_labels = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "\n",
    "            if ',' in line:\n",
    "                node, label = line.split(',')  # Comma-separated\n",
    "            else:\n",
    "                node, label = line.split()  # Space-separated\n",
    "            \n",
    "            node_labels[int(node)] = int(label)\n",
    "\n",
    "    # labels has to start from index 0, like embeddings. In the considered files labels starts from 1 and proceed in order\n",
    "    node_labels = {int(node)-1 :label for node, label in node_labels.items()} \n",
    "    return node_labels\n",
    "\n",
    "def train_SVM(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Train and evaluate an SVM classifier for multi-class node classification.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings (np.ndarray): Numpy array where each row is a node's embedding.\n",
    "        labels (dict): Dictionary mapping node indices to their labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with accuracy, F1 score, and a detailed classification report.\n",
    "    \"\"\"\n",
    "    # Ensure X (features) and y (labels) are aligned\n",
    "    X = np.array(embeddings)  # Node embeddings\n",
    "    y = np.array([labels[i] for i in range(len(labels))])  # Ensure correct ordering of labels\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=69)\n",
    "\n",
    "    clf = SVC(kernel='rbf', decision_function_shape='ovo')  # 'ovo' = one-vs-one for multi-class\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(\"SVM Classifier Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1-Score: {f1:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\"accuracy\": accuracy, \"macro_f1\": f1, \"report\": report}\n",
    "\n",
    "labels = {}\n",
    "labels[\"COX2\"] = load_node_labels(COX2_labels_path)\n",
    "labels[\"CL\"] = load_node_labels(CL_labels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146db805-4e7a-464b-8a5a-a46d8de3aff6",
   "metadata": {},
   "source": [
    "# Analysis of the datasets\n",
    "It is always a good idea to have a look at the datasets we are dealing with.<br>\n",
    "- features are the embeddings of the nodes, which is an D-dimensional vector, where D is the dimension of the specific embeddings\n",
    "we are using to training.\n",
    "- the labels represent the classes of the nodes. We can analyze the labels set to see how many different classes there are and understand how balanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87b8e962-b3fe-452a-9174-38adb7b05953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO DATASET COX2 GRAPH\n",
      "Number of samples: 7962\n",
      "Number of classes: 7\n",
      "Samples of class 1: 5575\n",
      "Samples of class 2: 599\n",
      "Samples of class 3: 571\n",
      "Samples of class 4: 354\n",
      "Samples of class 5: 767\n",
      "Samples of class 6: 89\n",
      "Samples of class 7: 7\n",
      "\n",
      "INFO DATASET CL GRAPH\n",
      "Number of samples: 92482\n",
      "Number of classes: 9\n",
      "Samples of class 8: 10276\n",
      "Samples of class 9: 10274\n",
      "Samples of class 2: 10276\n",
      "Samples of class 6: 10276\n",
      "Samples of class 1: 10276\n",
      "Samples of class 3: 10276\n",
      "Samples of class 5: 10276\n",
      "Samples of class 4: 10276\n",
      "Samples of class 7: 10276\n"
     ]
    }
   ],
   "source": [
    "def analyze_labels(labels):\n",
    "    labels_count = {}\n",
    "    for label in labels.values():\n",
    "        if not label in labels_count.keys():\n",
    "            labels_count[label] = 0\n",
    "        labels_count[label] += 1\n",
    "    print(f\"Number of samples: {len(labels)}\")\n",
    "    print(f\"Number of classes: {len(labels_count)}\")\n",
    "    for label in labels_count.keys():\n",
    "        print(f\"Samples of class {label}: {labels_count[label]}\")\n",
    "\n",
    "print(\"INFO DATASET COX2 GRAPH\")\n",
    "analyze_labels(labels[\"COX2\"])\n",
    "\n",
    "print(\"\\nINFO DATASET CL GRAPH\")\n",
    "analyze_labels(labels[\"CL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ca218-3478-478e-8365-dba1092d8c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_key = \"LINE\"\n",
    "graph_key = \"CL\"\n",
    "train_SVM(embeddings[graph_key][embedding_key], labels[graph_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c23bb-6f7d-44e9-967a-a0f06929471e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
